<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="第1章 SparkSQL 概述1.1 SparkSQL 是什么  Spark SQL 是 Spark 用于结构化数据(structured data)处理的 Spark 模块。   1.2 Hive and SparkSQL">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkSQL笔记">
<meta property="og:url" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="没有尾巴的小驴">
<meta property="og:description" content="第1章 SparkSQL 概述1.1 SparkSQL 是什么  Spark SQL 是 Spark 用于结构化数据(structured data)处理的 Spark 模块。   1.2 Hive and SparkSQL">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240518102148826.png">
<meta property="og:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521112332944.png">
<meta property="og:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521112403005.png">
<meta property="og:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521113939827.png">
<meta property="og:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521114046580.png">
<meta property="og:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521114231484.png">
<meta property="og:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521114429908.png">
<meta property="og:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521114458837.png">
<meta property="og:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521114540997.png">
<meta property="og:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521114710422.png">
<meta property="og:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521114820036.png">
<meta property="og:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521114901899.png">
<meta property="og:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521143336288.png">
<meta property="og:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521174655322.png">
<meta property="og:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240614155619334.png">
<meta property="og:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240614154934307.png">
<meta property="og:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240614155220015.png">
<meta property="article:published_time" content="2024-05-17T08:00:32.000Z">
<meta property="article:modified_time" content="2025-06-25T08:08:45.951Z">
<meta property="article:author" content="Rui Zhang">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240518102148826.png">

<link rel="canonical" href="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true
  };
</script>

  <title>SparkSQL笔记 | 没有尾巴的小驴</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">没有尾巴的小驴</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">记录生活</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">22</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">22</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">52</span></a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.jpg">
      <meta itemprop="name" content="Rui Zhang">
      <meta itemprop="description" content="不在沉默中爆发，就在沉默中灭亡">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="没有尾巴的小驴">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          SparkSQL笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-05-17 16:00:32" itemprop="dateCreated datePublished" datetime="2024-05-17T16:00:32+08:00">2024-05-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-06-25 16:08:45" itemprop="dateModified" datetime="2025-06-25T16:08:45+08:00">2025-06-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>42k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>38 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="第1章-SparkSQL-概述"><a href="#第1章-SparkSQL-概述" class="headerlink" title="第1章 SparkSQL 概述"></a>第1章 SparkSQL 概述</h1><p>1.1 SparkSQL 是什么</p>
<p><img src="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240518102148826.png" alt="image-20240518102148826"></p>
<p>Spark SQL 是 Spark 用于结构化数据(structured data)处理的 Spark 模块。  </p>
<h2 id="1-2-Hive-and-SparkSQL"><a href="#1-2-Hive-and-SparkSQL" class="headerlink" title="1.2 Hive and SparkSQL"></a>1.2 Hive and SparkSQL</h2><p>SparkSQL 的前身是 Shark，给熟悉 RDBMS 但又不理解 MapReduce 的技术人员提供快速上手的工具。</p>
<p>Hive 是早期唯一运行在 Hadoop 上的 SQL-on-Hadoop 工具。但是 MapReduce 计算过程中大量的中间磁盘落地过程消耗了大量的 I/O，降低的运行效率，为了提高 SQL-on-Hadoop的效率，大量的 SQL-on-Hadoop 工具开始产生，其中表现较为突出的是：  </p>
<ul>
<li>Drill</li>
<li>Impala</li>
<li>Shark  </li>
</ul>
<p>其中 Shark 是伯克利实验室 Spark 生态环境的组件之一， 是基于 Hive 所开发的工具， 它修改了下图所示的右下角的内存管理、物理计划、执行三个模块，并使之能运行在 Spark 引擎上。  </p>
<p><img src="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521112332944.png" alt="image-20240521112332944"></p>
<p>Shark 的出现，使得 SQL-on-Hadoop 的性能比 Hive 有了 10-100 倍的提高。  </p>
<p><img src="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521112403005.png" alt="image-20240521112403005"></p>
<p>但是，随着 Spark 的发展，对于野心勃勃的 Spark 团队来说， Shark 对于 Hive 的太多依赖（如采用 Hive 的语法解析器、查询优化器等等），制约了 Spark 的 One Stack Rule Them All的既定方针，制约了 Spark 各个组件的相互集成，所以提出了 SparkSQL 项目。 SparkSQL抛弃原有 Shark 的代码，汲取了 Shark 的一些优点，如内存列存储（In-Memory ColumnarStorage）、Hive兼容性等，重新开发了SparkSQL代码；由于摆脱了对Hive的依赖性， SparkSQL无论在数据兼容、性能优化、组件扩展方面都得到了极大的方便，真可谓“退一步，海阔天空”。  </p>
<ul>
<li>数据兼容方面 SparkSQL 不但兼容 Hive，还可以从 RDD、 parquet 文件、 JSON 文件中获取数据，未来版本甚至支持获取 RDBMS 数据以及 cassandra 等 NOSQL 数据；</li>
<li>性能优化方面 除了采取 In-Memory Columnar Storage、 byte-code generation 等优化技术外、将会引进 Cost Model 对查询进行动态评估、获取最佳物理计划等等；</li>
<li>组件扩展方面 无论是 SQL 的语法解析器、分析器还是优化器都可以重新定义，进行扩展。  </li>
</ul>
<p><img src="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521113939827.png" alt="image-20240521113939827"></p>
<p>2014 年 6 月 1 日 Shark 项目和 SparkSQL 项目的主持人 Reynold Xin 宣布：停止对 Shark 的开发，团队将所有资源放 SparkSQL 项目上，至此， Shark 的发展画上了句话，但也因此发展出两个支线： SparkSQL（Spark on Hive） 和 Hive on Spark。  </p>
<p><img src="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521114046580.png" alt="image-20240521114046580"></p>
<p>其中 SparkSQL 作为 Spark 生态的一员继续发展，而不再受限于 Hive，只是兼容 Hive；而Hive on Spark 是一个 Hive 的发展计划，该计划将 Spark 作为 Hive 的底层引擎之一，也就是说， Hive 将不再受限于一个引擎，可以采用 Map-Reduce、 Tez、 Spark 等引擎。</p>
<p>对于开发人员来讲， SparkSQL 可以简化 RDD 的开发，提高开发效率，且执行效率非常快，所以实际工作中，基本上采用的就是 SparkSQL。 Spark SQL 为了简化 RDD 的开发，提高开发效率， 提供了 2 个编程抽象，类似 Spark Core 中的 RDD  </p>
<ul>
<li>DataFrame</li>
<li>DataSet</li>
</ul>
<h2 id="1-3-SparkSQL-特点"><a href="#1-3-SparkSQL-特点" class="headerlink" title="1.3 SparkSQL 特点"></a>1.3 SparkSQL 特点</h2><h3 id="1-3-1-易整合"><a href="#1-3-1-易整合" class="headerlink" title="1.3.1 易整合"></a>1.3.1 易整合</h3><p>无缝的整合了 SQL 查询和 Spark 编程  </p>
<p><img src="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521114231484.png" alt="image-20240521114231484"></p>
<h3 id="1-3-2-统一的数据访问"><a href="#1-3-2-统一的数据访问" class="headerlink" title="1.3.2 统一的数据访问"></a>1.3.2 统一的数据访问</h3><p>使用相同的方式连接不同的数据源  </p>
<p><img src="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521114429908.png" alt="image-20240521114429908"></p>
<h3 id="1-3-3-兼容-Hive"><a href="#1-3-3-兼容-Hive" class="headerlink" title="1.3.3 兼容 Hive"></a>1.3.3 兼容 Hive</h3><p>在已有的仓库上直接运行 SQL 或者 HiveQL  </p>
<p><img src="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521114458837.png" alt="image-20240521114458837"></p>
<h3 id="1-3-4-标准数据连接"><a href="#1-3-4-标准数据连接" class="headerlink" title="1.3.4 标准数据连接"></a>1.3.4 标准数据连接</h3><p>通过 JDBC 或者 ODBC 来连接  </p>
<p><img src="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521114540997.png" alt="image-20240521114540997"></p>
<h2 id="1-4-DataFrame-是什么"><a href="#1-4-DataFrame-是什么" class="headerlink" title="1.4 DataFrame 是什么"></a>1.4 DataFrame 是什么</h2><p>在 Spark 中， DataFrame 是一种以 RDD 为基础的分布式数据集， 类似于传统数据库中的二维表格。 DataFrame 与 RDD 的主要区别在于，前者带有 schema 元信息，即 DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得 Spark SQL 得以洞察更多的结构信息，从而对藏于 DataFrame 背后的数据源以及作用于 DataFrame 之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观 RDD，由于无从得知所存数据元素的具体内部结构， SparkCore 只能在 stage 层面进行简单、通用的流水线优化。</p>
<p>同时，与 Hive 类似， DataFrame 也支持嵌套数据类型（struct、 array 和 map）。从 API易用性的角度上看， DataFrame API 提供的是一套高层的关系操作，比函数式的 RDD API 要更加友好，门槛更低。</p>
<p><img src="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521114710422.png" alt="image-20240521114710422"></p>
<p>上图直观地体现了 DataFrame 和 RDD 的区别。</p>
<p>左侧的 RDD[Person]虽然以 Person 为类型参数，但 Spark 框架本身不了解 Person 类的内部结构。而右侧的 DataFrame 却提供了详细的结构信息，使得 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。</p>
<p>DataFrame 是为数据提供了 Schema 的视图。可以把它当做数据库中的一张表来对待，DataFrame 也是懒执行的，但性能上比 RDD 要高，主要原因：优化的执行计划，即查询计划通过 Spark catalyst optimiser 进行优化。比如下面一个例子:  </p>
<p><img src="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521114820036.png" alt="image-20240521114820036"></p>
<p>为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们 join 之后又做了一次 filter 操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为 join 是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将 filter 下推到 join 下方，先对 DataFrame 进行过滤，再 join 过滤后的较小的结果集，便可以有效缩短执行时间。而 Spark SQL 的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。  </p>
<p><img src="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521114901899.png" alt="image-20240521114901899"></p>
<h2 id="1-5-DataSet-是什么"><a href="#1-5-DataSet-是什么" class="headerlink" title="1.5 DataSet 是什么"></a>1.5 DataSet 是什么</h2><p>DataSet 是分布式数据集合。 DataSet 是 Spark 1.6 中添加的一个新抽象，是 DataFrame的一个扩展。它提供了 RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及 SparkSQL 优化执行引擎的优点。 DataSet 也可以使用功能性的转换（操作 map， flatMap， filter等等）。  </p>
<ul>
<li>DataSet 是 DataFrame API 的一个扩展，是 SparkSQL 最新的数据抽象</li>
<li>用户友好的 API 风格，既具有类型安全检查也具有 DataFrame 的查询优化特性；</li>
<li>用样例类来对 DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet 中的字段名称；</li>
<li>DataSet 是强类型的。比如可以有 DataSet[Car]， DataSet[Person]。</li>
<li>DataFrame 是 DataSet 的特列， DataFrame=DataSet[Row] ，所以可以通过 as 方法将DataFrame 转换为 DataSet。 Row 是一个类型，跟 Car、 Person 这些的类型一样，所有的表结构信息都用 Row 来表示。 获取数据时需要指定顺序  </li>
</ul>
<h1 id="第2章-SparkSQL-核心编程"><a href="#第2章-SparkSQL-核心编程" class="headerlink" title="第2章 SparkSQL 核心编程"></a>第2章 SparkSQL 核心编程</h1><p>本课件重点学习如何使用 Spark SQL 所提供的 DataFrame 和 DataSet 模型进行编程.，以及了解它们之间的关系和转换， 关于具体的 SQL 书写不是我们的重点。  </p>
<h2 id="2-1-新的起点"><a href="#2-1-新的起点" class="headerlink" title="2.1 新的起点"></a>2.1 新的起点</h2><p>SparkCore 中，如果想要执行应用程序，需要首先构建上下文环境对象 SparkContext，Spark SQL 其实可以理解为对 Spark Core 的一种封装，不仅仅在模型上进行了封装，上下文环境对象也进行了封装。</p>
<p>在老的版本中， SparkSQL 提供两种 SQL 查询起始点：一个叫 SQLContext，用于 Spark自己提供的 SQL 查询；一个叫 HiveContext，用于连接 Hive 的查询。</p>
<p>SparkSession 是 Spark 最新的 SQL 查询起始点，实质上是 SQLContext 和 HiveContext的组合，所以在 SQLContext 和 HiveContext 上可用的 API 在 SparkSession 上同样是可以使用的。 SparkSession 内部封装了 SparkContext，所以计算实际上是由 sparkContext 完成的。当我们使用 spark-shell 的时候, spark 框架会自动的创建一个名称叫做 spark 的 SparkSession 对象, 就像我们以前可以自动获取到一个 sc 来表示 SparkContext 对象一样</p>
<p><img src="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521143336288.png" alt="image-20240521143336288"></p>
<h2 id="2-2-DataFrame"><a href="#2-2-DataFrame" class="headerlink" title="2.2 DataFrame"></a>2.2 DataFrame</h2><p>Spark SQL 的 DataFrame API 允许我们使用 DataFrame 而不用必须去注册临时表或者生成 SQL 表达式。 DataFrame API 既有 transformation 操作也有 action 操作。  </p>
<h3 id="2-2-1-创建-DataFrame"><a href="#2-2-1-创建-DataFrame" class="headerlink" title="2.2.1 创建 DataFrame"></a>2.2.1 创建 DataFrame</h3><p>在 Spark SQL 中 SparkSession 是创建 DataFrame 和执行 SQL 的入口，创建 DataFrame有三种方式：通过 Spark 的数据源进行创建；从一个存在的 RDD 进行转换；还可以从 HiveTable 进行查询返回。  </p>
<p>1) 从 Spark 数据源进行创建  </p>
<p>   查看 Spark 支持创建文件的数据源格式  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.read.</span><br><span class="line">csv format jdbc json load option options orc parquet schema table text textFile</span><br></pre></td></tr></table></figure>

<p>   在 spark 的 bin/data 目录中创建 user.json 文件  </p>
   <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"username"</span>:<span class="string">"zhangsan"</span>,<span class="attr">"age"</span>:<span class="number">20</span>&#125;</span><br></pre></td></tr></table></figure>

<p>   读取 json 文件创建 DataFrame  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">"data/user.json"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， username: string]</span><br></pre></td></tr></table></figure>

<p>   注意：如果从内存中获取数据， spark 可以知道数据类型具体是什么。 如果是数字，默认作为 Int 处理；但是从文件中读取的数字，不能确定是什么类型，所以用 bigint 接收，可以和Long 类型转换，但是和 Int 不能进行转换  </p>
<p>   展示结果</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| <span class="number">20</span>|zhangsan|</span><br><span class="line">+---+--------+</span><br></pre></td></tr></table></figure>

<p>2) 从 RDD 进行转换</p>
<p>   在后续章节中讨论</p>
<p>3) 从 Hive Table 进行查询返回</p>
<p>   在后续章节中讨论  </p>
<h3 id="2-2-2-SQL-语法"><a href="#2-2-2-SQL-语法" class="headerlink" title="2.2.2 SQL 语法"></a>2.2.2 SQL 语法</h3><p>SQL 语法风格是指我们查询数据的时候使用 SQL 语句来查询，这种风格的查询必须要有临时视图或者全局视图来辅助</p>
<p>1) 读取 JSON 文件创建 DataFrame  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">"data/user.json"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， username: string]</span><br></pre></td></tr></table></figure>

<p>2) 对 DataFrame 创建一个临时表  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br></pre></td></tr></table></figure>

<p>3) 通过 SQL 语句实现查询全表  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> sqlDF = spark.sql(<span class="string">"SELECT * FROM people"</span>)</span><br><span class="line">sqlDF: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， name: string]</span><br></pre></td></tr></table></figure>

<p>4) 结果展示  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sqlDF.show</span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| <span class="number">20</span>|zhangsan|</span><br><span class="line">| <span class="number">30</span>| lisi   |</span><br><span class="line">| <span class="number">40</span>| wangwu |</span><br><span class="line">+---+--------+</span><br></pre></td></tr></table></figure>

<p>   注意：普通临时表是 Session 范围内的，如果想应用范围内有效，可以使用全局临时表。使用全局临时表时需要全路径访问，如： global_temp.people  </p>
<p>5) 对于 DataFrame 创建一个全局表  ]</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.createGlobalTempView(<span class="string">"people"</span>)</span><br></pre></td></tr></table></figure>

<p>6) 通过 SQL 语句实现查询全表  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| <span class="number">20</span>|zhangsan|</span><br><span class="line">| <span class="number">30</span>| lisi   |</span><br><span class="line">| <span class="number">40</span>| wangwu |</span><br><span class="line">+---+--------+</span><br><span class="line">scala&gt; spark.newSession().sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| <span class="number">20</span>|zhangsan|</span><br><span class="line">| <span class="number">30</span>| lisi   |</span><br><span class="line">| <span class="number">40</span>| wangwu |</span><br><span class="line">+---+--------+</span><br></pre></td></tr></table></figure>

<h3 id="2-2-3-DSL-语法"><a href="#2-2-3-DSL-语法" class="headerlink" title="2.2.3 DSL 语法"></a>2.2.3 DSL 语法</h3><p>DataFrame 提供一个特定领域语言(domain-specific language, DSL)去管理结构化的数据。可以在 Scala, Java, Python 和 R 中使用 DSL，使用 DSL 语法风格不必去创建临时视图了。</p>
<p>1) 创建一个 DataFrame  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">"data/user.json"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， name: string]</span><br></pre></td></tr></table></figure>

<p>2) 查看 DataFrame 的 Schema 信息  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line">|-- age: <span class="type">Long</span> (nullable = <span class="literal">true</span>)</span><br><span class="line">|-- username: string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>

<p>3) 只查看”username”列数据</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select(<span class="string">"username"</span>).show()</span><br><span class="line">+--------+</span><br><span class="line">|username|</span><br><span class="line">+--------+</span><br><span class="line">|zhangsan|</span><br><span class="line">| lisi   |</span><br><span class="line">| wangwu |</span><br><span class="line">+--------+</span><br></pre></td></tr></table></figure>

<p>4) 查看”username”列数据以及”age+1”数据  </p>
<p>   <strong>注意:涉及到运算的时候, 每列都必须使用$, 或者采用引号表达式：单引号+字段名</strong>  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select($<span class="string">"username"</span>,$<span class="string">"age"</span> + <span class="number">1</span>).show</span><br><span class="line">scala&gt; df.select(<span class="symbol">'username</span>, <span class="symbol">'age</span> + <span class="number">1</span>).show()</span><br><span class="line">scala&gt; df.select(<span class="symbol">'username</span>, <span class="symbol">'age</span> + <span class="number">1</span> as <span class="string">"newage"</span>).show()</span><br><span class="line">+--------+---------+</span><br><span class="line">|username|(age + <span class="number">1</span>)|</span><br><span class="line">+--------+---------+</span><br><span class="line">|zhangsan|    <span class="number">21</span>   |</span><br><span class="line">| lisi   |    <span class="number">31</span>   |</span><br><span class="line">| wangwu |    <span class="number">41</span>   |</span><br><span class="line">+--------+---------+</span><br></pre></td></tr></table></figure>

<p>5) 查看”age”大于”30”的数据  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.filter($<span class="string">"age"</span>&gt;<span class="number">30</span>).show</span><br><span class="line">+---+---------+</span><br><span class="line">|age| username|</span><br><span class="line">+---+---------+</span><br><span class="line">| <span class="number">40</span>| wangwu  |</span><br><span class="line">+---+---------+</span><br></pre></td></tr></table></figure>

<p>6) 按照”age”分组，查看数据条数  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.groupBy(<span class="string">"age"</span>).count.show</span><br><span class="line">+---+-----+</span><br><span class="line">|age|count|</span><br><span class="line">+---+-----+</span><br><span class="line">| <span class="number">20</span>| <span class="number">1</span>   |</span><br><span class="line">| <span class="number">30</span>| <span class="number">1</span>   |</span><br><span class="line">| <span class="number">40</span>| <span class="number">1</span>   |</span><br><span class="line">+---+-----+</span><br></pre></td></tr></table></figure>

<h3 id="2-2-4-RDD-转换为-DataFrame"><a href="#2-2-4-RDD-转换为-DataFrame" class="headerlink" title="2.2.4 RDD 转换为 DataFrame"></a>2.2.4 RDD 转换为 DataFrame</h3><p>在 IDEA 中开发程序时， 如果需要 RDD 与 DF 或者 DS 之间互相操作，那么需要引入import spark.implicits._</p>
<p>这里的 spark 不是 Scala 中的包名，而是创建的 sparkSession 对象的变量名称，所以必须先创建 SparkSession 对象再导入。这里的 spark 对象不能使用 var 声明，因为 Scala 只支持val 修饰的对象的引入。</p>
<p>spark-shell 中无需导入，自动完成此操作。  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> idRDD = sc.textFile(<span class="string">"data/id.txt"</span>)</span><br><span class="line">scala&gt; idRDD.toDF(<span class="string">"id"</span>).show</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">| <span class="number">1</span> |</span><br><span class="line">| <span class="number">2</span> |</span><br><span class="line">| <span class="number">3</span> |</span><br><span class="line">| <span class="number">4</span> |</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure>

<p>实际开发中，一般通过样例类将 RDD 转换为 DataFrame  </p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; case class User(name:String, age:Int)</span><br><span class="line">defined class User</span><br><span class="line">scala&gt; sc.makeRDD(List(("zhangsan",30), ("lisi",40))).map(t=&gt;User(t.<span class="emphasis">_1,</span></span><br><span class="line"><span class="emphasis">t._</span>2)).toDF.show</span><br><span class="line"><span class="code">+--------+</span>---+</span><br><span class="line">| name   |age|</span><br><span class="line"><span class="code">+--------+</span>---+</span><br><span class="line">|zhangsan|30 |</span><br><span class="line">| lisi   |40 |</span><br><span class="line"><span class="code">+--------+</span>---+</span><br></pre></td></tr></table></figure>

<h3 id="2-2-5-DataFrame-转换为-RDD"><a href="#2-2-5-DataFrame-转换为-RDD" class="headerlink" title="2.2.5 DataFrame 转换为 RDD"></a>2.2.5 DataFrame 转换为 RDD</h3><p>DataFrame 其实就是对 RDD 的封装，所以可以直接获取内部的 RDD  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = sc.makeRDD(<span class="type">List</span>((<span class="string">"zhangsan"</span>,<span class="number">30</span>), (<span class="string">"lisi"</span>,<span class="number">40</span>))).map(t=&gt;<span class="type">User</span>(t._1, t._2)).toDF</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: int]</span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = df.rdd</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[org.apache.spark.sql.<span class="type">Row</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">46</span>] at rdd at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> array = rdd.collect</span><br><span class="line">array: <span class="type">Array</span>[org.apache.spark.sql.<span class="type">Row</span>] = <span class="type">Array</span>([zhangsan,<span class="number">30</span>], [lisi,<span class="number">40</span>])</span><br></pre></td></tr></table></figure>

<p>注意： 此时得到的 RDD 存储类型为 Row  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; array(<span class="number">0</span>)</span><br><span class="line">res28: org.apache.spark.sql.<span class="type">Row</span> = [zhangsan,<span class="number">30</span>]</span><br><span class="line">scala&gt; array(<span class="number">0</span>)(<span class="number">0</span>)</span><br><span class="line">res29: <span class="type">Any</span> = zhangsan</span><br><span class="line">scala&gt; array(<span class="number">0</span>).getAs[<span class="type">String</span>](<span class="string">"name"</span>)</span><br><span class="line">res30: <span class="type">String</span> = zhangsan</span><br></pre></td></tr></table></figure>

<h2 id="2-3-DataSet"><a href="#2-3-DataSet" class="headerlink" title="2.3 DataSet"></a>2.3 DataSet</h2><p>DataSet 是具有强类型的数据集合，需要提供对应的类型信息。  </p>
<h3 id="2-3-1-创建-DataSet"><a href="#2-3-1-创建-DataSet" class="headerlink" title="2.3.1 创建 DataSet"></a>2.3.1 创建 DataSet</h3><p>1） 使用样例类序列创建 DataSet</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">Person</span></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">caseClassDS</span> </span>= <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"zhangsan"</span>,<span class="number">2</span>)).toDS()</span><br><span class="line">caseClassDS: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: <span class="type">Long</span>]</span><br><span class="line">scala&gt; caseClassDS.show</span><br><span class="line">+---------+---+</span><br><span class="line">| name    |age|</span><br><span class="line">+---------+---+</span><br><span class="line">| zhangsan| <span class="number">2</span> |</span><br><span class="line">+---------+---+</span><br></pre></td></tr></table></figure>

<p>2） 使用基本类型的序列创建 DataSet  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).toDS</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Int</span>] = [value: int]</span><br><span class="line">scala&gt; ds.show</span><br><span class="line">+-----+</span><br><span class="line">|value|</span><br><span class="line">+-----+</span><br><span class="line">| <span class="number">1</span>|</span><br><span class="line">| <span class="number">2</span>|</span><br><span class="line">| <span class="number">3</span>|</span><br><span class="line">| <span class="number">4</span>|</span><br><span class="line">| <span class="number">5</span>|</span><br><span class="line">+-----+</span><br></pre></td></tr></table></figure>

<p><strong>注意： 在实际使用的时候，很少用到把序列转换成DataSet，更多的是通过RDD来得到DataSet</strong>  </p>
<h3 id="2-3-2-RDD-转换为-DataSet"><a href="#2-3-2-RDD-转换为-DataSet" class="headerlink" title="2.3.2 RDD 转换为 DataSet"></a>2.3.2 RDD 转换为 DataSet</h3><p>SparkSQL 能够自动将包含有 case 类的 RDD 转换成 DataSet， case 类定义了 table 的结构， case 类属性通过反射变成了表的列名。 Case 类可以包含诸如 Seq 或者 Array 等复杂的结构。  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">User</span></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">sc</span>.<span class="title">makeRDD</span>(<span class="params"><span class="type">List</span>(("zhangsan",30</span>), (<span class="params">"lisi",49</span>))).<span class="title">map</span>(<span class="params">t=&gt;<span class="type">User</span>(t._1, t._2</span>)).<span class="title">toDS</span></span></span><br><span class="line"><span class="class"><span class="title">res11</span></span>: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">User</span>] = [name: string, age: int]</span><br></pre></td></tr></table></figure>

<h3 id="2-3-3-DataSet-转换为-RDD"><a href="#2-3-3-DataSet-转换为-RDD" class="headerlink" title="2.3.3 DataSet 转换为 RDD"></a>2.3.3 DataSet 转换为 RDD</h3><p>DataSet 其实也是对 RDD 的封装，所以可以直接获取内部的 RDD  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">User</span></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">sc</span>.<span class="title">makeRDD</span>(<span class="params"><span class="type">List</span>(("zhangsan",30</span>), (<span class="params">"lisi",49</span>))).<span class="title">map</span>(<span class="params">t=&gt;<span class="type">User</span>(t._1, t._2</span>)).<span class="title">toDS</span></span></span><br><span class="line"><span class="class"><span class="title">res11</span></span>: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">User</span>] = [name: string, age: int]</span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = res11.rdd</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">User</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">51</span>] at rdd at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">User</span>] = <span class="type">Array</span>(<span class="type">User</span>(zhangsan,<span class="number">30</span>), <span class="type">User</span>(lisi,<span class="number">49</span>))</span><br></pre></td></tr></table></figure>

<h2 id="2-4-DataFrame-和-DataSet-转换"><a href="#2-4-DataFrame-和-DataSet-转换" class="headerlink" title="2.4 DataFrame 和 DataSet 转换"></a>2.4 DataFrame 和 DataSet 转换</h2><p>DataFrame 其实是 DataSet 的特例，所以它们之间是可以互相转换的。  </p>
<p>1）DataFrame 转换为 DataSet  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">User</span></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">df</span> </span>= sc.makeRDD(<span class="type">List</span>((<span class="string">"zhangsan"</span>,<span class="number">30</span>), (<span class="string">"lisi"</span>,<span class="number">49</span>))).toDF(<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: int]</span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = df.as[<span class="type">User</span>]</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">User</span>] = [name: string, age: int]</span><br></pre></td></tr></table></figure>

<p>2）DataSet 转换为 DataFrame  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> ds = df.as[<span class="type">User</span>]</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">User</span>] = [name: string, age: int]</span><br><span class="line">scala&gt; <span class="keyword">val</span> df = ds.toDF</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: int]</span><br></pre></td></tr></table></figure>

<h2 id="2-5-RDD、-DataFrame、-DataSet-三者的关系"><a href="#2-5-RDD、-DataFrame、-DataSet-三者的关系" class="headerlink" title="2.5 RDD、 DataFrame、 DataSet 三者的关系"></a>2.5 RDD、 DataFrame、 DataSet 三者的关系</h2><p>在 SparkSQL 中 Spark 为我们提供了两个新的抽象，分别是 DataFrame 和 DataSet。他们和 RDD 有什么区别呢？首先从版本的产生上来看：</p>
<ul>
<li>Spark1.0 =&gt; RDD</li>
<li>Spark1.3 =&gt; DataFrame</li>
<li>Spark1.6 =&gt; Dataset  </li>
</ul>
<p>如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。在后期的 Spark 版本中， DataSet 有可能会逐步取代 RDD和 DataFrame 成为唯一的 API 接口。</p>
<h3 id="2-5-1-三者的共性"><a href="#2-5-1-三者的共性" class="headerlink" title="2.5.1 三者的共性"></a>2.5.1 三者的共性</h3><ul>
<li>RDD、 DataFrame、 DataSet 全都是 spark 平台下的分布式弹性数据集，为处理超大型数据提供便利</li>
<li>三者都有惰性机制，在进行创建、转换，如 map 方法时，不会立即执行，只有在遇到Action 如 foreach 时，三者才会开始遍历运算</li>
<li>三者有许多共同的函数，如 filter，排序等</li>
<li>在对 DataFrame 和 Dataset 进行操作，许多操作都需要这个包:import spark.implicits._（在创建好 SparkSession 对象后尽量直接导入）</li>
<li>三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</li>
<li>三者都有 partition 的概念</li>
<li>DataFrame 和 DataSet 均可使用模式匹配获取各个字段的值和类型  </li>
</ul>
<h3 id="2-5-2-三者的区别"><a href="#2-5-2-三者的区别" class="headerlink" title="2.5.2 三者的区别"></a>2.5.2 三者的区别</h3><p>1）RDD  </p>
<ul>
<li>RDD 一般和 spark mllib 同时使用</li>
<li>RDD 不支持 sparksql 操作  </li>
</ul>
<p>2）DataFrame</p>
<ul>
<li>与 RDD 和 DataSet 不同， DataFrame 每一行的类型固定为 Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值</li>
<li>DataFrame 与 DataSet 一般不与 spark mllib 同时使用</li>
<li>DataFrame 与 DataSet 均支持 SparkSQL 的操作，比如 select， groupby 之类，还能注册临时表/视窗，进行 sql 语句操作</li>
<li>DataFrame 与 DataSet 支持一些特别方便的保存方式，比如保存成 csv，可以带上表头，这样每一列的字段名一目了然(后面专门讲解)</li>
</ul>
<p>3）DataSet  </p>
<ul>
<li>DataSet 和 DataFrame 拥有完全相同的成员函数，区别只是每一行的数据类型不同。 DataFrame 其实就是 DataSet 的一个特例 type DataFrame = Dataset[Row]  </li>
<li>DataFrame 也可以叫 Dataset[Row],每一行的类型是 Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者共性中的第七条提到的模式匹配拿出特定字段。而 DataSet 中，每一行是什么类型是不一定的，在自定义了 case class 之后可以很自由的获得每一行的信息  </li>
</ul>
<h3 id="2-5-3-三者的互相转换"><a href="#2-5-3-三者的互相转换" class="headerlink" title="2.5.3 三者的互相转换"></a>2.5.3 三者的互相转换</h3><p><img src="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240521174655322.png" alt="image-20240521174655322"></p>
<h2 id="2-6-IDEA-开发-SparkSQL"><a href="#2-6-IDEA-开发-SparkSQL" class="headerlink" title="2.6 IDEA 开发 SparkSQL"></a>2.6 IDEA 开发 SparkSQL</h2><p>实际开发中，都是使用 IDEA 进行开发的。  </p>
<h3 id="2-6-1-添加依赖"><a href="#2-6-1-添加依赖" class="headerlink" title="2.6.1 添加依赖"></a>2.6.1 添加依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="2-6-2-代码实现"><a href="#2-6-2-代码实现" class="headerlink" title="2.6.2 代码实现"></a>2.6.2 代码实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSQL01_Demo</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//创建上下文环境配置对象</span></span><br><span class="line">        <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span></span><br><span class="line">        <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkSQL01_Demo"</span>)</span><br><span class="line">        <span class="comment">//创建 SparkSession 对象</span></span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(conf).getOrCreate()</span><br><span class="line">        <span class="comment">//RDD=&gt;DataFrame=&gt;DataSet 转换需要引入隐式转换规则，否则无法转换</span></span><br><span class="line">        <span class="comment">//spark 不是包名，是上下文环境对象名</span></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        <span class="comment">//读取 json 文件 创建 DataFrame &#123;"username": "lisi","age": 18&#125;</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">"input/test.json"</span>)</span><br><span class="line">        <span class="comment">//df.show()</span></span><br><span class="line">        <span class="comment">//SQL 风格语法</span></span><br><span class="line">        df.createOrReplaceTempView(<span class="string">"user"</span>)</span><br><span class="line">        <span class="comment">//spark.sql("select avg(age) from user").show</span></span><br><span class="line">        <span class="comment">//DSL 风格语法</span></span><br><span class="line">        <span class="comment">//df.select("username","age").show()</span></span><br><span class="line">        <span class="comment">//*****RDD=&gt;DataFrame=&gt;DataSet*****</span></span><br><span class="line">        <span class="comment">//RDD</span></span><br><span class="line">        <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>)] =</span><br><span class="line">        spark.sparkContext.makeRDD(<span class="type">List</span>((<span class="number">1</span>,<span class="string">"zhangsan"</span>,<span class="number">30</span>),(<span class="number">2</span>,<span class="string">"lisi"</span>,<span class="number">28</span>),(<span class="number">3</span>,<span class="string">"wangwu"</span>, <span class="number">20</span>)))</span><br><span class="line">        <span class="comment">//DataFrame</span></span><br><span class="line">        <span class="keyword">val</span> df1: <span class="type">DataFrame</span> = rdd1.toDF(<span class="string">"id"</span>,<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">        <span class="comment">//df1.show()</span></span><br><span class="line">        <span class="comment">//DateSet</span></span><br><span class="line">        <span class="keyword">val</span> ds1: <span class="type">Dataset</span>[<span class="type">User</span>] = df1.as[<span class="type">User</span>]</span><br><span class="line">        <span class="comment">//ds1.show()</span></span><br><span class="line">        <span class="comment">//*****DataSet=&gt;DataFrame=&gt;RDD*****</span></span><br><span class="line">        <span class="comment">//DataFrame</span></span><br><span class="line">        <span class="keyword">val</span> df2: <span class="type">DataFrame</span> = ds1.toDF()</span><br><span class="line">        <span class="comment">//RDD 返回的 RDD 类型为 Row，里面提供的 getXXX 方法可以获取字段值，类似 jdbc 处理结果集，但是索引从 0 开始</span></span><br><span class="line">        <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">Row</span>] = df2.rdd</span><br><span class="line">        <span class="comment">//rdd2.foreach(a=&gt;println(a.getString(1)))</span></span><br><span class="line">        <span class="comment">//*****RDD=&gt;DataSet*****</span></span><br><span class="line">        rdd1.map&#123;</span><br><span class="line">        	<span class="keyword">case</span> (id,name,age)=&gt;<span class="type">User</span>(id,name,age)</span><br><span class="line">        &#125;.toDS()</span><br><span class="line">        <span class="comment">//*****DataSet=&gt;=&gt;RDD*****</span></span><br><span class="line">        ds1.rdd</span><br><span class="line">        <span class="comment">//释放资源</span></span><br><span class="line">        spark.stop()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">id:<span class="type">Int</span>,name:<span class="type">String</span>,age:<span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure>

<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.spark.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Dataset</span>, <span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark01_SparkSQL_Basic</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// TODO 基础操作</span></span><br><span class="line">    <span class="comment">// TODO 1. DataFrame</span></span><br><span class="line">    <span class="comment">// val df = spark.read.json("datas/user.json")</span></span><br><span class="line">    <span class="comment">// df.show()</span></span><br><span class="line">    <span class="comment">// 1.1 DataFrame =&gt; SQL</span></span><br><span class="line">    <span class="comment">// df.createOrReplaceTempView("user")</span></span><br><span class="line">    <span class="comment">// spark.sql("select * from user").show</span></span><br><span class="line">    <span class="comment">// spark.sql("select username, age from user").show</span></span><br><span class="line">    <span class="comment">// spark.sql("select avg(age) from user").show</span></span><br><span class="line">    <span class="comment">// 1.2 DataFrame =&gt; DSL</span></span><br><span class="line">    <span class="comment">// 在使用DatFrame时，如果涉及到转换操作，需要引入转换规则,spark在此是变量的名称，注意不是包名哦</span></span><br><span class="line">    <span class="comment">// df.select("username", "age").show()</span></span><br><span class="line">    <span class="comment">// df.select($"age" + 1).show()</span></span><br><span class="line">    <span class="comment">// df.select('age + 1).show()</span></span><br><span class="line">    <span class="comment">// df.printSchema()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 2. DataSet</span></span><br><span class="line">    <span class="comment">// DataFrame 其实是特定泛型的DataSet,所以DataFrame的方法DataSet也能够使用</span></span><br><span class="line">    <span class="comment">// val set = Seq(1,2,3,4)</span></span><br><span class="line">    <span class="comment">// val ds = set.toDS()</span></span><br><span class="line">    <span class="comment">// ds.show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 3. RDD &lt;=&gt; DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> rdd = spark.sparkContext.makeRDD(<span class="type">List</span>((<span class="number">1</span>,<span class="string">"zhangsan"</span>,<span class="number">20</span>),(<span class="number">2</span>,<span class="string">"lisi"</span>,<span class="number">21</span>)))</span><br><span class="line">    <span class="keyword">val</span> df = rdd.toDF(<span class="string">"id"</span>,<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">    df.show()</span><br><span class="line">    <span class="keyword">val</span> rowRDD: <span class="type">RDD</span>[<span class="type">Row</span>] = df.rdd</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 4. DataFrame &lt;=&gt; DataSet</span></span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User</span>] = df.as[<span class="type">User</span>]</span><br><span class="line">    <span class="keyword">val</span> df1 = ds.toDF()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 5. RDD &lt;=&gt; DataSet</span></span><br><span class="line">    <span class="keyword">val</span> ds1: <span class="type">Dataset</span>[<span class="type">User</span>] = rdd.map &#123;</span><br><span class="line">      <span class="keyword">case</span> (id, name, age) =&gt; &#123;</span><br><span class="line">        <span class="type">User</span>(id, name, age)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.toDS()</span><br><span class="line">    <span class="keyword">val</span> userRDD: <span class="type">RDD</span>[<span class="type">User</span>] = ds1.rdd</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">id: <span class="type">Int</span>, name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>

<h2 id="2-7-用户自定义函数"><a href="#2-7-用户自定义函数" class="headerlink" title="2.7 用户自定义函数"></a>2.7 用户自定义函数</h2><p>用户可以通过 spark.udf 功能添加自定义函数， 实现自定义功能。  </p>
<h3 id="2-7-1-UDF"><a href="#2-7-1-UDF" class="headerlink" title="2.7.1 UDF"></a>2.7.1 UDF</h3><p>1) 创建 DataFrame  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">"data/user.json"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， username: string]</span><br></pre></td></tr></table></figure>

<p>2) 注册 UDF  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.udf.register(<span class="string">"addName"</span>,(x:<span class="type">String</span>)=&gt; <span class="string">"Name:"</span>+x)</span><br><span class="line">res9: org.apache.spark.sql.expressions.<span class="type">UserDefinedFunction</span> = <span class="type">UserDefinedFunction</span>(&lt;function1&gt;,<span class="type">StringType</span>,<span class="type">Some</span>(<span class="type">List</span>(<span class="type">StringType</span>)))</span><br></pre></td></tr></table></figure>

<p>3) 创建临时表  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br></pre></td></tr></table></figure>

<p>4) 应用 UDF  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">"Select addName(name),age from people"</span>).show()</span><br></pre></td></tr></table></figure>

<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.spark.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark02_SparkSQL_UDF</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">    <span class="comment">// TODO 注册自定义的UDF函数</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.json(<span class="string">"datas/user.json"</span>)</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"user"</span>)</span><br><span class="line">    spark.udf.register(<span class="string">"prefixName"</span>,(name: <span class="type">String</span>) =&gt; &#123;<span class="string">"Name: "</span> + name&#125;)</span><br><span class="line">    spark.sql(<span class="string">"select prefixName(username) from user"</span>).show()</span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-7-2-UDAF"><a href="#2-7-2-UDAF" class="headerlink" title="2.7.2 UDAF"></a>2.7.2 UDAF</h3><p>强类型的 Dataset 和弱类型的 DataFrame 都提供了相关的聚合函数， 如 count()，countDistinct()， avg()， max()， min()。除此之外，用户可以设定自己的自定义聚合函数。通过继承 UserDefinedAggregateFunction 来实现用户自定义弱类型聚合函数。 从 Spark3.0 版本后， UserDefinedAggregateFunction 已经不推荐使用了。可以统一采用强类型聚合函数 Aggregator  </p>
<p>需求： 计算平均年龄 </p>
<p>一个需求可以采用很多种不同的方法实现需求</p>
<p><img src="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240614155619334.png" alt="image-20240614155619334"></p>
<h4 id="2-7-2-1-实现方式-RDD"><a href="#2-7-2-1-实现方式-RDD" class="headerlink" title="2.7.2.1 实现方式 - RDD"></a>2.7.2.1 实现方式 - RDD</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"app"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"><span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> res: (<span class="type">Int</span>, <span class="type">Int</span>) = sc.makeRDD(<span class="type">List</span>((<span class="string">"zhangsan"</span>, <span class="number">20</span>), (<span class="string">"lisi"</span>, <span class="number">30</span>), (<span class="string">"wangw"</span>, <span class="number">40</span>))).map &#123;</span><br><span class="line">	<span class="keyword">case</span> (name, age) =&gt; &#123;</span><br><span class="line">		(age, <span class="number">1</span>)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;.reduce &#123;</span><br><span class="line">	(t1, t2) =&gt; &#123;</span><br><span class="line">		(t1._1 + t2._1, t1._2 + t2._2)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">println(res._1 / res._2)</span><br><span class="line"><span class="comment">// 关闭连接</span></span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure>

<h4 id="2-7-2-2-实现方式-累加器"><a href="#2-7-2-2-实现方式-累加器" class="headerlink" title="2.7.2.2 实现方式 - 累加器"></a>2.7.2.2 实现方式 - 累加器</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAC</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">Int</span>,<span class="type">Int</span>]</span>&#123;</span><br><span class="line">    <span class="keyword">var</span> sum:<span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">    <span class="keyword">var</span> count:<span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    	<span class="keyword">return</span> sum ==<span class="number">0</span> &amp;&amp; count == <span class="number">0</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">Int</span>, <span class="type">Int</span>] = &#123;</span><br><span class="line">        <span class="keyword">val</span> newMyAc = <span class="keyword">new</span> <span class="type">MyAC</span></span><br><span class="line">        newMyAc.sum = <span class="keyword">this</span>.sum</span><br><span class="line">        newMyAc.count = <span class="keyword">this</span>.count</span><br><span class="line">        newMyAc</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        sum =<span class="number">0</span></span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(v: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        sum += v</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">Int</span>, <span class="type">Int</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        other <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> o:<span class="type">MyAC</span>=&gt;&#123;</span><br><span class="line">                sum += o.sum</span><br><span class="line">                count += o.count</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> _=&gt;</span><br><span class="line">    	&#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: <span class="type">Int</span> = sum/count</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2-7-2-3-实现方式-UDAF-弱类型-（新版spark不推荐使用）"><a href="#2-7-2-3-实现方式-UDAF-弱类型-（新版spark不推荐使用）" class="headerlink" title="2.7.2.3 实现方式 - UDAF - 弱类型 （新版spark不推荐使用）"></a>2.7.2.3 实现方式 - UDAF - 弱类型 （新版spark不推荐使用）</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">定义类继承 UserDefinedAggregateFunction，并重写其中方法</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAveragUDAF</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 聚合函数输入参数的数据类型</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">"age"</span>,<span class="type">IntegerType</span>)))</span><br><span class="line">    <span class="comment">// 聚合函数缓冲区中值的数据类型(age,count)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    	<span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">"sum"</span>,<span class="type">LongType</span>),<span class="type">StructField</span>(<span class="string">"count"</span>,<span class="type">LongType</span>)))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 函数返回值的数据类型</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line">    <span class="comment">// 稳定性：对于相同的输入是否一直返回相同的输出。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line">    <span class="comment">// 函数缓冲区初始化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 存年龄的总和</span></span><br><span class="line">        buffer(<span class="number">0</span>) = <span class="number">0</span>L</span><br><span class="line">        <span class="comment">// 存年龄的个数</span></span><br><span class="line">        buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">	&#125;</span><br><span class="line"><span class="comment">// 更新缓冲区中的数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>,input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">            buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + input.getInt(<span class="number">0</span>)</span><br><span class="line">            buffer(<span class="number">1</span>) = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 合并缓冲区</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>,buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>)</span><br><span class="line">        buffer1(<span class="number">1</span>) = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 计算最终结果	</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Double</span> = buffer.getLong(<span class="number">0</span>).toDouble / buffer.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line">。。。</span><br><span class="line"><span class="comment">//创建聚合函数</span></span><br><span class="line"><span class="keyword">var</span> myAverage = <span class="keyword">new</span> <span class="type">MyAveragUDAF</span></span><br><span class="line"><span class="comment">//在 spark 中注册聚合函数</span></span><br><span class="line">spark.udf.register(<span class="string">"avgAge"</span>,myAverage)</span><br><span class="line">spark.sql(<span class="string">"select avgAge(age) from user"</span>).show()</span><br></pre></td></tr></table></figure>

<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.spark.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">DataType</span>, <span class="type">LongType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark03_SparkSQL_UDAF</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">    <span class="comment">// TODO 注册自定义的弱类型UDF函数（新版spark不推荐使用）</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.json(<span class="string">"datas/user.json"</span>)</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"user"</span>)</span><br><span class="line">    spark.udf.register(<span class="string">"ageAvg"</span>, <span class="keyword">new</span> <span class="type">MyAvgUDAF</span>())</span><br><span class="line">    spark.sql(<span class="string">"select ageAvg(age) from user"</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">   * 自定义聚合函数类：计算年龄的平均值</span></span><br><span class="line"><span class="comment">   * 1、集成UserDefinedAggregateFunction</span></span><br><span class="line"><span class="comment">   * 2、重写方法(8)</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyAvgUDAF</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 输入的类型-输入</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">      <span class="type">StructType</span>(</span><br><span class="line">        <span class="type">Array</span>(</span><br><span class="line">          <span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">LongType</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 缓冲区数据计算的结构-缓冲区</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">      <span class="type">StructType</span>(</span><br><span class="line">        <span class="type">Array</span>(</span><br><span class="line">          <span class="type">StructField</span>(<span class="string">"total"</span>, <span class="type">LongType</span>),</span><br><span class="line">          <span class="type">StructField</span>(<span class="string">"count"</span>, <span class="type">LongType</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 函数计算结果的数据类型-输出</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = &#123;</span><br><span class="line">      <span class="type">LongType</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 函数的稳定性：传入相同的参数，结果是否相同</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 缓冲区初始化</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// buffer(0) = 0</span></span><br><span class="line">      <span class="comment">// buffer(1) = 0</span></span><br><span class="line">      <span class="comment">// 或者</span></span><br><span class="line">      buffer.update(<span class="number">0</span>,<span class="number">0</span>L)</span><br><span class="line">      buffer.update(<span class="number">1</span>,<span class="number">0</span>L)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据输入的值更新缓冲区数据</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      buffer.update(<span class="number">0</span>,buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>))</span><br><span class="line">      buffer.update(<span class="number">1</span>, buffer.getLong(<span class="number">1</span>) + <span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 缓冲区数据合并</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      buffer1.update(<span class="number">0</span>, buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>))</span><br><span class="line">      buffer1.update(<span class="number">1</span>, buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算平均值</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">      buffer.getLong(<span class="number">0</span>) / buffer.getLong(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2-7-2-4-实现方式-UDAF-强类型-（新版Spark3-0之后出现的，推荐使用）"><a href="#2-7-2-4-实现方式-UDAF-强类型-（新版Spark3-0之后出现的，推荐使用）" class="headerlink" title="2.7.2.4 实现方式 - UDAF - 强类型 （新版Spark3.0之后出现的，推荐使用）"></a>2.7.2.4 实现方式 - UDAF - 强类型 （新版Spark3.0之后出现的，推荐使用）</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TODO 创建 UDAF 函数</span></span><br><span class="line"><span class="keyword">val</span> udaf = <span class="keyword">new</span> <span class="type">MyAvgAgeUDAF</span></span><br><span class="line"><span class="comment">// TODO 注册到 SparkSQL 中</span></span><br><span class="line">spark.udf.register(<span class="string">"avgAge"</span>, functions.udaf(udaf))</span><br><span class="line"><span class="comment">// TODO 在 SQL 中使用聚合函数</span></span><br><span class="line"><span class="comment">// 定义用户的自定义聚合函数</span></span><br><span class="line">spark.sql(<span class="string">"select avgAge(age) from user"</span>).show</span><br><span class="line"><span class="comment">// **************************************************</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Buff</span>(<span class="params"> var sum:<span class="type">Long</span>, var cnt:<span class="type">Long</span> </span>)</span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">totalage</span>, <span class="title">count</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">MyAvgAgeUDAF</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Long</span>, <span class="type">Buff</span>, <span class="type">Double</span>]</span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Buff</span> = <span class="type">Buff</span>(<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: <span class="type">Buff</span>, a: <span class="type">Long</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">        b.sum += a</span><br><span class="line">        b.cnt += <span class="number">1</span></span><br><span class="line">        b</span><br><span class="line">	&#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">Buff</span>, b2: <span class="type">Buff</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">        b1.sum += b2.sum</span><br><span class="line">        b1.cnt += b2.cnt</span><br><span class="line">        b1</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">Buff</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">    	reduction.sum.toDouble/reduction.cnt</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Buff</span>] = <span class="type">Encoders</span>.product</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.spark.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Encoder</span>, <span class="type">Encoders</span>, <span class="type">SparkSession</span>, functions&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark03_SparkSQL_UDAF1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">    <span class="comment">// TODO 注册自定义的强类型UDF函数（新版Spark3.0之后出现的，推荐使用）</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.json(<span class="string">"datas/user.json"</span>)</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"user"</span>)</span><br><span class="line">    <span class="comment">// 增加了转换功能，将强类型函数转换为弱类型操作</span></span><br><span class="line">    spark.udf.register(<span class="string">"ageAvg"</span>, functions.udaf(<span class="keyword">new</span> <span class="type">MyAvgUDAF</span>()))</span><br><span class="line">    spark.sql(<span class="string">"select ageAvg(age) from user"</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">   * 自定义聚合函数类：计算年龄的平均值</span></span><br><span class="line"><span class="comment">   * 1、集成 org.apache.spark.sql.expressions.Aggregator, 定义泛型</span></span><br><span class="line"><span class="comment">   *    IN: 输入的数据类型 Long</span></span><br><span class="line"><span class="comment">   *    BUF: 缓冲区的数据类型 Buff</span></span><br><span class="line"><span class="comment">   *    OUT: 输出的数据类型 Long</span></span><br><span class="line"><span class="comment">   * 2、重写方法(6)</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Buff</span>(<span class="params">var total: <span class="type">Long</span>, var count: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">class</span> <span class="title">MyAvgUDAF</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Long</span>,<span class="type">Buff</span>,<span class="type">Long</span>]</span>&#123;</span><br><span class="line">    <span class="comment">// z &amp; zero 初始值或零值</span></span><br><span class="line">    <span class="comment">// 缓冲区的初始化</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Buff</span> = &#123;</span><br><span class="line">      <span class="type">Buff</span>(<span class="number">0</span>L,<span class="number">0</span>L)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据输入的数据来更新缓冲区的数据</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buff: <span class="type">Buff</span>, in: <span class="type">Long</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">      buff.total = buff.total + in</span><br><span class="line">      buff.count = buff.count + <span class="number">1</span></span><br><span class="line">      buff</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 合并缓冲区</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buff1: <span class="type">Buff</span>, buff2: <span class="type">Buff</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">      buff1.total = buff1.total + buff2.total</span><br><span class="line">      buff1.count = buff1.count + buff2.count</span><br><span class="line">      buff1</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算结果</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(buff: <span class="type">Buff</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">      buff.total / buff.count</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 缓冲区的编码操作</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Buff</span>] = &#123;</span><br><span class="line">      <span class="comment">// 缓冲区自定义的固定写法</span></span><br><span class="line">      <span class="type">Encoders</span>.product</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输出的编码操作</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Long</span>] = &#123;</span><br><span class="line">      <span class="comment">// scala自带的固定写法</span></span><br><span class="line">      <span class="type">Encoders</span>.scalaLong</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2-7-2-5-实现方式-UDAF-早期版本进行强类型函数实现"><a href="#2-7-2-5-实现方式-UDAF-早期版本进行强类型函数实现" class="headerlink" title="2.7.2.5 实现方式 - UDAF - 早期版本进行强类型函数实现"></a>2.7.2.5 实现方式 - UDAF - 早期版本进行强类型函数实现</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//输入数据类型</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User01</span>(<span class="params">username:<span class="type">String</span>,age:<span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">//缓存类型</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">AgeBuffer</span>(<span class="params">var sum:<span class="type">Long</span>,var count:<span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">/**</span></span></span><br><span class="line"><span class="class"><span class="title">*</span> <span class="title">定义类继承</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">sql</span>.<span class="title">expressions</span>.<span class="title">Aggregator</span></span></span><br><span class="line"><span class="class"><span class="title">*</span> <span class="title">重写类中的方法</span></span></span><br><span class="line"><span class="class"><span class="title">*/</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">MyAveragUDAF1</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">User01</span>,<span class="type">AgeBuffer</span>,<span class="type">Double</span>]</span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">AgeBuffer</span> = &#123;</span><br><span class="line">    <span class="type">Age</span>	<span class="type">Buffer</span>(<span class="number">0</span>L,<span class="number">0</span>L)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: <span class="type">AgeBuffer</span>, a: <span class="type">User01</span>): <span class="type">AgeBuffer</span> = &#123;</span><br><span class="line">        b.sum = b.sum + a.age</span><br><span class="line">        b.count = b.count + <span class="number">1</span></span><br><span class="line">        b</span><br><span class="line">    &#125;	</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">AgeBuffer</span>, b2: <span class="type">AgeBuffer</span>): <span class="type">AgeBuffer</span> = &#123;</span><br><span class="line">        b1.sum = b1.sum + b2.sum</span><br><span class="line">        b1.count = b1.count + b2.count</span><br><span class="line">        b1</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(buff: <span class="type">AgeBuffer</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">    	buff.sum.toDouble/buff.count</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//DataSet 默认额编解码器，用于序列化，固定写法</span></span><br><span class="line">    <span class="comment">//自定义类型就是 product 自带类型根据类型选择</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">AgeBuffer</span>] = &#123;</span><br><span class="line">    	<span class="type">Encoders</span>.product</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    	<span class="type">Encoders</span>.scalaDouble</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">。。。</span><br><span class="line"><span class="comment">//封装为 DataSet</span></span><br><span class="line"><span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User01</span>] = df.as[<span class="type">User01</span>]</span><br><span class="line"><span class="comment">//创建聚合函数</span></span><br><span class="line"><span class="keyword">var</span> myAgeUdaf1 = <span class="keyword">new</span> <span class="type">MyAveragUDAF1</span></span><br><span class="line"><span class="comment">//将聚合函数转换为查询的列</span></span><br><span class="line"><span class="keyword">val</span> col: <span class="type">TypedColumn</span>[<span class="type">User01</span>, <span class="type">Double</span>] = myAgeUdaf1.toColumn</span><br><span class="line"><span class="comment">//查询</span></span><br><span class="line">ds.select(col).show()</span><br></pre></td></tr></table></figure>

<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.spark.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Dataset</span>, <span class="type">Encoder</span>, <span class="type">Encoders</span>, <span class="type">SparkSession</span>, <span class="type">TypedColumn</span>, functions&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark03_SparkSQL_UDAF2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// TODO 早期版本进行强类型函数实现</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.json(<span class="string">"datas/user.json"</span>)</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"user"</span>)</span><br><span class="line">    <span class="comment">// 早期版本中，spark不能在sql中使用强类型UDAF操作</span></span><br><span class="line">    <span class="comment">// 早期的UDAF强类型聚合函数使用DSL语法操作</span></span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User</span>] = df.as[<span class="type">User</span>]</span><br><span class="line">    <span class="comment">// 将UDAF函数转换为查询的列对象</span></span><br><span class="line">    <span class="keyword">val</span> udafCol: <span class="type">TypedColumn</span>[<span class="type">User</span>, <span class="type">Long</span>] = <span class="keyword">new</span> <span class="type">MyAvgUDAF</span>().toColumn</span><br><span class="line">    ds.select(udafCol).show()</span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">   * 自定义聚合函数类：计算年龄的平均值</span></span><br><span class="line"><span class="comment">   * 1、集成 org.apache.spark.sql.expressions.Aggregator, 定义泛型</span></span><br><span class="line"><span class="comment">   *    IN: 输入的数据类型 Long</span></span><br><span class="line"><span class="comment">   *    BUF: 缓冲区的数据类型 Buff</span></span><br><span class="line"><span class="comment">   *    OUT: 输出的数据类型 Long</span></span><br><span class="line"><span class="comment">   * 2、重写方法(6)</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">username: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class">  <span class="title">case</span> <span class="title">class</span> <span class="title">Buff</span>(<span class="params">var total: <span class="type">Long</span>, var count: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">class</span> <span class="title">MyAvgUDAF</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">User</span>,<span class="type">Buff</span>,<span class="type">Long</span>]</span>&#123;</span><br><span class="line">    <span class="comment">// z &amp; zero 初始值或零值</span></span><br><span class="line">    <span class="comment">// 缓冲区的初始化</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Buff</span> = &#123;</span><br><span class="line">      <span class="type">Buff</span>(<span class="number">0</span>L,<span class="number">0</span>L)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据输入的数据来更新缓冲区的数据</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buff: <span class="type">Buff</span>, in: <span class="type">User</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">      buff.total = buff.total + in.age</span><br><span class="line">      buff.count = buff.count + <span class="number">1</span></span><br><span class="line">      buff</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 合并缓冲区</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buff1: <span class="type">Buff</span>, buff2: <span class="type">Buff</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">      buff1.total = buff1.total + buff2.total</span><br><span class="line">      buff1.count = buff1.count + buff2.count</span><br><span class="line">      buff1</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算结果</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(buff: <span class="type">Buff</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">      buff.total / buff.count</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 缓冲区的编码操作</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Buff</span>] = &#123;</span><br><span class="line">      <span class="comment">// 缓冲区自定义的固定写法</span></span><br><span class="line">      <span class="type">Encoders</span>.product</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输出的编码操作</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Long</span>] = &#123;</span><br><span class="line">      <span class="comment">// scala自带的固定写法</span></span><br><span class="line">      <span class="type">Encoders</span>.scalaLong</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="2-8-数据的加载和保存"><a href="#2-8-数据的加载和保存" class="headerlink" title="2.8 数据的加载和保存"></a>2.8 数据的加载和保存</h2><h3 id="2-8-1-通用的加载和保存方式"><a href="#2-8-1-通用的加载和保存方式" class="headerlink" title="2.8.1 通用的加载和保存方式"></a>2.8.1 通用的加载和保存方式</h3><p>SparkSQL 提供了通用的保存数据和数据加载的方式。这里的通用指的是使用相同的API，根据不同的参数读取和保存不同格式的数据， SparkSQL 默认读取和保存的文件格式为 parquet</p>
<p>1) 加载数据  </p>
<p>   spark.read.load 是加载数据的通用方法</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.read.</span><br><span class="line">csv format jdbc json load option options orc parquet schema</span><br><span class="line">table text textFile</span><br></pre></td></tr></table></figure>

<p>   如果读取不同格式的数据，可以对不同的数据格式进行设定</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.read.format(<span class="string">"…"</span>)[.option(<span class="string">"…"</span>)].load(<span class="string">"…"</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>format(“…”)：指定加载的数据类型，包括”csv”、 “jdbc”、 “json”、 “orc”、 “parquet” 和 “textFile”。</p>
</li>
<li><p>load(“…”)：在”csv”、 “jdbc”、 “json”、 “orc”、 “parquet”和”textFile”格式下需要传入加载数据的路径。</p>
</li>
<li><p>option(“…”)：在”jdbc”格式下需要传入 JDBC 相应参数， url、 user、 password 和 dbtable我们前面都是使用 read API 先把文件加载到 DataFrame 然后再查询，其实，我们也可以直接在文件上进行查询：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;spark.sql(<span class="string">"select * from json.`/opt/module/data/user.json`"</span>).show</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>2) 保存数据</p>
<p>   df.write.save 是保存数据的通用方法</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;df.write.</span><br><span class="line">csv jdbc json orc parquet textFile… …</span><br></pre></td></tr></table></figure>

<p>   如果保存不同格式的数据，可以对不同的数据格式进行设定  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;df.write.format(<span class="string">"…"</span>)[.option(<span class="string">"…"</span>)].save(<span class="string">"…"</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>format(“…”)：指定保存的数据类型，包括”csv”、 “jdbc”、 “json”、 “orc”、 “parquet” 和 “textFile”。</p>
</li>
<li><p>save (“…”)：在”csv”、 “orc”、 “parquet”和”textFile”格式下需要传入保存数据的路径。</p>
</li>
<li><p>option(“…”)：在”jdbc”格式下需要传入 JDBC 相应参数， url、 user、 password 和 dbtable保存操作</p>
</li>
<li><p>可以使用 SaveMode, 用来指明如何处理数据，使用 mode()方法来设置。有一点很重要: 这些 SaveMode 都是没有加锁的, 也不是原子操作。</p>
<p>SaveMode 是一个枚举类，其中的常量包括：  </p>
<table>
<thead>
<tr>
<th>Scala/Java</th>
<th>Any Language</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td>SaveMode.ErrorIfExists(default)</td>
<td>“error”(default)</td>
<td>如果文件已经存在则抛出异常</td>
</tr>
<tr>
<td>SaveMode.Append</td>
<td>“append”</td>
<td>如果文件已经存在则追加</td>
</tr>
<tr>
<td>SaveMode.Overwrite</td>
<td>“overwrite”</td>
<td>如果文件已经存在则覆盖</td>
</tr>
<tr>
<td>SaveMode.Ignore</td>
<td>“ignore”</td>
<td>如果文件已经存在则忽略</td>
</tr>
</tbody></table>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.mode(<span class="string">"append"</span>).json(<span class="string">"/opt/module/data/output"</span>)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="2-8-2-Parquet"><a href="#2-8-2-Parquet" class="headerlink" title="2.8.2 Parquet"></a>2.8.2 Parquet</h3><p>Spark SQL 的默认数据源为 Parquet 格式。 Parquet 是一种能够有效存储嵌套数据的列式存储格式。<br>数据源为 Parquet 文件时， Spark SQL 可以方便的执行所有的操作，不需要使用 format。修改配置项 spark.sql.sources.default，可修改默认数据源格式。  </p>
<p>1) 加载数据  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.load(<span class="string">"examples/src/main/resources/users.parquet"</span>)</span><br><span class="line">scala&gt; df.show</span><br></pre></td></tr></table></figure>

<p>2) 保存数据  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> df = spark.read.json(<span class="string">"/opt/module/data/input/people.json"</span>)</span><br><span class="line"><span class="comment">//保存为 parquet 格式</span></span><br><span class="line">scala&gt; df.write.mode(<span class="string">"append"</span>).save(<span class="string">"/opt/module/data/output"</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-8-3-JSON"><a href="#2-8-3-JSON" class="headerlink" title="2.8.3 JSON"></a>2.8.3 JSON</h3><p>Spark SQL 能够自动推测 JSON 数据集的结构，并将它加载为一个 Dataset[Row]. 可以通过 SparkSession.read.json()去加载 JSON 文件。  </p>
<p><strong>注意：</strong> Spark 读取的 JSON 文件不是传统的 JSON 文件，每一行都应该是一个 JSON 串。格式如下：  </p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Michael"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Andy"</span>, <span class="attr">"age"</span>:<span class="number">30</span>&#125;</span><br><span class="line">[&#123;<span class="attr">"name"</span>:<span class="string">"Justin"</span>, <span class="attr">"age"</span>:<span class="number">19</span>&#125;,&#123;<span class="attr">"name"</span>:<span class="string">"Justin"</span>, <span class="attr">"age"</span>:<span class="number">19</span>&#125;]</span><br></pre></td></tr></table></figure>

<p>1）导入隐式转换  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure>

<p>2）加载 JSON 文件  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"/opt/module/spark-local/people.json"</span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.read.json(path)</span><br></pre></td></tr></table></figure>

<p>3）创建临时表  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br></pre></td></tr></table></figure>

<p>4）数据查询  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> teenagerNamesDF = spark.sql(<span class="string">"SELECT name FROM people WHERE age BETWEEN 13</span></span><br><span class="line"><span class="string">AND 19"</span>)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line">+------+</span><br><span class="line">| name|</span><br><span class="line">+------+</span><br><span class="line">|<span class="type">Justin</span>|</span><br><span class="line">+------+</span><br></pre></td></tr></table></figure>

<h3 id="2-8-4-CSV"><a href="#2-8-4-CSV" class="headerlink" title="2.8.4 CSV"></a>2.8.4 CSV</h3><p>Spark SQL 可以配置 CSV 文件的列表信息，读取 CSV 文件,CSV 文件的第一行设置为数据列</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">"csv"</span>).option(<span class="string">"sep"</span>, <span class="string">";"</span>).option(<span class="string">"inferSchema"</span>, <span class="string">"true"</span>).option(<span class="string">"header"</span>, <span class="string">"true"</span>).load(<span class="string">"data/user.csv"</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-8-5-MySQL"><a href="#2-8-5-MySQL" class="headerlink" title="2.8.5 MySQL"></a>2.8.5 MySQL</h3><p>Spark SQL 可以通过 JDBC 从关系型数据库中读取数据的方式创建 DataFrame，通过对DataFrame 一系列的计算后，还可以将数据再写回关系型数据库中。如果使用 spark-shell 操作，可在启动 shell 时指定相关的数据库驱动路径或者将相关的数据库驱动放到 spark 的类路径下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure>

<p>我们这里只演示在 Idea 中通过 JDBC 对 Mysql 进行操作  </p>
<p>1）导入依赖  </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2） 读取数据  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkSQL"</span>)</span><br><span class="line"><span class="comment">//创建 SparkSession 对象</span></span><br><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(conf).getOrCreate()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="comment">//方式 1：通用的 load 方法读取</span></span><br><span class="line">spark.read.format(<span class="string">"jdbc"</span>)</span><br><span class="line">    .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://linux1:3306/spark-sql"</span>)</span><br><span class="line">    .option(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">    .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">    .option(<span class="string">"password"</span>, <span class="string">"123123"</span>)</span><br><span class="line">    .option(<span class="string">"dbtable"</span>, <span class="string">"user"</span>)</span><br><span class="line">    .load().show</span><br><span class="line"><span class="comment">//方式 2:通用的 load 方法读取 参数另一种形式</span></span><br><span class="line">spark.read.format(<span class="string">"jdbc"</span>)</span><br><span class="line">	.options(<span class="type">Map</span>(<span class="string">"url"</span>-&gt;<span class="string">"jdbc:mysql://linux1:3306/spark-sql?user=root&amp;password=123123"</span>, <span class="string">"dbtable"</span>-&gt;<span class="string">"user"</span>,<span class="string">"driver"</span>-&gt;<span class="string">"com.mysql.jdbc.Driver"</span>))</span><br><span class="line">	.load().show</span><br><span class="line"><span class="comment">//方式 3:使用 jdbc 方法读取</span></span><br><span class="line"><span class="keyword">val</span> props: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">props.setProperty(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">props.setProperty(<span class="string">"password"</span>, <span class="string">"123123"</span>)</span><br><span class="line"><span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.jdbc(<span class="string">"jdbc:mysql://linux1:3306/spark-sql"</span>, <span class="string">"user"</span>, props)</span><br><span class="line">df.show</span><br><span class="line"><span class="comment">//释放资源</span></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<p>3） 写入数据  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User2</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">。。。</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">conf</span></span>: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkSQL"</span>)</span><br><span class="line"><span class="comment">//创建 SparkSession 对象</span></span><br><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(conf).getOrCreate()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">User2</span>] = spark.sparkContext.makeRDD(<span class="type">List</span>(<span class="type">User2</span>(<span class="string">"lisi"</span>, <span class="number">20</span>), <span class="type">User2</span>(<span class="string">"zs"</span>, <span class="number">30</span>)))</span><br><span class="line"><span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User2</span>] = rdd.toDS</span><br><span class="line"><span class="comment">//方式 1：通用的方式 format 指定写出类型</span></span><br><span class="line">ds.write</span><br><span class="line">    .format(<span class="string">"jdbc"</span>)</span><br><span class="line">    .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://linux1:3306/spark-sql"</span>)</span><br><span class="line">    .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">    .option(<span class="string">"password"</span>, <span class="string">"123123"</span>)</span><br><span class="line">    .option(<span class="string">"dbtable"</span>, <span class="string">"user"</span>)</span><br><span class="line">    .mode(<span class="type">SaveMode</span>.<span class="type">Append</span>)</span><br><span class="line">    .save()</span><br><span class="line"><span class="comment">//方式 2：通过 jdbc 方法</span></span><br><span class="line"><span class="keyword">val</span> props: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">props.setProperty(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">props.setProperty(<span class="string">"password"</span>, <span class="string">"123123"</span>)</span><br><span class="line">ds.write.mode(<span class="type">SaveMode</span>.<span class="type">Append</span>).jdbc(<span class="string">"jdbc:mysql://linux1:3306/spark-sql"</span>, <span class="string">"user"</span>, props)</span><br><span class="line"><span class="comment">//释放资源</span></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.spark.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark04_SparkSQL_JDBC</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// TODO 读取Mysql数据</span></span><br><span class="line">    <span class="comment">// TODO 方式1：通过load方法读取</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.read</span><br><span class="line">      .format(<span class="string">"jdbc"</span>)</span><br><span class="line">      .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://10.8.106.108:3306/test"</span>)</span><br><span class="line">      .option(<span class="string">"driver"</span>, <span class="string">"com.mysql.cj.jdbc.Driver"</span>)</span><br><span class="line">      .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">      .option(<span class="string">"password"</span>, <span class="string">"Nucleus0755!"</span>)</span><br><span class="line">      .option(<span class="string">"dbtable"</span>, <span class="string">"user"</span>)</span><br><span class="line">      .load()</span><br><span class="line">    df.show</span><br><span class="line">    <span class="comment">//println("=======================")</span></span><br><span class="line">    <span class="comment">// TODO 方式 2:通用的 load 方法读取 参数另一种形式</span></span><br><span class="line">    <span class="comment">//val df1 = spark.read</span></span><br><span class="line">    <span class="comment">//  .format("jdbc")</span></span><br><span class="line">    <span class="comment">//  .options(Map("url" -&gt; "jdbc:mysql://10.8.106.108:3306/test?user=root&amp;password=Nucleus0755!","dbtable" -&gt; "user","driver" -&gt; "com.mysql.cj.jdbc.Driver"))</span></span><br><span class="line">    <span class="comment">//  .load()</span></span><br><span class="line">    <span class="comment">//df1.show</span></span><br><span class="line">    <span class="comment">//println("=======================")</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 方式 3:使用 jdbc 方法读取</span></span><br><span class="line">    <span class="comment">//val props: Properties = new Properties()</span></span><br><span class="line">    <span class="comment">//props.setProperty("user", "root")</span></span><br><span class="line">    <span class="comment">//props.setProperty("password", "Nucleus0755!")</span></span><br><span class="line">    <span class="comment">//val df2: DataFrame = spark.read</span></span><br><span class="line">    <span class="comment">//  .jdbc("jdbc:mysql://10.8.106.108:3306/test", "user", props)</span></span><br><span class="line">    <span class="comment">//df2.show</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 保存Mysql数据</span></span><br><span class="line">    df.write</span><br><span class="line">      .format(<span class="string">"jdbc"</span>)</span><br><span class="line">      .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://10.8.106.108:3306/test"</span>)</span><br><span class="line">      .option(<span class="string">"driver"</span>, <span class="string">"com.mysql.cj.jdbc.Driver"</span>)</span><br><span class="line">      .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">      .option(<span class="string">"password"</span>, <span class="string">"Nucleus0755!"</span>)</span><br><span class="line">      .option(<span class="string">"dbtable"</span>, <span class="string">"user1"</span>)</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Append</span>)</span><br><span class="line">      .save()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-8-6-Hive"><a href="#2-8-6-Hive" class="headerlink" title="2.8.6 Hive"></a>2.8.6 Hive</h3><p>Apache Hive 是 Hadoop 上的 SQL 引擎， Spark SQL 编译时可以包含 Hive 支持，也可以不包含。包含 Hive 支持的 Spark SQL 可以支持 Hive 表访问、 UDF (用户自定义函数)  以及 Hive 查询语言(HiveQL/HQL)等。需要强调的一点是，如果要在 Spark SQL 中包含Hive 的库，并不需要事先安装 Hive。一般来说，最好还是在编译 Spark SQL 时引入 Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。  </p>
<p>若要把 Spark SQL 连接到一个部署好的 Hive 上，你必须把 hive-site.xml 复制到Spark 的配置文件目录中($SPARK_HOME/conf)。即使没有部署好 Hive， Spark SQL 也可以运行。 需要注意的是，如果你没有部署好 Hive， Spark SQL 会在当前的工作目录中创建出自己的 Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。</p>
<p>`spark-shell 默认是 Hive 支持的；代码中是默认不支持的，需要手动指定（加一个参数即可）。  </p>
<p>1） 内嵌的 HIVE  </p>
<p>如果使用 Spark 内嵌的 Hive, 则什么都不用做, 直接使用即可.<br>Hive 的元数据存储在 derby 中, 默认仓库地址:$SPARK_HOME/spark-warehouse  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">"show tables"</span>).show</span></span><br><span class="line">。。。</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">"create table aa(id int)"</span>)</span></span><br><span class="line">。。。</span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">"show tables"</span>).show</span></span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">| default| aa| false|</span><br><span class="line">+--------+---------+-----------+</span><br></pre></td></tr></table></figure>

<p>向表加载本地数据  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">"load data local inpath 'input/ids.txt' into table aa"</span>)</span></span><br><span class="line">。。。</span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">"select * from aa"</span>).show</span></span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">| 1|</span><br><span class="line">| 2|</span><br><span class="line">| 3|</span><br><span class="line">| 4|</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure>

<p>在实际使用中, 几乎没有任何人会使用内置的 Hive  </p>
<p>2） 外部的 HIVE  </p>
<p>如果想连接外部已经部署好的 Hive，需要通过以下几个步骤：</p>
<ul>
<li>Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下</li>
<li>把 Mysql 的驱动 copy 到 jars/目录下</li>
<li>如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下</li>
<li>重启 spark-shell  </li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">"show tables"</span>).show</span></span><br><span class="line">20/04/25 22:05:14 WARN ObjectStore: Failed to get database global_temp, returning</span><br><span class="line">NoSuchObjectException</span><br><span class="line">+--------+--------------------+-----------+</span><br><span class="line">|database| tableName|isTemporary|</span><br><span class="line">+--------+--------------------+-----------+</span><br><span class="line">| default| emp| false|</span><br><span class="line">| default|hive_hbase_emp_table| false|</span><br><span class="line">| default| relevance_hbase_emp| false|</span><br><span class="line">| default| staff_hive| false|</span><br><span class="line">| default| ttt| false|</span><br><span class="line">| default| user_visit_action| false|</span><br><span class="line">+--------+--------------------+-----------+</span><br></pre></td></tr></table></figure>

<p>3） 运行 Spark SQL CLI  </p>
<p>Spark SQL CLI 可以很方便的在本地运行 Hive 元数据服务以及从命令行执行查询任务。在Spark 目录下执行如下命令启动 Spark SQL CLI，直接执行 SQL 语句，类似一 Hive 窗口  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-sql</span><br></pre></td></tr></table></figure>

<p>4） 运行 Spark beeline</p>
<p>Spark Thrift Server 是 Spark 社区基于 HiveServer2 实现的一个 Thrift 服务。旨在无缝兼容HiveServer2。因为 Spark Thrift Server 的接口和协议都和 HiveServer2 完全一致，因此我们部署好 Spark Thrift Server 后，可以直接使用 hive 的 beeline 访问 Spark Thrift Server 执行相关语句。 Spark Thrift Server 的目的也只是取代 HiveServer2，因此它依旧可以和 Hive Metastore进行交互，获取到 hive 的元数据。  </p>
<p>如果想连接 Thrift Server，需要通过以下几个步骤：  </p>
<ul>
<li><p>Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 conf/目录下</p>
</li>
<li><p>把 Mysql 的驱动 copy 到 jars/目录下</p>
</li>
<li><p>如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下</p>
</li>
<li><p>启动 Thrift Server</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-thriftserver.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用 beeline 连接 Thrift Server  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/beeline -u jdbc:hive2://bigdata2:10000 -n root</span><br></pre></td></tr></table></figure>

<p><img src="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240614154934307.png" alt="image-20240614154934307"></p>
</li>
</ul>
<p>5） 代码操作 Hive  </p>
<ol>
<li><p>导入依赖  </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span>		</span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>将 hive-site.xml 文件拷贝到项目的 resources 目录中，代码实现  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建 SparkSession</span></span><br><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">    .builder()</span><br><span class="line">    .enableHiveSupport()</span><br><span class="line">    .master(<span class="string">"local[*]"</span>)</span><br><span class="line">    .appName(<span class="string">"sql"</span>)</span><br><span class="line">    .getOrCreate()</span><br></pre></td></tr></table></figure>

<p>注意：在开发工具中创建数据库默认是在本地仓库，通过参数修改数据库仓库的地址:  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config(<span class="string">"spark.sql.warehouse.dir"</span>, <span class="string">"hdfs://bigdata1:8020/user/hive/warehouse"</span>)</span><br></pre></td></tr></table></figure>

<p>如果在执行操作时，出现如下错误：  </p>
<p><img src="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/image-20240614155220015.png" alt="image-20240614155220015"></p>
<p>可以代码最前面增加如下代码解决：  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">System</span>.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"root"</span>)</span><br></pre></td></tr></table></figure>

<p>此处的 root 改为你们自己的 hadoop 用户名称</p>
<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.spark.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark05_SparkSQL_Hive</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 如果没有权限，在最前面加上如下代码，设置Hadoop访问用户为root</span></span><br><span class="line">    <span class="type">System</span>.setProperty(<span class="string">"HADOOP_USER_NAME"</span>,<span class="string">"root"</span>)</span><br><span class="line">    <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().enableHiveSupport().config(sparkConf).getOrCreate()</span><br><span class="line">    <span class="comment">// TODO 使用SparkSQL连接外置Hive</span></span><br><span class="line">    <span class="comment">// 1. 拷贝Hive-site.xml文件到classpath下</span></span><br><span class="line">    <span class="comment">// 2. 启用Hive的支持</span></span><br><span class="line">    <span class="comment">// 3. 增加对应的依赖（包含mysql的驱动）</span></span><br><span class="line">    spark.sql(<span class="string">"show databases"</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h1 id="第3章-SparkSQL-项目实战"><a href="#第3章-SparkSQL-项目实战" class="headerlink" title="第3章 SparkSQL 项目实战"></a>第3章 SparkSQL 项目实战</h1><h2 id="3-1-数据准备"><a href="#3-1-数据准备" class="headerlink" title="3.1 数据准备"></a>3.1 数据准备</h2><p>我们这次 Spark-sql 操作中所有的数据均来自 Hive，首先在 Hive 中创建表,，并导入数据。一共有 3 张表： 1 张用户行为表， 1 张城市表， 1 张产品表  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE &#96;user_visit_action&#96;(</span><br><span class="line">    &#96;date&#96; string,</span><br><span class="line">    &#96;user_id&#96; bigint,</span><br><span class="line">    &#96;session_id&#96; string,</span><br><span class="line">    &#96;page_id&#96; bigint,</span><br><span class="line">    &#96;action_time&#96; string,</span><br><span class="line">    &#96;search_keyword&#96; string,</span><br><span class="line">    &#96;click_category_id&#96; bigint,</span><br><span class="line">    &#96;click_product_id&#96; bigint,</span><br><span class="line">    &#96;order_category_ids&#96; string,</span><br><span class="line">    &#96;order_product_ids&#96; string,</span><br><span class="line">    &#96;pay_category_ids&#96; string,</span><br><span class="line">    &#96;pay_product_ids&#96; string,</span><br><span class="line">    &#96;city_id&#96; bigint)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;;</span><br><span class="line">load data local inpath &#39;input&#x2F;user_visit_action.txt&#39; into table user_visit_action;</span><br><span class="line">CREATE TABLE &#96;product_info&#96;(</span><br><span class="line">    &#96;product_id&#96; bigint,</span><br><span class="line">    &#96;product_name&#96; string,</span><br><span class="line">    &#96;extend_info&#96; string)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;;</span><br><span class="line">load data local inpath &#39;input&#x2F;product_info.txt&#39; into table product_info;</span><br><span class="line">CREATE TABLE &#96;city_info&#96;(</span><br><span class="line">    &#96;city_id&#96; bigint,</span><br><span class="line">    &#96;city_name&#96; string,</span><br><span class="line">    &#96;area&#96; string)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;;</span><br><span class="line">load data local inpath &#39;input&#x2F;city_info.txt&#39; into table city_info;</span><br></pre></td></tr></table></figure>

<h2 id="3-2-需求：各区域热门商品-Top3"><a href="#3-2-需求：各区域热门商品-Top3" class="headerlink" title="3.2 需求：各区域热门商品 Top3"></a>3.2 需求：各区域热门商品 Top3</h2><h3 id="3-2-1-需求简介"><a href="#3-2-1-需求简介" class="headerlink" title="3.2.1 需求简介"></a>3.2.1 需求简介</h3><p>这里的热门商品是从点击量的维度来看的，计算各个区域前三大热门商品，并备注上每个商品在主要城市中的分布比例，超过两个城市用其他显示。  </p>
<p>例如：  </p>
<table>
<thead>
<tr>
<th>地区</th>
<th>商品名称</th>
<th>点击次数</th>
<th>城市备注</th>
</tr>
</thead>
<tbody><tr>
<td>华北</td>
<td>商品 A</td>
<td>100000</td>
<td>北京 21.2%，天津 13.2%，其他 65.6%</td>
</tr>
<tr>
<td>华北</td>
<td>商品 P</td>
<td>80200</td>
<td>北京 63.0%，太原 10%，其他 27.0%</td>
</tr>
<tr>
<td>华北</td>
<td>商品 M</td>
<td>40000</td>
<td>北京 63.0%，太原 10%，其他 27.0%</td>
</tr>
<tr>
<td>东北</td>
<td>商品 J</td>
<td>92000</td>
<td>大连 28%，辽宁 17.0%，其他 55.0%</td>
</tr>
</tbody></table>
<h3 id="3-2-2-需求分析"><a href="#3-2-2-需求分析" class="headerlink" title="3.2.2 需求分析"></a>3.2.2 需求分析</h3><ul>
<li>查询出来所有的点击记录，并与 city_info 表连接，得到每个城市所在的地区，与Product_info 表连接得到产品名称</li>
<li>按照地区和商品 id 分组，统计出每个商品在每个地区的总点击次数</li>
<li>每个地区内按照点击次数降序排列</li>
<li>只取前三名</li>
<li>城市备注需要自定义 UDAF 函数  </li>
</ul>
<h3 id="3-2-3-功能实现"><a href="#3-2-3-功能实现" class="headerlink" title="3.2.3 功能实现"></a>3.2.3 功能实现</h3><ul>
<li>连接三张表的数据，获取完整的数据（只有点击）</li>
<li>将数据根据地区，商品名称分组</li>
<li>统计商品点击次数总和,取 Top3</li>
<li>实现自定义聚合函数显示备注  </li>
</ul>
<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.spark.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark06_SparkSQL_Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 如果没有权限，在最前面加上如下代码，设置Hadoop访问用户为root</span></span><br><span class="line">    <span class="type">System</span>.setProperty(<span class="string">"HADOOP_USER_NAME"</span>,<span class="string">"root"</span>)</span><br><span class="line">    <span class="comment">// TODO 创建SparkSQL的运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"sparkSQL"</span>)</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      .config(sparkConf)</span><br><span class="line">      <span class="comment">// 创建数据库</span></span><br><span class="line">      .config(<span class="string">"spark.sql.warehouse.dir"</span>,<span class="string">"hdfs://bigdata1:8020/user/hive/warehouse"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="comment">// TODO 案例实操</span></span><br><span class="line">    spark.sql(<span class="string">"create database IF NOT EXISTS learn"</span>)</span><br><span class="line">    spark.sql(<span class="string">"use learn"</span>)</span><br><span class="line">    <span class="comment">// 1. 准备数据</span></span><br><span class="line"><span class="comment">//    spark.sql(</span></span><br><span class="line"><span class="comment">//      """</span></span><br><span class="line"><span class="comment">//        |CREATE TABLE IF NOT EXISTS `user_visit_action`(</span></span><br><span class="line"><span class="comment">//        |`date` string,</span></span><br><span class="line"><span class="comment">//        |`user_id` bigint,</span></span><br><span class="line"><span class="comment">//        |`session_id` string,</span></span><br><span class="line"><span class="comment">//        |`page_id` bigint,</span></span><br><span class="line"><span class="comment">//        |`action_time` string,</span></span><br><span class="line"><span class="comment">//        |`search_keyword` string,</span></span><br><span class="line"><span class="comment">//        |`click_category_id` bigint,</span></span><br><span class="line"><span class="comment">//        |`click_product_id` bigint,</span></span><br><span class="line"><span class="comment">//        |`order_category_ids` string,</span></span><br><span class="line"><span class="comment">//        |`order_product_ids` string,</span></span><br><span class="line"><span class="comment">//        |`pay_category_ids` string,</span></span><br><span class="line"><span class="comment">//        |`pay_product_ids` string,</span></span><br><span class="line"><span class="comment">//        |`city_id` bigint)</span></span><br><span class="line"><span class="comment">//        |row format delimited fields terminated by '\t'</span></span><br><span class="line"><span class="comment">//        |""".stripMargin).show()</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//    spark.sql("load data local inpath 'datasSql/user_visit_action.txt' into table learn.user_visit_action")</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//    spark.sql(</span></span><br><span class="line"><span class="comment">//      """</span></span><br><span class="line"><span class="comment">//        |CREATE TABLE IF NOT EXISTS `product_info`(</span></span><br><span class="line"><span class="comment">//        |`product_id` bigint,</span></span><br><span class="line"><span class="comment">//        |`product_name` string,</span></span><br><span class="line"><span class="comment">//        |`extend_info` string)</span></span><br><span class="line"><span class="comment">//        |row format delimited fields terminated by '\t'</span></span><br><span class="line"><span class="comment">//        |""".stripMargin)</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//    spark.sql("load data local inpath 'datasSql/product_info.txt' into table learn.product_info")</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//    spark.sql(</span></span><br><span class="line"><span class="comment">//      """</span></span><br><span class="line"><span class="comment">//        |CREATE TABLE IF NOT EXISTS `city_info`(</span></span><br><span class="line"><span class="comment">//        |`city_id` bigint,</span></span><br><span class="line"><span class="comment">//        |`city_name` string,</span></span><br><span class="line"><span class="comment">//        |`area` string)</span></span><br><span class="line"><span class="comment">//        |row format delimited fields terminated by '\t'</span></span><br><span class="line"><span class="comment">//        |""".stripMargin)</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//    spark.sql("load data local inpath 'datasSql/city_info.txt' into table learn.city_info")</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 部分实现</span></span><br><span class="line"><span class="comment">//    spark.sql(</span></span><br><span class="line"><span class="comment">//      """</span></span><br><span class="line"><span class="comment">//        |select</span></span><br><span class="line"><span class="comment">//        |	*</span></span><br><span class="line"><span class="comment">//        |from (</span></span><br><span class="line"><span class="comment">//        |	select</span></span><br><span class="line"><span class="comment">//        |		*,</span></span><br><span class="line"><span class="comment">//        |		rank() over(partition by area order by clickCnt desc) as rank</span></span><br><span class="line"><span class="comment">//        |	from (</span></span><br><span class="line"><span class="comment">//        |		select</span></span><br><span class="line"><span class="comment">//        |		area,</span></span><br><span class="line"><span class="comment">//        |		product_name,</span></span><br><span class="line"><span class="comment">//        |		count(*) clickCnt</span></span><br><span class="line"><span class="comment">//        |		from (</span></span><br><span class="line"><span class="comment">//        |			select</span></span><br><span class="line"><span class="comment">//        |				a.*,</span></span><br><span class="line"><span class="comment">//        |				p.product_name,</span></span><br><span class="line"><span class="comment">//        |				c.area,</span></span><br><span class="line"><span class="comment">//        |				c.city_name</span></span><br><span class="line"><span class="comment">//        |			from user_visit_action a</span></span><br><span class="line"><span class="comment">//        |			join product_info p on a.click_product_id = p.product_id</span></span><br><span class="line"><span class="comment">//        |			join city_info c on a.city_id = c.city_id</span></span><br><span class="line"><span class="comment">//        |			where a.click_product_id &gt; -1</span></span><br><span class="line"><span class="comment">//        |		) t1 group by area,product_name</span></span><br><span class="line"><span class="comment">//        |	) t2</span></span><br><span class="line"><span class="comment">//        |) t3 where rank &lt;= 3</span></span><br><span class="line"><span class="comment">//        |""".stripMargin).show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 完整实现</span></span><br><span class="line">    <span class="comment">// 查询基本数据</span></span><br><span class="line">    spark.sql(</span><br><span class="line">      <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        |	select</span></span><br><span class="line"><span class="string">        |		a.*,</span></span><br><span class="line"><span class="string">        |		p.product_name,</span></span><br><span class="line"><span class="string">        |		c.area,</span></span><br><span class="line"><span class="string">        |		c.city_name</span></span><br><span class="line"><span class="string">        |	from user_visit_action a</span></span><br><span class="line"><span class="string">        |	join product_info p on a.click_product_id = p.product_id</span></span><br><span class="line"><span class="string">        |	join city_info c on a.city_id = c.city_id</span></span><br><span class="line"><span class="string">        |	where a.click_product_id &gt; -1</span></span><br><span class="line"><span class="string">        |"</span><span class="string">""</span>.stripMargin).createOrReplaceTempView(<span class="string">"t1"</span>)</span><br><span class="line"></span><br><span class="line">    spark.udf.register(<span class="string">"cityRemark"</span>,functions.udaf(<span class="keyword">new</span> <span class="type">CityRemarkUDAF</span>()))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据区域，商品进行数据聚合</span></span><br><span class="line">    spark.sql(</span><br><span class="line">      <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        | select</span></span><br><span class="line"><span class="string">        |	area,</span></span><br><span class="line"><span class="string">        |	product_name,</span></span><br><span class="line"><span class="string">        |	count(*) clickCnt,</span></span><br><span class="line"><span class="string">        | cityRemark(city_name) as city_remark</span></span><br><span class="line"><span class="string">        |	from t1 group by area,product_name</span></span><br><span class="line"><span class="string">        |"</span><span class="string">""</span>.stripMargin).createOrReplaceTempView(<span class="string">"t2"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 区域内对点击数量进行排行</span></span><br><span class="line">    spark.sql(</span><br><span class="line">      <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        | select</span></span><br><span class="line"><span class="string">        |		*,</span></span><br><span class="line"><span class="string">        |		rank() over(partition by area order by clickCnt desc) as rank</span></span><br><span class="line"><span class="string">        |	from t2</span></span><br><span class="line"><span class="string">        |"</span><span class="string">""</span>.stripMargin).createOrReplaceTempView(<span class="string">"t3"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 取前三名,不进行省略截取，全部显示字符串信息</span></span><br><span class="line">    spark.sql(</span><br><span class="line">      <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        |select * from t3</span></span><br><span class="line"><span class="string">        |where rank &lt;= 3</span></span><br><span class="line"><span class="string">        |"</span><span class="string">""</span>.stripMargin).show(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Buffer</span>(<span class="params">var total: <span class="type">Long</span>, var cityMap: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">//</span> <span class="title">自定义聚合函数，实现城市备注功能</span></span></span><br><span class="line"><span class="class">  <span class="title">//</span> 1. <span class="title">集成Aggrerator</span></span></span><br><span class="line"><span class="class">  <span class="title">//</span> <span class="title">IN</span></span>: 城市名称</span><br><span class="line">  <span class="comment">// Buff：【总点击数量，Map[(city,cnt),(city,cnt)]】</span></span><br><span class="line">  <span class="comment">// OUT: 备注信息</span></span><br><span class="line">  <span class="comment">// 2. 重写方法</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">CityRemarkUDAF</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">String</span>,<span class="type">Buffer</span>,<span class="type">String</span>]</span>&#123;</span><br><span class="line">    <span class="comment">// 缓冲区的初始化</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Buffer</span> = &#123;</span><br><span class="line">      <span class="type">Buffer</span>(<span class="number">0</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">Long</span>]())</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 更新缓冲区数据</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buff: <span class="type">Buffer</span>, city: <span class="type">String</span>): <span class="type">Buffer</span> = &#123;</span><br><span class="line">      buff.total += <span class="number">1</span></span><br><span class="line">      <span class="keyword">val</span> newCount = buff.cityMap.getOrElse(city, <span class="number">0</span>L) + <span class="number">1</span></span><br><span class="line">      buff.cityMap.update(city,newCount)</span><br><span class="line">      buff</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 合并缓冲区数据</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buff1: <span class="type">Buffer</span>, buff2: <span class="type">Buffer</span>): <span class="type">Buffer</span> = &#123;</span><br><span class="line">      buff1.total = buff1.total + buff2.total</span><br><span class="line">      <span class="keyword">val</span> map1 = buff1.cityMap</span><br><span class="line">      <span class="keyword">val</span> map2 = buff2.cityMap</span><br><span class="line">      buff1.cityMap = map1.foldLeft(map2)&#123;</span><br><span class="line">        <span class="keyword">case</span> (map2,(city, cnt)) =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> count = map2.getOrElse(city, <span class="number">0</span>L) + cnt</span><br><span class="line">          map2.update(city,count)</span><br><span class="line">          map2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      buff1</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 将统计的结果生成字符串信息</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(buff: <span class="type">Buffer</span>): <span class="type">String</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> remarkList = <span class="type">ListBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">      <span class="keyword">val</span> totalCnt = buff.total</span><br><span class="line">      <span class="keyword">val</span> cityMap = buff.cityMap</span><br><span class="line">      <span class="comment">// 降序排列</span></span><br><span class="line">      <span class="keyword">val</span> cityCntList = cityMap.toList.sortWith(</span><br><span class="line">        (left, right) =&gt; &#123;</span><br><span class="line">          left._2 &gt; right._2</span><br><span class="line">        &#125;</span><br><span class="line">      ).take(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> hasMore = cityMap.size &gt; <span class="number">2</span></span><br><span class="line">      <span class="keyword">var</span> rsum = <span class="number">0</span>L</span><br><span class="line">      cityCntList.foreach&#123;</span><br><span class="line">        <span class="keyword">case</span> (city, cnt) =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> r = cnt * <span class="number">100</span> / totalCnt</span><br><span class="line">          remarkList.append(<span class="string">s"<span class="subst">$&#123;city&#125;</span> <span class="subst">$&#123;r&#125;</span>%"</span>)</span><br><span class="line">          rsum += r</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span>(hasMore)&#123;</span><br><span class="line">        remarkList.append(<span class="string">s"其他 <span class="subst">$&#123;100 - rsum&#125;</span>%"</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      remarkList.mkString(<span class="string">", "</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Buffer</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">String</span>] = <span class="type">Encoders</span>.<span class="type">STRING</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Rui Zhang
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://yoursite.com/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/" title="SparkSQL笔记">http://yoursite.com/2024/05/17/SparkSQL笔记/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Spark/" rel="tag"># Spark</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/03/26/Spark%E5%86%85%E6%A0%B8%E4%B8%8E%E6%BA%90%E7%A0%81/" rel="prev" title="Spark内核与源码">
      <i class="fa fa-chevron-left"></i> Spark内核与源码
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/" rel="next" title="SparkStreaming笔记">
      SparkStreaming笔记 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#第1章-SparkSQL-概述"><span class="nav-number">1.</span> <span class="nav-text">第1章 SparkSQL 概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-Hive-and-SparkSQL"><span class="nav-number">1.1.</span> <span class="nav-text">1.2 Hive and SparkSQL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-SparkSQL-特点"><span class="nav-number">1.2.</span> <span class="nav-text">1.3 SparkSQL 特点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-1-易整合"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.3.1 易整合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-2-统一的数据访问"><span class="nav-number">1.2.2.</span> <span class="nav-text">1.3.2 统一的数据访问</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-3-兼容-Hive"><span class="nav-number">1.2.3.</span> <span class="nav-text">1.3.3 兼容 Hive</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-4-标准数据连接"><span class="nav-number">1.2.4.</span> <span class="nav-text">1.3.4 标准数据连接</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-DataFrame-是什么"><span class="nav-number">1.3.</span> <span class="nav-text">1.4 DataFrame 是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-DataSet-是什么"><span class="nav-number">1.4.</span> <span class="nav-text">1.5 DataSet 是什么</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第2章-SparkSQL-核心编程"><span class="nav-number">2.</span> <span class="nav-text">第2章 SparkSQL 核心编程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-新的起点"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 新的起点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-DataFrame"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 DataFrame</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-创建-DataFrame"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1 创建 DataFrame</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-SQL-语法"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.2 SQL 语法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-DSL-语法"><span class="nav-number">2.2.3.</span> <span class="nav-text">2.2.3 DSL 语法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-4-RDD-转换为-DataFrame"><span class="nav-number">2.2.4.</span> <span class="nav-text">2.2.4 RDD 转换为 DataFrame</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-5-DataFrame-转换为-RDD"><span class="nav-number">2.2.5.</span> <span class="nav-text">2.2.5 DataFrame 转换为 RDD</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-DataSet"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 DataSet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-1-创建-DataSet"><span class="nav-number">2.3.1.</span> <span class="nav-text">2.3.1 创建 DataSet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-2-RDD-转换为-DataSet"><span class="nav-number">2.3.2.</span> <span class="nav-text">2.3.2 RDD 转换为 DataSet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-3-DataSet-转换为-RDD"><span class="nav-number">2.3.3.</span> <span class="nav-text">2.3.3 DataSet 转换为 RDD</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-DataFrame-和-DataSet-转换"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 DataFrame 和 DataSet 转换</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-RDD、-DataFrame、-DataSet-三者的关系"><span class="nav-number">2.5.</span> <span class="nav-text">2.5 RDD、 DataFrame、 DataSet 三者的关系</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-1-三者的共性"><span class="nav-number">2.5.1.</span> <span class="nav-text">2.5.1 三者的共性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-2-三者的区别"><span class="nav-number">2.5.2.</span> <span class="nav-text">2.5.2 三者的区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-3-三者的互相转换"><span class="nav-number">2.5.3.</span> <span class="nav-text">2.5.3 三者的互相转换</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-6-IDEA-开发-SparkSQL"><span class="nav-number">2.6.</span> <span class="nav-text">2.6 IDEA 开发 SparkSQL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-1-添加依赖"><span class="nav-number">2.6.1.</span> <span class="nav-text">2.6.1 添加依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-2-代码实现"><span class="nav-number">2.6.2.</span> <span class="nav-text">2.6.2 代码实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-7-用户自定义函数"><span class="nav-number">2.7.</span> <span class="nav-text">2.7 用户自定义函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-1-UDF"><span class="nav-number">2.7.1.</span> <span class="nav-text">2.7.1 UDF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-2-UDAF"><span class="nav-number">2.7.2.</span> <span class="nav-text">2.7.2 UDAF</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-7-2-1-实现方式-RDD"><span class="nav-number">2.7.2.1.</span> <span class="nav-text">2.7.2.1 实现方式 - RDD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-7-2-2-实现方式-累加器"><span class="nav-number">2.7.2.2.</span> <span class="nav-text">2.7.2.2 实现方式 - 累加器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-7-2-3-实现方式-UDAF-弱类型-（新版spark不推荐使用）"><span class="nav-number">2.7.2.3.</span> <span class="nav-text">2.7.2.3 实现方式 - UDAF - 弱类型 （新版spark不推荐使用）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-7-2-4-实现方式-UDAF-强类型-（新版Spark3-0之后出现的，推荐使用）"><span class="nav-number">2.7.2.4.</span> <span class="nav-text">2.7.2.4 实现方式 - UDAF - 强类型 （新版Spark3.0之后出现的，推荐使用）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-7-2-5-实现方式-UDAF-早期版本进行强类型函数实现"><span class="nav-number">2.7.2.5.</span> <span class="nav-text">2.7.2.5 实现方式 - UDAF - 早期版本进行强类型函数实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-8-数据的加载和保存"><span class="nav-number">2.8.</span> <span class="nav-text">2.8 数据的加载和保存</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-1-通用的加载和保存方式"><span class="nav-number">2.8.1.</span> <span class="nav-text">2.8.1 通用的加载和保存方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-2-Parquet"><span class="nav-number">2.8.2.</span> <span class="nav-text">2.8.2 Parquet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-3-JSON"><span class="nav-number">2.8.3.</span> <span class="nav-text">2.8.3 JSON</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-4-CSV"><span class="nav-number">2.8.4.</span> <span class="nav-text">2.8.4 CSV</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-5-MySQL"><span class="nav-number">2.8.5.</span> <span class="nav-text">2.8.5 MySQL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-6-Hive"><span class="nav-number">2.8.6.</span> <span class="nav-text">2.8.6 Hive</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第3章-SparkSQL-项目实战"><span class="nav-number">3.</span> <span class="nav-text">第3章 SparkSQL 项目实战</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-数据准备"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 数据准备</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-需求：各区域热门商品-Top3"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 需求：各区域热门商品 Top3</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-需求简介"><span class="nav-number">3.2.1.</span> <span class="nav-text">3.2.1 需求简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-需求分析"><span class="nav-number">3.2.2.</span> <span class="nav-text">3.2.2 需求分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-3-功能实现"><span class="nav-number">3.2.3.</span> <span class="nav-text">3.2.3 功能实现</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Rui Zhang"
      src="/images/head.jpg">
  <p class="site-author-name" itemprop="name">Rui Zhang</p>
  <div class="site-description" itemprop="description">不在沉默中爆发，就在沉默中灭亡</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">52</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rui Zhang</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">1.7m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">25:10</span>
</div>




        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":120,"height":230},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>
