<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="第1章 SparkStreaming 概述1.1 Spark Streaming 是什么 Spark Streaming 用于流式数据的处理。 Spark Streaming 支持的数据输入源很多，例如： Kafka、Flume、 Twitter、 ZeroMQ 和简单的 TCP 套接字等等。数据输入后可以用 Spark 的高度抽象原语如： map、 reduce、 join、 window 等进">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkStreaming笔记">
<meta property="og:url" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="没有尾巴的小驴">
<meta property="og:description" content="第1章 SparkStreaming 概述1.1 Spark Streaming 是什么 Spark Streaming 用于流式数据的处理。 Spark Streaming 支持的数据输入源很多，例如： Kafka、Flume、 Twitter、 ZeroMQ 和简单的 TCP 套接字等等。数据输入后可以用 Spark 的高度抽象原语如： map、 reduce、 join、 window 等进">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240704180901901.png">
<meta property="og:image" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240704181003841.png">
<meta property="og:image" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240704181218144.png">
<meta property="og:image" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240704181255008.png">
<meta property="og:image" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240704181341841.png">
<meta property="og:image" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240704181447827.png">
<meta property="og:image" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240704181704537.png">
<meta property="og:image" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240708144700946.png">
<meta property="og:image" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240708141916093.png">
<meta property="og:image" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240708141951879.png">
<meta property="og:image" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240708142042672.png">
<meta property="og:image" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240722094219967.png">
<meta property="og:image" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240722102650044.png">
<meta property="og:image" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240722101745231.png">
<meta property="og:image" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240725181435513.png">
<meta property="og:image" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240725181522164.png">
<meta property="article:published_time" content="2024-07-04T03:05:03.000Z">
<meta property="article:modified_time" content="2025-06-25T08:08:45.967Z">
<meta property="article:author" content="Rui Zhang">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240704180901901.png">

<link rel="canonical" href="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true
  };
</script>

  <title>SparkStreaming笔记 | 没有尾巴的小驴</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">没有尾巴的小驴</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">记录生活</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">22</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">22</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">52</span></a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.jpg">
      <meta itemprop="name" content="Rui Zhang">
      <meta itemprop="description" content="不在沉默中爆发，就在沉默中灭亡">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="没有尾巴的小驴">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          SparkStreaming笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-07-04 11:05:03" itemprop="dateCreated datePublished" datetime="2024-07-04T11:05:03+08:00">2024-07-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-06-25 16:08:45" itemprop="dateModified" datetime="2025-06-25T16:08:45+08:00">2025-06-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>77k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1:10</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="第1章-SparkStreaming-概述"><a href="#第1章-SparkStreaming-概述" class="headerlink" title="第1章 SparkStreaming 概述"></a>第1章 SparkStreaming 概述</h1><h2 id="1-1-Spark-Streaming-是什么"><a href="#1-1-Spark-Streaming-是什么" class="headerlink" title="1.1 Spark Streaming 是什么"></a>1.1 Spark Streaming 是什么</h2><p><img src="/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240704180901901.png" alt="image-20240704180901901"></p>
<p>Spark Streaming 用于流式数据的处理。 Spark Streaming 支持的数据输入源很多，例如： Kafka、Flume、 Twitter、 ZeroMQ 和简单的 TCP 套接字等等。数据输入后可以用 Spark 的高度抽象原语如： map、 reduce、 join、 window 等进行运算。而结果也能保存在很多地方，如 HDFS，数据库等。  </p>
<p><img src="/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240704181003841.png" alt="image-20240704181003841"></p>
<p>和 Spark 基于 RDD 的概念很相似， Spark Streaming 使用<strong>离散化流</strong>(discretized stream)作为抽象表示，叫作 DStream。 DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为 RDD 存在，而 DStream 是由这些 RDD 所组成的序列(因此得名“离散化”)。 所以简单来将， DStream 就是对 RDD 在实时数据处理场景的一种封装。</p>
<h2 id="1-2-Spark-Streaming-的特点"><a href="#1-2-Spark-Streaming-的特点" class="headerlink" title="1.2 Spark Streaming 的特点"></a>1.2 Spark Streaming 的特点</h2><ul>
<li><p>易用</p>
<p><img src="/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240704181218144.png" alt="image-20240704181218144"></p>
</li>
<li><p>容错</p>
<p><img src="/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240704181255008.png" alt="image-20240704181255008"></p>
</li>
<li><p>易整合到Spark体系</p>
<p><img src="/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240704181341841.png" alt="image-20240704181341841"></p>
</li>
</ul>
<h2 id="1-3-Spark-Streaming-架构"><a href="#1-3-Spark-Streaming-架构" class="headerlink" title="1.3 Spark Streaming 架构"></a>1.3 Spark Streaming 架构</h2><h3 id="1-3-1-架构图"><a href="#1-3-1-架构图" class="headerlink" title="1.3.1 架构图"></a>1.3.1 架构图</h3><ul>
<li><p>整体架构图</p>
<p><img src="/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240704181447827.png" alt="image-20240704181447827"></p>
</li>
<li><p>SparkStreaming架构图</p>
<p><img src="/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240704181704537.png" alt="image-20240704181704537"></p>
<p><img src="/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240708144700946.png" alt="image-20240708144700946"></p>
</li>
</ul>
<h3 id="1-3-2-背压机制"><a href="#1-3-2-背压机制" class="headerlink" title="1.3.2 背压机制"></a>1.3.2 背压机制</h3><p>Spark 1.5 以前版本，用户如果要限制 Receiver 的数据接收速率，可以通过设置静态配制参数“spark.streaming.receiver.maxRate”的值来实现，此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题。比如： producer 数据生产高于 maxRate，当前集群处理能力也高于 maxRate，这就会造成资源利用率下降等问题。</p>
<p>为了更好的协调数据接收速率与资源处理能力， 1.5 版本开始 Spark Streaming 可以动态控制数据接收速率来适配集群数据处理能力。 背压机制（即 Spark Streaming Backpressure） : 根据JobScheduler 反馈作业的执行信息来动态调整 Receiver 数据接收率。</p>
<p>通过属性“spark.streaming.backpressure.enabled”来控制是否启用 backpressure 机制，默认值false，即不启用。  </p>
<h1 id="第-2-章-Dstream-入门"><a href="#第-2-章-Dstream-入门" class="headerlink" title="第 2 章 Dstream 入门"></a>第 2 章 Dstream 入门</h1><h2 id="2-1-WordCount-案例实操"><a href="#2-1-WordCount-案例实操" class="headerlink" title="2.1 WordCount 案例实操"></a>2.1 WordCount 案例实操</h2><p>需求：使用 netcat 工具向 9999 端口不断的发送数据，通过 SparkStreaming 读取端口数据并<br>统计不同单词出现的次数  </p>
<p>1) 添加依赖  </p>
   <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2) 编写代码  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamWordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//1.初始化 Spark 配置信息</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span></span><br><span class="line">        <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"StreamWordCount"</span>)</span><br><span class="line">        <span class="comment">//2.初始化 SparkStreamingContext</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">        <span class="comment">//3.通过监控端口创建 DStream，读进来的数据为一行行</span></span><br><span class="line">        <span class="keyword">val</span> lineStreams = ssc.socketTextStream(<span class="string">"linux1"</span>, <span class="number">9999</span>)</span><br><span class="line">        <span class="comment">//将每一行数据做切分，形成一个个单词</span></span><br><span class="line">        <span class="keyword">val</span> wordStreams = lineStreams.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="comment">//将单词映射成元组（word,1）</span></span><br><span class="line">        <span class="keyword">val</span> wordAndOneStreams = wordStreams.map((_, <span class="number">1</span>))</span><br><span class="line">        <span class="comment">//将相同的单词次数做统计</span></span><br><span class="line">        <span class="keyword">val</span> wordAndCountStreams = wordAndOneStreams.reduceByKey(_+_)</span><br><span class="line">        <span class="comment">//打印</span></span><br><span class="line">        wordAndCountStreams.print()</span><br><span class="line">        <span class="comment">//启动 SparkStreamingContext</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>3) 启动程序并通过 netcat 发送数据</p>
   <figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nc -lk <span class="number">9999</span></span><br><span class="line">hello spark</span><br></pre></td></tr></table></figure>

<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.spark.streaming</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming01_WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 创建环境对象</span></span><br><span class="line">    <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">    <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf,<span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 逻辑处理</span></span><br><span class="line">    <span class="comment">// 获取端口数据</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">8888</span>)</span><br><span class="line">    <span class="keyword">val</span> wordToCount = lines.flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">    wordToCount.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    <span class="comment">// 由于SparkStreaming采集器是长期执行的任务，所以不能直接关闭</span></span><br><span class="line">    <span class="comment">// 如果main方法执行关闭，应用程序也会自动结束，所以不能让main方法执行完毕</span></span><br><span class="line">    <span class="comment">// ssc.stop()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 问题解决：</span></span><br><span class="line">    <span class="comment">// 1. 启动采集器</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">// 2. 等待采集器的关闭,采集器不关，主线程不能停</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="2-2-WordCount-解析"><a href="#2-2-WordCount-解析" class="headerlink" title="2.2 WordCount 解析"></a>2.2 WordCount 解析</h2><p>Discretized Stream 是 Spark Streaming 的基础抽象，代表持续性的数据流和经过各种 Spark 原语操作后的结果数据流。在内部实现上， DStream 是一系列连续的 RDD 来表示。每个 RDD 含有一段时间间隔内的数据。</p>
<p><img src="/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240708141916093.png" alt="image-20240708141916093"></p>
<p>对数据的操作也是按照 RDD 为单位来进行的</p>
<p><img src="/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240708141951879.png" alt="image-20240708141951879"></p>
<p>计算过程由 Spark Engine 来完成</p>
<p><img src="/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240708142042672.png" alt="image-20240708142042672"></p>
<h1 id="第-3-章-DStream-创建"><a href="#第-3-章-DStream-创建" class="headerlink" title="第 3 章 DStream 创建"></a>第 3 章 DStream 创建</h1><h2 id="3-1-RDD-队列"><a href="#3-1-RDD-队列" class="headerlink" title="3.1 RDD 队列"></a>3.1 RDD 队列</h2><h3 id="3-1-1-用法及说明"><a href="#3-1-1-用法及说明" class="headerlink" title="3.1.1 用法及说明"></a>3.1.1 用法及说明</h3><p>测试过程中，可以通过使用 ssc.queueStream(queueOfRDDs)来创建 DStream，每一个推送到这个队列中的 RDD，都会作为一个 DStream 处理。</p>
<h3 id="3-1-2-案例实操"><a href="#3-1-2-案例实操" class="headerlink" title="3.1.2 案例实操"></a>3.1.2 案例实操</h3><p>需求：循环创建几个 RDD，将 RDD 放入队列。通过 SparkStream 创建 Dstream，计算WordCount</p>
<p>1) 编写代码  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDDStream</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">        <span class="comment">//1.初始化 Spark 配置信息</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"RDDStream"</span>)</span><br><span class="line">        <span class="comment">//2.初始化 SparkStreamingContext</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">4</span>))</span><br><span class="line">        <span class="comment">//3.创建 RDD 队列</span></span><br><span class="line">        <span class="keyword">val</span> rddQueue = <span class="keyword">new</span> mutable.<span class="type">Queue</span>[<span class="type">RDD</span>[<span class="type">Int</span>]]()</span><br><span class="line">        <span class="comment">//4.创建 QueueInputDStream</span></span><br><span class="line">        <span class="keyword">val</span> inputStream = ssc.queueStream(rddQueue,oneAtATime = <span class="literal">false</span>)</span><br><span class="line">        <span class="comment">//5.处理队列中的 RDD 数据</span></span><br><span class="line">        <span class="keyword">val</span> mappedStream = inputStream.map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> reducedStream = mappedStream.reduceByKey(_ + _)</span><br><span class="line">        <span class="comment">//6.打印结果</span></span><br><span class="line">        reducedStream.print()</span><br><span class="line">        <span class="comment">//7.启动任务</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        <span class="comment">//8.循环创建并向 RDD 队列中放入 RDD</span></span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">5</span>) &#123;</span><br><span class="line">            rddQueue += ssc.sparkContext.makeRDD(<span class="number">1</span> to <span class="number">300</span>, <span class="number">10</span>)</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">2000</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2) 结果展示  </p>
   <figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">-------------------------------------------</span></span><br><span class="line"><span class="attr">Time</span>: <span class="string">1539075280000 ms</span></span><br><span class="line"><span class="attr">-------------------------------------------</span></span><br><span class="line"><span class="attr">(4,60)</span></span><br><span class="line"><span class="attr">(0,60)</span></span><br><span class="line"><span class="attr">(6,60)</span></span><br><span class="line"><span class="attr">(8,60)</span></span><br><span class="line"><span class="attr">(2,60)</span></span><br><span class="line"><span class="attr">(1,60)</span></span><br><span class="line"><span class="attr">(3,60)</span></span><br><span class="line"><span class="attr">(7,60)</span></span><br><span class="line"><span class="attr">(9,60)</span></span><br><span class="line"><span class="attr">(5,60)</span></span><br><span class="line"><span class="attr">-------------------------------------------</span></span><br><span class="line"><span class="attr">Time</span>: <span class="string">1539075284000 ms</span></span><br><span class="line"><span class="attr">-------------------------------------------</span></span><br><span class="line"><span class="attr">(4,60)</span></span><br><span class="line"><span class="attr">(0,60)</span></span><br><span class="line"><span class="attr">(6,60)</span></span><br><span class="line"><span class="attr">(8,60)</span></span><br><span class="line"><span class="attr">(2,60)</span></span><br><span class="line"><span class="attr">(1,60)</span></span><br><span class="line"><span class="attr">(3,60)</span></span><br><span class="line"><span class="attr">(7,60)</span></span><br><span class="line"><span class="attr">(9,60)</span></span><br><span class="line"><span class="attr">(5,60)</span></span><br><span class="line"><span class="attr">-------------------------------------------</span></span><br><span class="line"><span class="attr">Time</span>: <span class="string">1539075288000 ms</span></span><br><span class="line"><span class="attr">-------------------------------------------</span></span><br><span class="line"><span class="attr">(4,30)</span></span><br><span class="line"><span class="attr">(0,30)</span></span><br><span class="line"><span class="attr">(6,30)</span></span><br><span class="line"><span class="attr">(8,30)</span></span><br><span class="line"><span class="attr">(2,30)</span></span><br><span class="line"><span class="attr">(1,30)</span></span><br><span class="line"><span class="attr">(3,30)</span></span><br><span class="line"><span class="attr">(7,30)</span></span><br><span class="line"><span class="attr">(9,30)</span></span><br><span class="line"><span class="attr">(5,30)</span></span><br><span class="line"><span class="attr">-------------------------------------------</span></span><br><span class="line"><span class="attr">Time</span>: <span class="string">1539075292000 ms</span></span><br><span class="line"><span class="attr">-------------------------------------------</span></span><br></pre></td></tr></table></figure>

<p>   练习与测试</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.spark.streaming</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming02_Queue</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO Queue</span></span><br><span class="line">    <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">    <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf,<span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 RDD 队列</span></span><br><span class="line">    <span class="keyword">val</span> rddQueue = <span class="keyword">new</span> mutable.<span class="type">Queue</span>[<span class="type">RDD</span>[<span class="type">Int</span>]]()</span><br><span class="line">    <span class="comment">// 创建 QueueInputDStream</span></span><br><span class="line">    <span class="keyword">val</span> inputStream = ssc.queueStream(rddQueue,oneAtATime = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 处理队列中的 RDD 数据</span></span><br><span class="line">    <span class="keyword">val</span> mappedStream = inputStream.map((_,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> reducedStream = mappedStream.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印结果</span></span><br><span class="line">    reducedStream.print()</span><br><span class="line">    ssc.start()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 循环创建并向 RDD 队列中放入 RDD</span></span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">5</span>) &#123;</span><br><span class="line">      rddQueue += ssc.sparkContext.makeRDD(<span class="number">1</span> to <span class="number">300</span>, <span class="number">10</span>)</span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">2000</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="3-2-自定义数据源"><a href="#3-2-自定义数据源" class="headerlink" title="3.2 自定义数据源"></a>3.2 自定义数据源</h2><h3 id="3-2-1-用法及说明"><a href="#3-2-1-用法及说明" class="headerlink" title="3.2.1 用法及说明"></a>3.2.1 用法及说明</h3><p>需要继承 Receiver，并实现 onStart、 onStop 方法来自定义数据源采集。</p>
<h3 id="3-2-2-案例实操"><a href="#3-2-2-案例实操" class="headerlink" title="3.2.2 案例实操"></a>3.2.2 案例实操</h3><p>需求：自定义数据源，实现监控某个端口号，获取该端口号内容。</p>
<p>1) 自定义数据源  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomerReceiver</span>(<span class="params">host: <span class="type">String</span>, port: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Receiver</span>[<span class="type">String</span>](<span class="params"><span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span></span>) </span>&#123;</span><br><span class="line">    <span class="comment">//最初启动的时候，调用该方法，作用为：读数据并将数据发送给 Spark</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Thread</span>(<span class="string">"Socket Receiver"</span>) &#123;</span><br><span class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">            	receive()</span><br><span class="line">            &#125;</span><br><span class="line">    	&#125;.start()	</span><br><span class="line">	&#125;	</span><br><span class="line">    </span><br><span class="line">	<span class="comment">//读数据并将数据发送给 Spark</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//创建一个 Socket</span></span><br><span class="line">        <span class="keyword">var</span> socket: <span class="type">Socket</span> = <span class="keyword">new</span> <span class="type">Socket</span>(host, port)</span><br><span class="line">        <span class="comment">//定义一个变量，用来接收端口传过来的数据</span></span><br><span class="line">        <span class="keyword">var</span> input: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line">        <span class="comment">//创建一个 BufferedReader 用于读取端口传来的数据</span></span><br><span class="line">        <span class="keyword">val</span> reader = <span class="keyword">new</span> <span class="type">BufferedReader</span>(<span class="keyword">new</span> <span class="type">InputStreamReader</span>(socket.getInputStream, <span class="type">StandardCharsets</span>.<span class="type">UTF_8</span>))</span><br><span class="line">        <span class="comment">//读取数据</span></span><br><span class="line">        input = reader.readLine()</span><br><span class="line">        <span class="comment">//当 receiver 没有关闭并且输入数据不为空，则循环发送数据给 Spark</span></span><br><span class="line">        <span class="keyword">while</span> (!isStopped() &amp;&amp; input != <span class="literal">null</span>) &#123;</span><br><span class="line">            store(input)</span><br><span class="line">            input = reader.readLine()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//跳出循环则关闭资源</span></span><br><span class="line">        reader.close()</span><br><span class="line">        socket.close()</span><br><span class="line">        <span class="comment">//重启任务</span></span><br><span class="line">        restart(<span class="string">"restart"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStop</span></span>(): <span class="type">Unit</span> = &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   2) 使用自定义的数据源采集数据  </p>
<pre><code><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FileStream</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//1.初始化 Spark 配置信息</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">        .setAppName(<span class="string">"StreamWordCount"</span>)</span><br><span class="line">        <span class="comment">//2.初始化 SparkStreamingContext</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        <span class="comment">//3.创建自定义 receiver 的 Streaming</span></span><br><span class="line">        <span class="keyword">val</span> lineStream = ssc.receiverStream(<span class="keyword">new</span> <span class="type">CustomerReceiver</span>(<span class="string">"hadoop102"</span>, <span class="number">9999</span>))</span><br><span class="line">        <span class="comment">//4.将每一行数据做切分，形成一个个单词</span></span><br><span class="line">        <span class="keyword">val</span> wordStream = lineStream.flatMap(_.split(<span class="string">"\t"</span>))</span><br><span class="line">        <span class="comment">//5.将单词映射成元组（word,1）</span></span><br><span class="line">        <span class="keyword">val</span> wordAndOneStream = wordStream.map((_, <span class="number">1</span>))</span><br><span class="line">        <span class="comment">//6.将相同的单词次数做统计</span></span><br><span class="line">        <span class="keyword">val</span> wordAndCountStream = wordAndOneStream.reduceByKey(_ + _)</span><br><span class="line">        <span class="comment">//7.打印</span></span><br><span class="line">        wordAndCountStream.print()</span><br><span class="line">        <span class="comment">//8.启动 SparkStreamingContext</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

练习与测试

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.spark.streaming</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">ReceiverInputDStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.receiver.<span class="type">Receiver</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming03_DIY</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 自定义数据采集器</span></span><br><span class="line">    <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">    <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf,<span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="keyword">val</span> messageDS: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.receiverStream(<span class="keyword">new</span> <span class="type">MyReceiver</span>())</span><br><span class="line">    messageDS.print()</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 自定义数据采集器</span></span><br><span class="line"><span class="comment">   * 1. 集成Receiver, 定义泛型，传递参数</span></span><br><span class="line"><span class="comment">   * 2. 重写方法</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyReceiver</span> <span class="keyword">extends</span> <span class="title">Receiver</span>[<span class="type">String</span>](<span class="params"><span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span></span>)</span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> flg = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Thread</span>(<span class="keyword">new</span> <span class="type">Runnable</span>() &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">while</span> (flg) &#123;</span><br><span class="line">            <span class="keyword">val</span> message = <span class="string">"采集的数据为："</span> + <span class="keyword">new</span> <span class="type">Random</span>().nextInt(<span class="number">10</span>).toString</span><br><span class="line">            store(message)</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">500</span>)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;).start()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStop</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">this</span>.flg = <span class="literal">false</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></code></pre><h2 id="3-3-Kafka-数据源（面试、开发重点）"><a href="#3-3-Kafka-数据源（面试、开发重点）" class="headerlink" title="3.3 Kafka 数据源（面试、开发重点）"></a>3.3 Kafka 数据源（面试、开发重点）</h2><h3 id="3-3-1-版本选型"><a href="#3-3-1-版本选型" class="headerlink" title="3.3.1 版本选型"></a>3.3.1 版本选型</h3><p>ReceiverAPI：需要一个专门的 Executor 去接收数据，然后发送给其他的 Executor 做计算。存在的问题，接收数据的 Executor 和计算的 Executor 速度会有所不同，特别在接收数据的 Executor速度大于计算的 Executor 速度，会导致计算数据的节点内存溢出。 早期版本中提供此方式，当前版本不适用</p>
<p>DirectAPI：是由计算的 Executor 来主动消费 Kafka 的数据，速度由自身控制。</p>
<h3 id="3-3-2-Kafka-0-8-Receiver-模式（当前版本不适用）"><a href="#3-3-2-Kafka-0-8-Receiver-模式（当前版本不适用）" class="headerlink" title="3.3.2 Kafka 0-8 Receiver 模式（当前版本不适用）"></a>3.3.2 Kafka 0-8 Receiver 模式（当前版本不适用）</h3><p>1） 需求：通过 SparkStreaming 从 Kafka 读取数据，并将读取过来的数据做简单计算，最终打印到控制台。  </p>
<p>2）导入依赖  </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-8_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3）编写代码  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.kafka</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">ReceiverInputDStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.<span class="type">KafkaUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ReceiverAPI</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//1.创建 SparkConf</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">        <span class="comment">//2.创建 StreamingContext</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">        <span class="comment">//3.读取 Kafka 数据创建 DStream(基于 Receive 方式)</span></span><br><span class="line">        <span class="keyword">val</span> kafkaDStream: <span class="type">ReceiverInputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] =</span><br><span class="line">            <span class="type">KafkaUtils</span>.createStream(ssc,</span><br><span class="line">            <span class="string">"linux1:2181,linux2:2181,linux3:2181"</span>,</span><br><span class="line">            <span class="string">"atguigu"</span>,</span><br><span class="line">        	<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>](<span class="string">"atguigu"</span> -&gt; <span class="number">1</span>))</span><br><span class="line">        <span class="comment">//4.计算 WordCount</span></span><br><span class="line">        kafkaDStream.map &#123; </span><br><span class="line">            <span class="keyword">case</span> (_, value) =&gt; (value, <span class="number">1</span>)</span><br><span class="line">        &#125;.reduceByKey(_ + _).print()</span><br><span class="line">        <span class="comment">//5.开启任务</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-3-3-Kafka-0-8-Direct-模式（当前版本不适用）"><a href="#3-3-3-Kafka-0-8-Direct-模式（当前版本不适用）" class="headerlink" title="3.3.3 Kafka 0-8 Direct 模式（当前版本不适用）"></a>3.3.3 Kafka 0-8 Direct 模式（当前版本不适用）</h3><p>1）需求：通过 SparkStreaming 从 Kafka 读取数据，并将读取过来的数据做简单计算，最终打印到控制台。  </p>
<p>2）导入依赖  </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>	&gt;</span>spark-streaming-kafka-0-8_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3）编写代码（自动维护 offset）  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> kafka.serializer.<span class="type">StringDecoder</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">InputDStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.<span class="type">KafkaUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DirectAPIAuto02</span> </span>&#123;</span><br><span class="line">    <span class="keyword">val</span> getSSC1: () =&gt; <span class="type">StreamingContext</span> = () =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">        ssc</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getSSC</span></span>: <span class="type">StreamingContext</span> = &#123;</span><br><span class="line">        <span class="comment">//1.创建 SparkConf</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span></span><br><span class="line">        <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">        <span class="comment">//2.创建 StreamingContext</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">        <span class="comment">//设置 CK</span></span><br><span class="line">        ssc.checkpoint(<span class="string">"./ck2"</span>)</span><br><span class="line">        <span class="comment">//3.定义 Kafka 参数</span></span><br><span class="line">        <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">            <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt;</span><br><span class="line">            <span class="string">"linux1:9092,linux2:9092,linux3:9092"</span>,</span><br><span class="line">            <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"atguigu"</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment">//4.读取 Kafka 数据</span></span><br><span class="line">        <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] =</span><br><span class="line">        <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>](ssc, kafkaPara, <span class="type">Set</span>(<span class="string">"atguigu"</span>))</span><br><span class="line">        <span class="comment">//5.计算 WordCount</span></span><br><span class="line">        kafkaDStream.map(_._2)</span><br><span class="line">            .flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">            .map((_, <span class="number">1</span>))</span><br><span class="line">            .reduceByKey(_ + _)</span><br><span class="line">            .print()</span><br><span class="line">        <span class="comment">//6.返回数据</span></span><br><span class="line">        ssc</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//获取 SSC</span></span><br><span class="line">        <span class="keyword">val</span> ssc: <span class="type">StreamingContext</span> = <span class="type">StreamingContext</span>.getActiveOrCreate(<span class="string">"./ck2"</span>, () =&gt; getSSC)</span><br><span class="line">        <span class="comment">//开启任务</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>4）编写代码（手动维护 offset）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> kafka.common.<span class="type">TopicAndPartition</span></span><br><span class="line"><span class="keyword">import</span> kafka.message.<span class="type">MessageAndMetadata</span></span><br><span class="line"><span class="keyword">import</span> kafka.serializer.<span class="type">StringDecoder</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.&#123;<span class="type">HasOffsetRanges</span>, <span class="type">KafkaUtils</span>, <span class="type">OffsetRange</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DirectAPIHandler</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//1.创建 SparkConf</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span></span><br><span class="line">        <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">        <span class="comment">//2.创建 StreamingContext</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">        <span class="comment">//3.Kafka 参数</span></span><br><span class="line">        <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">            <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt;</span><br><span class="line">            <span class="string">"hadoop102:9092,hadoop103:9092,hadoop104:9092"</span>,</span><br><span class="line">            <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"atguigu"</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment">//4.获取上一次启动最后保留的 Offset=&gt;getOffset(MySQL)</span></span><br><span class="line">        <span class="keyword">val</span> fromOffsets: <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>] = <span class="type">Map</span>[<span class="type">TopicAndPartition</span>,</span><br><span class="line">        <span class="type">Long</span>](<span class="type">TopicAndPartition</span>(<span class="string">"atguigu"</span>, <span class="number">0</span>) -&gt; <span class="number">20</span>)</span><br><span class="line">        <span class="comment">//5.读取 Kafka 数据创建 DStream</span></span><br><span class="line">        <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[<span class="type">String</span>] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>,</span><br><span class="line">        <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>, <span class="type">String</span>](ssc, kafkaPara, fromOffsets, (m: <span class="type">MessageAndMetadata</span>[<span class="type">String</span>, <span class="type">String</span>]) =&gt; 					m.message())</span><br><span class="line">        <span class="comment">//6.创建一个数组用于存放当前消费数据的 offset 信息</span></span><br><span class="line">        <span class="keyword">var</span> offsetRanges = <span class="type">Array</span>.empty[<span class="type">OffsetRange</span>]</span><br><span class="line">        <span class="comment">//7.获取当前消费数据的 offset 信息</span></span><br><span class="line">        <span class="keyword">val</span> wordToCountDStream: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = kafkaDStream.transform &#123; rdd =&gt;</span><br><span class="line">            offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">            rdd</span><br><span class="line">        &#125;.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        .map((_, <span class="number">1</span>))</span><br><span class="line">        .reduceByKey(_ + _)</span><br><span class="line">        <span class="comment">//8.打印 Offset 信息</span></span><br><span class="line">        wordToCountDStream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">            <span class="keyword">for</span> (o &lt;- offsetRanges) &#123;</span><br><span class="line">                println(<span class="string">s"<span class="subst">$&#123;o.topic&#125;</span>:<span class="subst">$&#123;o.partition&#125;</span>:<span class="subst">$&#123;o.fromOffset&#125;</span>:<span class="subst">$&#123;o.untilOffset&#125;</span>"</span>)</span><br><span class="line">            &#125;</span><br><span class="line">            rdd.foreach(println)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//9.开启任务</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-3-4-Kafka-0-10-Direct-模式"><a href="#3-3-4-Kafka-0-10-Direct-模式" class="headerlink" title="3.3.4 Kafka 0-10 Direct 模式"></a>3.3.4 Kafka 0-10 Direct 模式</h3><p>1）需求：通过 SparkStreaming 从 Kafka 读取数据，并将读取过来的数据做简单计算，最终打印到控制台。  </p>
<p>2）导入依赖  </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.fasterxml.jackson.core<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jackson-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.10.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3）编写代码  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.&#123;<span class="type">ConsumerConfig</span>, <span class="type">ConsumerRecord</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DirectAPI</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//1.创建 SparkConf</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span></span><br><span class="line">        <span class="type">SparkConf</span>().setAppName(<span class="string">"ReceiverWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">        <span class="comment">//2.创建 StreamingContext</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">        <span class="comment">//3.定义 Kafka 参数</span></span><br><span class="line">        <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">            <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"linux1:9092,linux2:9092,linux3:9092"</span>,</span><br><span class="line">            <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"topicGroup"</span>,</span><br><span class="line">                <span class="string">"key.deserializer"</span> -&gt;</span><br><span class="line">                <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>,</span><br><span class="line">                <span class="string">"value.deserializer"</span> -&gt;</span><br><span class="line">                <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment">//4.读取 Kafka 数据创建 DStream</span></span><br><span class="line">        <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = </span><br><span class="line">            <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</span><br><span class="line">            <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>, </span><br><span class="line">            <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">"topicName"</span>), kafkaPara))</span><br><span class="line">        <span class="comment">//5.将每条消息的 KV 取出</span></span><br><span class="line">        <span class="keyword">val</span> valueDStream: <span class="type">DStream</span>[<span class="type">String</span>] = kafkaDStream.map(record =&gt; record.value())</span><br><span class="line">        <span class="comment">//6.计算 WordCount</span></span><br><span class="line">        valueDStream.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        .map((_, <span class="number">1</span>))</span><br><span class="line">        .reduceByKey(_ + _)</span><br><span class="line">        .print()</span><br><span class="line">        <span class="comment">//7.开启任务</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>查看 Kafka 消费进度  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-groups.sh --describe --bootstrap-server linux1:9092 --group topicGroup</span><br></pre></td></tr></table></figure>

<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.spark.streaming</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.&#123;<span class="type">ConsumerConfig</span>, <span class="type">ConsumerRecord</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">InputDStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.receiver.<span class="type">Receiver</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming04_Kafka</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO Kafka数据源</span></span><br><span class="line">    <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">    <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">// 定义 Kafka 参数</span></span><br><span class="line">    <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"bigdata1:6667,bigdata2:6667,bigdata3:6667"</span>,</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"test12"</span>,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>,</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> kafkaDataDS: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</span><br><span class="line">      <span class="comment">// 配置自动选择采集节点和执行节点的最优选择机制</span></span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">"test12"</span>), kafkaPara)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    kafkaDataDS.map(_.value()).print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1 id="第-4-章-DStream-转换"><a href="#第-4-章-DStream-转换" class="headerlink" title="第 4 章 DStream 转换"></a>第 4 章 DStream 转换</h1><p>DStream 上的操作与 RDD 的类似，分为 Transformations（转换）和 Output Operations（输出）两种，此外转换操作中还有一些比较特殊的原语，如： updateStateByKey()、 transform()以及各种 Window 相关的原语。</p>
<h2 id="4-1-无状态转化操作"><a href="#4-1-无状态转化操作" class="headerlink" title="4.1 无状态转化操作"></a>4.1 无状态转化操作</h2><p>无状态转化操作就是把简单的 RDD 转化操作应用到每个批次上，也就是转化 DStream 中的每一个 RDD。部分无状态转化操作列在了下表中。注意，针对键值对的 DStream 转化操作(比如reduceByKey())要添加 import StreamingContext._才能在 Scala 中使用。</p>
<p><img src="/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240722094219967.png" alt="image-20240722094219967"></p>
<p>需要记住的是，尽管这些函数看起来像作用在整个流上一样，但事实上每个 DStream 在内部是由许多 RDD（批次） 组成，且无状态转化操作是分别应用到每个 RDD 上的。<br>例如： reduceByKey()会归约每个时间区间中的数据，但不会归约不同区间之间的数据。  </p>
<h3 id="4-1-1-Transform"><a href="#4-1-1-Transform" class="headerlink" title="4.1.1 Transform"></a>4.1.1 Transform</h3><p>Transform 允许 DStream 上执行任意的 RDD-to-RDD 函数。即使这些函数并没有在 DStream的 API 中暴露出来，通过该函数可以方便的扩展 Spark API。该函数每一批次调度一次。其实也就是对 DStream 中的 RDD 应用转换。  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Transform</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//创建 SparkConf</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span></span><br><span class="line">        <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">        <span class="comment">//创建 StreamingContext</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">        <span class="comment">//创建 DStream</span></span><br><span class="line">        <span class="keyword">val</span> lineDStream: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"linux1"</span>, <span class="number">9999</span>)</span><br><span class="line">        <span class="comment">//转换为 RDD 操作</span></span><br><span class="line">        <span class="keyword">val</span> wordAndCountDStream: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = lineDStream.transform(rdd =&gt;</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">val</span> words: <span class="type">RDD</span>[<span class="type">String</span>] = rdd.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">            <span class="keyword">val</span> wordAndOne: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = words.map((_, <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">val</span> value: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordAndOne.reduceByKey(_ + _)</span><br><span class="line">            value</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//打印</span></span><br><span class="line">        wordAndCountDStream.print</span><br><span class="line">        <span class="comment">//启动</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.spark.streaming</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">DStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming06_State_Transform</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 无状态操作 transform</span></span><br><span class="line">    <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">    <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="comment">// transform 方法可以将底层RDD获取到后进行操作，用来解决DStream无法实现的功能</span></span><br><span class="line">    <span class="comment">// val newDS: DStream[String] = lines.transform(rdd =&gt; rdd.map(str =&gt; str))</span></span><br><span class="line">    <span class="comment">// val newDS1 = lines.map(str =&gt; str)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// transform 方法可以将底层RDD获取到后进行操作，有以下两种情形可以使用此方法</span></span><br><span class="line">    <span class="comment">// 1. DStream功能不完善</span></span><br><span class="line">    <span class="comment">// 2. 需要代码进行周期性的执行，周期性执行原因如下：</span></span><br><span class="line">    <span class="comment">// Code: Driver端</span></span><br><span class="line">    <span class="keyword">val</span> newDS: <span class="type">DStream</span>[<span class="type">String</span>] = lines.transform(</span><br><span class="line">      rdd =&gt; &#123;</span><br><span class="line">        <span class="comment">// Code: Driver端, (此处进行周期性执行，一个采集周期，一个rdd,然后此处对应就执行一遍相应的代码逻辑)</span></span><br><span class="line">        rdd.map(</span><br><span class="line">          str =&gt; &#123;</span><br><span class="line">            <span class="comment">// Code: Executor端</span></span><br><span class="line">            str</span><br><span class="line">          &#125;</span><br><span class="line">        )</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// Code: Driver端</span></span><br><span class="line">    <span class="keyword">val</span> newDS1 = lines.map(</span><br><span class="line">      str =&gt; &#123;</span><br><span class="line">        <span class="comment">// Code: Executor端</span></span><br><span class="line">        str</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-1-2-join"><a href="#4-1-2-join" class="headerlink" title="4.1.2 join"></a>4.1.2 join</h3><p>两个流之间的 join 需要两个流的批次大小一致，这样才能做到同时触发计算。计算过程就是对当前批次的两个流中各自的 RDD 进行 join，与两个 RDD 的 join 效果相同。  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JoinTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//1.创建 SparkConf</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"JoinTest"</span>)</span><br><span class="line">        <span class="comment">//2.创建 StreamingContext</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        <span class="comment">//3.从端口获取数据创建流</span></span><br><span class="line">        <span class="keyword">val</span> lineDStream1: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"linux1"</span>, <span class="number">9999</span>)</span><br><span class="line">        <span class="keyword">val</span> lineDStream2: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"linux2"</span>, <span class="number">8888</span>)</span><br><span class="line">        <span class="comment">//4.将两个流转换为 KV 类型</span></span><br><span class="line">        <span class="keyword">val</span> wordToOneDStream: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = lineDStream1.flatMap(_.split(<span class="string">""</span>)).map((_, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordToADStream: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = lineDStream2.flatMap(_.split(<span class="string">""</span>)).map((_, <span class="string">"a"</span>))</span><br><span class="line">        <span class="comment">//5.流的 JOIN</span></span><br><span class="line">        <span class="keyword">val</span> joinDStream: <span class="type">DStream</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">String</span>))] = wordToOneDStream.join(wordToADStream)</span><br><span class="line">        <span class="comment">//6.打印</span></span><br><span class="line">        joinDStream.print()</span><br><span class="line">        <span class="comment">//7.启动任务</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.spark.streaming</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">DStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming06_State_Join</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 无状态操作 join</span></span><br><span class="line">    <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">    <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> data9999 = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">val</span> data8888 = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">8888</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> map9999: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = data9999.map((_, <span class="number">9</span>))</span><br><span class="line">    <span class="keyword">val</span> map8888: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = data8888.map((_, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 所谓的DStream的Join操作，其实就是两个RDD的join</span></span><br><span class="line">    <span class="keyword">val</span> joinDS: <span class="type">DStream</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = map9999.join(map8888)</span><br><span class="line">    joinDS.print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="4-2-有状态转化操作"><a href="#4-2-有状态转化操作" class="headerlink" title="4.2 有状态转化操作"></a>4.2 有状态转化操作</h2><h3 id="4-2-1-UpdateStateByKey"><a href="#4-2-1-UpdateStateByKey" class="headerlink" title="4.2.1 UpdateStateByKey"></a>4.2.1 UpdateStateByKey</h3><p>UpdateStateByKey 原语用于记录历史记录，有时，我们需要在 DStream 中跨批次维护状态(例如流计算中累加 wordcount)。针对这种情况， updateStateByKey()为我们提供了对一个状态变量的访问，用于键值对形式的 DStream。给定一个由(键，事件)对构成的 DStream，并传递一个指定如何根据新的事件更新每个键对应状态的函数，它可以构建出一个新的 DStream，其内部数据为(键，状态) 对。<br>updateStateByKey() 的结果会是一个新的 DStream，其内部的 RDD 序列是由每个时间区间对应的(键，状态)对组成的。</p>
<p> updateStateByKey 操作使得我们可以在用新信息进行更新时保持任意的状态。为使用这个功能，需要做下面两步：</p>
<ol>
<li>定义状态，状态可以是一个任意的数据类型。</li>
<li>定义状态更新函数，用此函数阐明如何使用之前的状态和来自输入流的新值对状态进行更新。</li>
</ol>
<p>使用 updateStateByKey 需要对检查点目录进行配置，会使用检查点来保存状态。<br>更新版的 wordcount</p>
<p>1) 编写代码  </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WorldCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">        <span class="comment">// 定义更新状态方法，参数 values 为当前批次单词频度， state 为以往批次单词频度</span></span><br><span class="line">        <span class="keyword">val</span> updateFunc = (values: <span class="type">Seq</span>[<span class="type">Int</span>], state: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> currentCount = values.foldLeft(<span class="number">0</span>)(_ + _)</span><br><span class="line">            <span class="keyword">val</span> previousCount = state.getOrElse(<span class="number">0</span>)</span><br><span class="line">            <span class="type">Some</span>(currentCount + previousCount)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">        ssc.checkpoint(<span class="string">"./ck"</span>)</span><br><span class="line">        <span class="comment">// Create a DStream that will connect to hostname:port, like hadoop102:9999</span></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"linux1"</span>, <span class="number">9999</span>)</span><br><span class="line">        <span class="comment">// Split each line into words</span></span><br><span class="line">        <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="comment">// import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3</span></span><br><span class="line">        <span class="comment">// Count each word in each batch</span></span><br><span class="line">        <span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">        <span class="comment">// 使用 updateStateByKey 来更新状态，统计从运行开始以来单词总的次数</span></span><br><span class="line">        <span class="keyword">val</span> stateDstream = pairs.updateStateByKey[<span class="type">Int</span>](updateFunc)</span><br><span class="line">        stateDstream.print()</span><br><span class="line">        ssc.start() <span class="comment">// Start the computation</span></span><br><span class="line">        ssc.awaitTermination() <span class="comment">// Wait for the computation to terminate</span></span><br><span class="line">        <span class="comment">//ssc.stop()</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2) 启动程序并向 9999 端口发送数据  </p>
   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nc -lk 9999</span><br><span class="line">Hello World</span><br><span class="line">Hello Scala</span><br></pre></td></tr></table></figure>

<p>3) 结果展示  </p>
   <figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Time</span>: <span class="string">1504685175000 ms</span></span><br><span class="line"><span class="attr">-------------------------------------------</span></span><br><span class="line"><span class="attr">-------------------------------------------</span></span><br><span class="line"><span class="attr">Time</span>: <span class="string">1504685181000 ms</span></span><br><span class="line"><span class="attr">-------------------------------------------</span></span><br><span class="line"><span class="attr">(shi,1)</span></span><br><span class="line"><span class="attr">(shui,1)</span></span><br><span class="line"><span class="attr">(ni,1)</span></span><br><span class="line"><span class="attr">-------------------------------------------</span></span><br><span class="line"><span class="attr">Time</span>: <span class="string">1504685187000 ms</span></span><br><span class="line"><span class="attr">-------------------------------------------</span></span><br><span class="line"><span class="attr">(shi,1)</span></span><br><span class="line"><span class="attr">(ma,1)</span></span><br><span class="line"><span class="attr">(hao,1)</span></span><br><span class="line"><span class="attr">(shui,1)</span></span><br></pre></td></tr></table></figure>

<p>   练习与测试</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.spark.streaming</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming05_State</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 有状态操作 updateStateByKey</span></span><br><span class="line">    <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">    <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">// 使用有状态操作时，需要设定检查点路径。状态数据是在缓冲区中，这里进行设置缓冲区存放目录，也就是检查点目录</span></span><br><span class="line">    ssc.checkpoint(<span class="string">"cp"</span>)</span><br><span class="line">    <span class="comment">// 无状态数据操作，只对当前的采集收齐内的数据进行处理</span></span><br><span class="line">    <span class="comment">// 在某些场合下，需要进行有状态操作，如需要保留数据统计结果（状态），实现数据的汇总</span></span><br><span class="line">    <span class="keyword">val</span> datas = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> wordToOne = datas.map((_, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">// 这里只能进行每个微批中的聚合，不能进行所有批次中的单词计数统计，所以需要保存状态数据进行单词计数统计</span></span><br><span class="line">    <span class="comment">// val wordToCount = wordToOne.reduceByKey(_ + _)</span></span><br><span class="line">    <span class="comment">// updateStateByKey: 根据key对数据的状态进行更新</span></span><br><span class="line">    <span class="comment">// 传递参数中含有两个值</span></span><br><span class="line">    <span class="comment">// 第一个值表示相同的key的value数据</span></span><br><span class="line">    <span class="comment">// 第二个值表示缓冲区相同key的value数据</span></span><br><span class="line">    <span class="keyword">val</span> state = wordToOne.updateStateByKey(</span><br><span class="line">      (seq: <span class="type">Seq</span>[<span class="type">Int</span>], buffer: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> newCount = buffer.getOrElse(<span class="number">0</span>) + seq.sum</span><br><span class="line">        <span class="type">Option</span>(newCount)</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line">    state.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-2-2-WindowOperations"><a href="#4-2-2-WindowOperations" class="headerlink" title="4.2.2 WindowOperations"></a>4.2.2 WindowOperations</h3><p>Window Operations 可以设置窗口的大小和滑动窗口的间隔来动态的获取当前 Steaming 的允许状态。 所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长。</p>
<ul>
<li>窗口时长：计算内容的时间范围。</li>
<li>滑动步长：隔多久触发一次计算。  </li>
</ul>
<p>注意：这两者都必须为采集周期大小的整数倍。</p>
<p><img src="/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240722102650044.png" alt="image-20240722102650044"></p>
<p>WordCount 第三版： 3 秒一个批次，窗口 12 秒，滑步 6 秒。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WorldCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">        ssc.checkpoint(<span class="string">"./ck"</span>)</span><br><span class="line">        <span class="comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"linux1"</span>, <span class="number">9999</span>)</span><br><span class="line">        <span class="comment">// Split each line into words</span></span><br><span class="line">        <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="comment">// Count each word in each batch</span></span><br><span class="line">        <span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCounts = pairs.reduceByKeyAndWindow((a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; (a + b),<span class="type">Seconds</span>(<span class="number">12</span>), <span class="type">Seconds</span>(<span class="number">6</span>))</span><br><span class="line">        <span class="comment">// Print the first ten elements of each RDD generated in this DStream to the console</span></span><br><span class="line">        wordCounts.print()</span><br><span class="line">        ssc.start() <span class="comment">// Start the computation</span></span><br><span class="line">        ssc.awaitTermination() <span class="comment">// Wait for the computation to terminate</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>关于 Window 的操作还有如下方法：  </p>
<p>（1） window(windowLength, slideInterval): 基于对源 DStream 窗化的批次进行计算返回一个新的 DStream<br>（2） countByWindow(windowLength, slideInterval): 返回一个滑动窗口计数流中的元素个数<br>（3） reduceByWindow(func, windowLength, slideInterval): 通过使用自定义函数整合滑动区间流元素来创建一个新的单元素流<br>（4） reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]): 当在一个(K,V)对的 DStream 上调用此函数，会返回一个新(K,V)对的 DStream，此处通过对滑动窗口中批次数据使用 reduce 函数来整合每个 key 的 value 值<br>（5） reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]): 这个函数是上述函数的变化版本，每个窗口的 reduce 值都是通过用前一个窗的 reduce 值来递增计算。通过 reduce 进入到滑动窗口数据并”反向 reduce”离开窗口的旧数据来实现这个操作。一个例子是随着窗口滑动对 keys的“加”“减”计数。通过前边介绍可以想到，这个函数只适用于” 可逆的 reduce 函数”，也就是这些 reduce 函数有相应的”反 reduce”函数(以参数 invFunc 形式传入)。如前述函数， reduce 任务的数量通过可选参数来配置</p>
<p><img src="/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240722101745231.png" alt="image-20240722101745231"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ipDStream = accessLogsDStream.map(logEntry =&gt; (logEntry.getIpAddress(), <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> ipCountDStream = ipDStream.reduceByKeyAndWindow(</span><br><span class="line">    &#123;(x, y) =&gt; x + y&#125;,</span><br><span class="line">    &#123;(x, y) =&gt; x - y&#125;,</span><br><span class="line">    <span class="type">Seconds</span>(<span class="number">30</span>),</span><br><span class="line">    <span class="type">Seconds</span>(<span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"><span class="comment">//加上新进入窗口的批次中的元素 //移除离开窗口的老批次中的元素 //窗口时长// 滑动步长</span></span><br></pre></td></tr></table></figure>

<p>countByWindow()和 countByValueAndWindow()作为对数据进行计数操作的简写。<br>countByWindow()返回一个表示每个窗口中元素个数的 DStream，而 countByValueAndWindow() 返回的 DStream 则包含窗口中每个值的个数。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ipDStream = accessLogsDStream.map&#123;entry =&gt; entry.getIpAddress()&#125;</span><br><span class="line"><span class="keyword">val</span> ipAddressRequestCount = ipDStream.countByValueAndWindow(<span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">val</span> requestCount = accessLogsDStream.countByWindow(<span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.spark.streaming</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">DStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming06_State_Window</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 有状态操作 Window</span></span><br><span class="line">    <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">    <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">val</span> wordToOne = lines.map((_, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">// 窗口的范围应该是采集周期的整数倍</span></span><br><span class="line">    <span class="comment">// 窗口是可以滑动的，默认情况下，是以一个采集周期的进行滑动</span></span><br><span class="line">    <span class="comment">// val windowDS: DStream[(String, Int)] = wordToOne.window(Seconds(6))</span></span><br><span class="line">    <span class="comment">// 以一个采集周期的进行滑动，可能会出现重复数据的计算，为了避免这种情况，可以改变滑动的幅度，也就是步长,步长如果和窗口范围一致，则变成了滚动，如果步长大于或等于窗口周期，此时窗口统计不会有重复数据</span></span><br><span class="line">    <span class="comment">// 步长也必须为采集周期大小的整数倍</span></span><br><span class="line">    <span class="keyword">val</span> windowDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOne.window(<span class="type">Seconds</span>(<span class="number">6</span>),<span class="type">Seconds</span>(<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> wordToCount = windowDS.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    wordToCount.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.spark.streaming</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">DStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming06_State_Window1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 有状态操作 特殊的reduceByKeyAndWindow</span></span><br><span class="line">    <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">    <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">// 这里要进行保存状态，因为某些数据不能重复计算，要进行减掉，所以要先存在缓冲区中，也就是状态中。</span></span><br><span class="line">    ssc.checkpoint(<span class="string">"cp"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">val</span> wordToOne = lines.map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 当窗口范围比较大，但是滑动幅度比较小，那么可以采用增加数据和减少数据的方式，无需重复计算，目的是提升性能</span></span><br><span class="line"><span class="comment">     * reduceByKeyAndWindow:</span></span><br><span class="line"><span class="comment">     * 参数1：新增的数据加进来</span></span><br><span class="line"><span class="comment">     * 参数2：过去的数据减掉</span></span><br><span class="line"><span class="comment">     * 参数3：窗口范围</span></span><br><span class="line"><span class="comment">     * 参数4：步长</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> windowDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOne.reduceByKeyAndWindow(</span><br><span class="line">      _ + _,</span><br><span class="line">      _ - _,</span><br><span class="line">      <span class="type">Seconds</span>(<span class="number">9</span>),</span><br><span class="line">      <span class="type">Seconds</span>(<span class="number">3</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    windowDS.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="第-5-章-DStream-输出"><a href="#第-5-章-DStream-输出" class="headerlink" title="第 5 章 DStream 输出"></a>第 5 章 DStream 输出</h1><p>输出操作指定了对流数据经转化操作得到的数据所要执行的操作(例如把结果推入外部数据库或输出到屏幕上)。与 RDD 中的惰性求值类似，如果一个 DStream 及其派生出的 DStream 都没有被执行输出操作，那么这些 DStream 就都不会被求值。如果 StreamingContext 中没有设定输出操作，整个 context 就都不会启动。  </p>
<p>输出操作如下：  </p>
<ul>
<li>print()：在运行流程序的驱动结点上打印 DStream 中每一批次数据的最开始 10 个元素。这用于开发和调试。在 Python API 中，同样的操作叫 print()。</li>
<li>saveAsTextFiles(prefix, [suffix])：以 text 文件形式存储这个 DStream 的内容。每一批次的存储文件名基于参数中的 prefix 和 suffix。” prefix-Time_IN_MS[.suffix]”。</li>
<li>saveAsObjectFiles(prefix, [suffix])：以 Java 对象序列化的方式将 Stream 中的数据保存为SequenceFiles . 每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”. Python 中目前不可用。</li>
<li>saveAsHadoopFiles(prefix, [suffix])：将 Stream 中的数据保存为 Hadoop files. 每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”。 Python API 中目前不可用。</li>
<li>foreachRDD(func)：这是最通用的输出操作，即将函数 func 用于产生于 stream 的每一个RDD。其中参数传入的函数 func 应该实现将每一个 RDD 中数据推送到外部系统，如将RDD 存入文件或者通过网络将其写入数据库。  </li>
</ul>
<p>通用的输出操作 foreachRDD()，它用来对 DStream 中的 RDD 运行任意计算。这和 transform()有些类似，都可以让我们访问任意 RDD。在 foreachRDD()中，可以重用我们在 Spark 中实现的所有行动操作。比如，常见的用例之一是把数据写到诸如 MySQL 的外部数据库中。</p>
<p><strong>注意：</strong>  </p>
<p>1) 连接不能写在 driver 层面（序列化）<br>2) 如果写在 foreach 则每个 RDD 中的每一条数据都创建，得不偿失<br>3) 增加 foreachPartition，在分区创建（获取）</p>
<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.spark.streaming</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">DStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming07_Output</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO</span></span><br><span class="line">    <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">    <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">// 这里要进行保存状态，因为某些数据不能重复计算，要进行减掉，所以要先存在缓冲区中，也就是状态中。</span></span><br><span class="line">    ssc.checkpoint(<span class="string">"cp"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">val</span> wordToOne = lines.map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 当窗口范围比较大，但是滑动幅度比较小，那么可以采用增加数据和减少数据的方式，无需重复计算，目的是提升性能</span></span><br><span class="line"><span class="comment">     * reduceByKeyAndWindow:</span></span><br><span class="line"><span class="comment">     * 参数1：新增的数据加进来</span></span><br><span class="line"><span class="comment">     * 参数2：过去的数据减掉</span></span><br><span class="line"><span class="comment">     * 参数3：窗口范围</span></span><br><span class="line"><span class="comment">     * 参数4：步长</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> windowDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOne.reduceByKeyAndWindow(</span><br><span class="line">      _ + _,</span><br><span class="line">      _ - _,</span><br><span class="line">      <span class="type">Seconds</span>(<span class="number">9</span>),</span><br><span class="line">      <span class="type">Seconds</span>(<span class="number">3</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// SparkStreaming如果没有输出操作，那么会提示错误</span></span><br><span class="line">    <span class="comment">// windowDS.print()</span></span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.spark.streaming</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">DStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming07_Output1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO</span></span><br><span class="line">    <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">    <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">// 这里要进行保存状态，因为某些数据不能重复计算，要进行减掉，所以要先存在缓冲区中，也就是状态中。</span></span><br><span class="line">    ssc.checkpoint(<span class="string">"cp"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">val</span> wordToOne = lines.map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> windowDS: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOne.reduceByKeyAndWindow(</span><br><span class="line">      _ + _,</span><br><span class="line">      _ - _,</span><br><span class="line">      <span class="type">Seconds</span>(<span class="number">9</span>),</span><br><span class="line">      <span class="type">Seconds</span>(<span class="number">3</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// foreachRDD不会出现时间戳</span></span><br><span class="line">    windowDS.foreachRDD(rdd =&gt; &#123;</span><br><span class="line"></span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="第-6-章-优雅关闭"><a href="#第-6-章-优雅关闭" class="headerlink" title="第 6 章 优雅关闭"></a>第 6 章 优雅关闭</h1><p>流式任务需要 7*24 小时执行，但是有时涉及到升级代码需要主动停止程序，但是分布式程序，没办法做到一个个进程去杀死，所有配置优雅的关闭就显得至关重要了。使用外部文件系统来控制内部程序关闭。</p>
<ul>
<li><p>MonitorStop  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.net.<span class="type">URI</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.&#123;<span class="type">FileSystem</span>, <span class="type">Path</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">StreamingContext</span>, <span class="type">StreamingContextState</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MonitorStop</span>(<span class="params">ssc: <span class="type">StreamingContext</span></span>) <span class="keyword">extends</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> fs: <span class="type">FileSystem</span> = <span class="type">FileSystem</span>.get(<span class="keyword">new</span> <span class="type">URI</span>(<span class="string">"hdfs://linux1:9000"</span>), <span class="keyword">new</span> <span class="type">Configuration</span>(), <span class="string">"atguigu"</span>)</span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span></span><br><span class="line">            	<span class="type">Thread</span>.sleep(<span class="number">5000</span>)</span><br><span class="line">            <span class="keyword">catch</span> &#123;</span><br><span class="line">            	<span class="keyword">case</span> e: <span class="type">InterruptedException</span> =&gt; e.printStackTrace()</span><br><span class="line">        	&#125;</span><br><span class="line">            <span class="keyword">val</span> state: <span class="type">StreamingContextState</span> = ssc.getState</span><br><span class="line">            <span class="keyword">val</span> bool: <span class="type">Boolean</span> = fs.exists(<span class="keyword">new</span> <span class="type">Path</span>(<span class="string">"hdfs://linux1:9000/stopSpark"</span>))</span><br><span class="line">            <span class="keyword">if</span> (bool) &#123;</span><br><span class="line">                <span class="keyword">if</span> (state == <span class="type">StreamingContextState</span>.<span class="type">ACTIVE</span>) &#123;</span><br><span class="line">                	ssc.stop(stopSparkContext = <span class="literal">true</span>, stopGracefully = <span class="literal">true</span>)</span><br><span class="line">               		<span class="type">System</span>.exit(<span class="number">0</span>)</span><br><span class="line">            	&#125;</span><br><span class="line">        	&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>SparkTest</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createSSC</span></span>(): _root_.org.apache.spark.streaming.<span class="type">StreamingContext</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> update: (<span class="type">Seq</span>[<span class="type">Int</span>], <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; <span class="type">Some</span>[<span class="type">Int</span>] = (values: <span class="type">Seq</span>[<span class="type">Int</span>], status: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">            <span class="comment">//当前批次内容的计算</span></span><br><span class="line">            <span class="keyword">val</span> sum: <span class="type">Int</span> = values.sum</span><br><span class="line">            <span class="comment">//取出状态信息中上一次状态</span></span><br><span class="line">            <span class="keyword">val</span> lastStatu: <span class="type">Int</span> = status.getOrElse(<span class="number">0</span>)</span><br><span class="line">            <span class="type">Some</span>(sum + lastStatu)</span><br><span class="line">    	&#125;</span><br><span class="line">        <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[4]"</span>).setAppName(<span class="string">"SparkTest"</span>)</span><br><span class="line">        <span class="comment">//设置优雅的关闭</span></span><br><span class="line">        sparkConf.set(<span class="string">"spark.streaming.stopGracefullyOnShutdown"</span>, <span class="string">"true"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.checkpoint(<span class="string">"./ck"</span>)</span><br><span class="line">        <span class="keyword">val</span> line: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"linux1"</span>, <span class="number">9999</span>)</span><br><span class="line">        <span class="keyword">val</span> word: <span class="type">DStream</span>[<span class="type">String</span>] = line.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordAndOne: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = word.map((_, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> wordAndCount: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordAndOne.updateStateByKey(update)</span><br><span class="line">        wordAndCount.print()</span><br><span class="line">        ssc</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> ssc: <span class="type">StreamingContext</span> = <span class="type">StreamingContext</span>.getActiveOrCreate(<span class="string">"./ck"</span>, () =&gt; createSSC())</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Thread</span>(<span class="keyword">new</span> <span class="type">MonitorStop</span>(ssc)).start()</span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.spark.streaming</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">DStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>, <span class="type">StreamingContextState</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming08_Close</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 优雅关闭</span></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 线程的关闭：</span></span><br><span class="line"><span class="comment">     * val thread = new Thread()</span></span><br><span class="line"><span class="comment">     * thread.start()</span></span><br><span class="line"><span class="comment">     * thread.stop() // 强制关闭</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">val</span> wordToOne = lines.map((_, <span class="number">1</span>))</span><br><span class="line">    wordToOne.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果想要关闭采集器，那么需要创建新线程</span></span><br><span class="line">    <span class="comment">// 而且需要在第三方程序中增加关闭状态</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Thread</span>(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 优雅的关闭</span></span><br><span class="line">        <span class="comment">// 第二个参数，使采集器不再接收数据</span></span><br><span class="line">        <span class="comment">// 计算节点不再接收新的数据，而是将现有的数据处理完毕，然后关闭,主要是第二个参数起作用</span></span><br><span class="line">        <span class="comment">// Mysql: Table(stopSpark) =&gt; Row =&gt; data</span></span><br><span class="line">        <span class="comment">// Redis: Data(K-V)</span></span><br><span class="line">        <span class="comment">// ZK: /stopSpark</span></span><br><span class="line">        <span class="comment">// HDFS: /stopSpark</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 最佳实践</span></span><br><span class="line">        <span class="comment">// while (true) &#123;</span></span><br><span class="line">        <span class="comment">//   if (true) &#123;</span></span><br><span class="line">        <span class="comment">//     获取SparkStreaming状态</span></span><br><span class="line">        <span class="comment">//     val state: StreamingContextState = ssc.getState()</span></span><br><span class="line">        <span class="comment">//     if (state == StreamingContextState.ACTIVE) &#123;</span></span><br><span class="line">        <span class="comment">//       ssc.stop(true, true)</span></span><br><span class="line">        <span class="comment">//     &#125;</span></span><br><span class="line">        <span class="comment">//   &#125;</span></span><br><span class="line">        <span class="comment">//   Thread.sleep(5000)</span></span><br><span class="line">        <span class="comment">// &#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 简单测试</span></span><br><span class="line">        <span class="type">Thread</span>.sleep(<span class="number">5000</span>)</span><br><span class="line">        <span class="comment">// 获取SparkStreaming状态</span></span><br><span class="line">        <span class="keyword">val</span> state: <span class="type">StreamingContextState</span> = ssc.getState()</span><br><span class="line">        <span class="keyword">if</span> (state == <span class="type">StreamingContextState</span>.<span class="type">ACTIVE</span>) &#123;</span><br><span class="line">          <span class="comment">// 优雅的关闭，不直接强制关闭，而是让计算节点不再接收新的数据，而是把当前的数据处理完毕之后再关闭</span></span><br><span class="line">          ssc.stop(<span class="literal">true</span>, <span class="literal">true</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;).start()</span><br><span class="line"></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.spark.streaming</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>, <span class="type">StreamingContextState</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming09_Resume</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 恢复数据</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="type">StreamingContext</span>.getActiveOrCreate(<span class="string">"cp"</span>, ()=&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">      <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">      <span class="keyword">val</span> wordToOne = lines.map((_, <span class="number">1</span>))</span><br><span class="line">      wordToOne.print()</span><br><span class="line">      ssc</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.checkpoint(<span class="string">"cp"</span>)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h1 id="第-7-章-SparkStreaming-案例实操"><a href="#第-7-章-SparkStreaming-案例实操" class="headerlink" title="第 7 章 SparkStreaming 案例实操"></a>第 7 章 SparkStreaming 案例实操</h1><h2 id="7-1-环境准备"><a href="#7-1-环境准备" class="headerlink" title="7.1 环境准备"></a>7.1 环境准备</h2><h3 id="7-1-1-pom-文件"><a href="#7-1-1-pom-文件" class="headerlink" title="7.1.1 pom 文件"></a>7.1.1 pom 文件</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/com.alibaba/druid --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>druid<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1.10<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.fasterxml.jackson.core<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jackson-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.10.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="7-1-2-工具类"><a href="#7-1-2-工具类" class="headerlink" title="7.1.2 工具类"></a>7.1.2 工具类</h3><ul>
<li><p>PropertiesUtil</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.<span class="type">InputStreamReader</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PropertiesUtil</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span></span>(propertiesName:<span class="type">String</span>): <span class="type">Properties</span> =&#123;</span><br><span class="line">        <span class="keyword">val</span> prop=<span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">        prop.load(<span class="keyword">new</span> <span class="type">InputStreamReader</span>(<span class="type">Thread</span>.currentThread().getContextClassLoader.getResourceAsStream(propertiesName) , <span class="string">"UTF-8"</span>))</span><br><span class="line">        prop</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="7-2-实时数据生成模块"><a href="#7-2-实时数据生成模块" class="headerlink" title="7.2 实时数据生成模块"></a>7.2 实时数据生成模块</h2><ul>
<li><p>config.properties  </p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#jdbc 配置</span></span><br><span class="line"><span class="meta">jdbc.datasource.size</span>=<span class="string">10</span></span><br><span class="line"><span class="meta">jdbc.url</span>=<span class="string">jdbc:mysql://linux1:3306/spark2020?useUnicode=true&amp;characterEncoding=utf</span></span><br><span class="line"><span class="meta">8&amp;rewriteBatchedStatements</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">jdbc.user</span>=<span class="string">root</span></span><br><span class="line"><span class="meta">jdbc.password</span>=<span class="string">000000</span></span><br><span class="line"><span class="comment"># Kafka 配置</span></span><br><span class="line"><span class="meta">kafka.broker.list</span>=<span class="string">linux1:9092,linux2:9092,linux3:9092</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>CityInfo</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* 城市信息表</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* @param city_id 城市 id</span></span><br><span class="line"><span class="comment">* @param city_name 城市名称</span></span><br><span class="line"><span class="comment">* @param area 城市所在大区</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">CityInfo</span> (<span class="params">city_id:<span class="type">Long</span>, city_name:<span class="type">String</span>, area:<span class="type">String</span></span>)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>RandomOptions</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">RanOpt</span>[<span class="type">T</span>](<span class="params">value: <span class="type">T</span>, weight: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">RandomOptions</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>[<span class="type">T</span>](opts: <span class="type">RanOpt</span>[<span class="type">T</span>]*): <span class="type">RandomOptions</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">        <span class="keyword">val</span> randomOptions = <span class="keyword">new</span> <span class="type">RandomOptions</span>[<span class="type">T</span>]()</span><br><span class="line">        <span class="keyword">for</span> (opt &lt;- opts) &#123;</span><br><span class="line">            randomOptions.totalWeight += opt.weight</span><br><span class="line">            <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to opt.weight) &#123;</span><br><span class="line">            	randomOptions.optsBuffer += opt.value</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        randomOptions</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">RandomOptions</span>[<span class="type">T</span>](<span class="params">opts: <span class="type">RanOpt</span>[<span class="type">T</span>]*</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">var</span> totalWeight = <span class="number">0</span></span><br><span class="line">        <span class="keyword">var</span> optsBuffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">T</span>]</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">getRandomOpt</span></span>: <span class="type">T</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> randomNum: <span class="type">Int</span> = <span class="keyword">new</span> <span class="type">Random</span>().nextInt(totalWeight)</span><br><span class="line">        optsBuffer(randomNum)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>MockerRealTime  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">Properties</span>, <span class="type">Random</span>&#125;</span><br><span class="line"><span class="keyword">import</span> com.test.bean.<span class="type">CityInfo</span></span><br><span class="line"><span class="keyword">import</span> com.atguigu.utils.&#123;<span class="type">PropertiesUtil</span>, <span class="type">RanOpt</span>, <span class="type">RandomOptions</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.&#123;<span class="type">KafkaProducer</span>, <span class="type">ProducerConfig</span>,<span class="type">ProducerRecord</span>&#125;</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MockerRealTime</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 模拟的数据</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 格式 ： timestamp area city userid adid</span></span><br><span class="line"><span class="comment">    * 某个时间点 某个地区 某个城市 某个用户 某个广告</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generateMockData</span></span>(): <span class="type">Array</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">        <span class="keyword">val</span> array: <span class="type">ArrayBuffer</span>[<span class="type">String</span>] = <span class="type">ArrayBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">        <span class="keyword">val</span> <span class="type">CityRandomOpt</span> = <span class="type">RandomOptions</span>(<span class="type">RanOpt</span>(<span class="type">CityInfo</span>(<span class="number">1</span>, <span class="string">"北京"</span>, <span class="string">"华北"</span>), <span class="number">30</span>),</span><br><span class="line">        <span class="type">RanOpt</span>(<span class="type">CityInfo</span>(<span class="number">2</span>, <span class="string">"上海"</span>, <span class="string">"华东"</span>), <span class="number">30</span>),</span><br><span class="line">        <span class="type">RanOpt</span>(<span class="type">CityInfo</span>(<span class="number">3</span>, <span class="string">"广州"</span>, <span class="string">"华南"</span>), <span class="number">10</span>),</span><br><span class="line">        <span class="type">RanOpt</span>(<span class="type">CityInfo</span>(<span class="number">4</span>, <span class="string">"深圳"</span>, <span class="string">"华南"</span>), <span class="number">20</span>),</span><br><span class="line">        <span class="type">RanOpt</span>(<span class="type">CityInfo</span>(<span class="number">5</span>, <span class="string">"天津"</span>, <span class="string">"华北"</span>), <span class="number">10</span>))</span><br><span class="line">        <span class="keyword">val</span> random = <span class="keyword">new</span> <span class="type">Random</span>()</span><br><span class="line">        <span class="comment">// 模拟实时数据：</span></span><br><span class="line">        <span class="comment">// timestamp province city userid adid</span></span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">0</span> to <span class="number">50</span>) &#123;</span><br><span class="line">            <span class="keyword">val</span> timestamp: <span class="type">Long</span> = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">            <span class="keyword">val</span> cityInfo: <span class="type">CityInfo</span> = <span class="type">CityRandomOpt</span>.getRandomOpt</span><br><span class="line">            <span class="keyword">val</span> city: <span class="type">String</span> = cityInfo.city_name</span><br><span class="line">            <span class="keyword">val</span> area: <span class="type">String</span> = cityInfo.area</span><br><span class="line">            <span class="keyword">val</span> adid: <span class="type">Int</span> = <span class="number">1</span> + random.nextInt(<span class="number">6</span>)</span><br><span class="line">            <span class="keyword">val</span> userid: <span class="type">Int</span> = <span class="number">1</span> + random.nextInt(<span class="number">6</span>)</span><br><span class="line">            <span class="comment">// 拼接实时数据</span></span><br><span class="line">            array += timestamp + <span class="string">" "</span> + area + <span class="string">" "</span> + city + <span class="string">" "</span> + userid + <span class="string">" "</span> + adid</span><br><span class="line">        &#125;</span><br><span class="line">        array.toArray</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createKafkaProducer</span></span>(broker: <span class="type">String</span>): <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>] = &#123;</span><br><span class="line">        <span class="comment">// 创建配置对象</span></span><br><span class="line">        <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">        <span class="comment">// 添加配置</span></span><br><span class="line">        prop.put(<span class="type">ProducerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span>, broker)</span><br><span class="line">        prop.put(<span class="type">ProducerConfig</span>.<span class="type">KEY_SERIALIZER_CLASS_CONFIG</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>)</span><br><span class="line">        prop.put(<span class="type">ProducerConfig</span>.<span class="type">VALUE_SERIALIZER_CLASS_CONFIG</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>)</span><br><span class="line">        <span class="comment">// 根据配置创建 Kafka 生产者</span></span><br><span class="line">        <span class="keyword">new</span> <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>](prop)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 获取配置文件 config.properties 中的 Kafka 配置参数</span></span><br><span class="line">        <span class="keyword">val</span> config: <span class="type">Properties</span> = <span class="type">PropertiesUtil</span>.load(<span class="string">"config.properties"</span>)</span><br><span class="line">        <span class="keyword">val</span> broker: <span class="type">String</span> = config.getProperty(<span class="string">"kafka.broker.list"</span>)</span><br><span class="line">        <span class="keyword">val</span> topic = <span class="string">"test"</span></span><br><span class="line">        <span class="comment">// 创建 Kafka 消费者</span></span><br><span class="line">        <span class="keyword">val</span> kafkaProducer: <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>] = createKafkaProducer(broker)</span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 随机产生实时数据并通过 Kafka 生产者发送到 Kafka 集群中</span></span><br><span class="line">            <span class="keyword">for</span> (line &lt;- generateMockData()) &#123;</span><br><span class="line">                kafkaProducer.send(<span class="keyword">new</span> <span class="type">ProducerRecord</span>[<span class="type">String</span>, <span class="type">String</span>](topic, line))</span><br><span class="line">                println(line)</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">2000</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.spark.streaming</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.&#123;<span class="type">KafkaProducer</span>, <span class="type">ProducerConfig</span>, <span class="type">ProducerRecord</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author zrquan</span></span><br><span class="line"><span class="comment"> * @Date 2024/7/5 9:56</span></span><br><span class="line"><span class="comment"> * @Email zhangruiquan@genew.com</span></span><br><span class="line"><span class="comment"> * @Package com.atguigu.spark.streaming</span></span><br><span class="line"><span class="comment"> * @ClassName SparkStreaming10_MockData</span></span><br><span class="line"><span class="comment"> * @Description</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming10_MockData</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//TODO 案例实操-生成模拟数据</span></span><br><span class="line">    <span class="comment">//格式：timestamp area city userid adid</span></span><br><span class="line">    <span class="comment">//含义： 时间戳    区域  城市  用户    广告</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Application =&gt; Kafka =&gt; SparkStreaming =&gt; Analysis</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建Kafka配置对象</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    <span class="comment">// 添加配置</span></span><br><span class="line">    prop.put(<span class="type">ProducerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span>, <span class="string">"bigdata1:6667"</span>)</span><br><span class="line">    prop.put(<span class="type">ProducerConfig</span>.<span class="type">KEY_SERIALIZER_CLASS_CONFIG</span>,</span><br><span class="line">      <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>)</span><br><span class="line">    prop.put(<span class="type">ProducerConfig</span>.<span class="type">VALUE_SERIALIZER_CLASS_CONFIG</span>,</span><br><span class="line">      <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>)</span><br><span class="line">    <span class="comment">// 根据配置创建 Kafka 生产者</span></span><br><span class="line">    <span class="keyword">val</span> kakfaProducer = <span class="keyword">new</span> <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>](prop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">      mockdata().foreach(</span><br><span class="line">        data =&gt; &#123;</span><br><span class="line">          println(data)</span><br><span class="line">          <span class="comment">// 向Kafka中生成数据</span></span><br><span class="line">          <span class="keyword">val</span> record = <span class="keyword">new</span> <span class="type">ProducerRecord</span>[<span class="type">String</span>,<span class="type">String</span>](<span class="string">"test1212"</span>,data)</span><br><span class="line">          kakfaProducer.send(record)</span><br><span class="line">        &#125;</span><br><span class="line">      )</span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">2000</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mockdata</span></span>(): <span class="type">ListBuffer</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">      <span class="keyword">val</span> list = <span class="type">ListBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">      <span class="keyword">val</span> areaList = <span class="type">ListBuffer</span>[<span class="type">String</span>](<span class="string">"华北"</span>,<span class="string">"华东"</span>,<span class="string">"华南"</span>)</span><br><span class="line">      <span class="keyword">val</span> cityList = <span class="type">ListBuffer</span>[<span class="type">String</span>](<span class="string">"北京"</span>,<span class="string">"上海"</span>,<span class="string">"深圳"</span>)</span><br><span class="line">      <span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">30</span>) &#123;</span><br><span class="line">        <span class="keyword">val</span> area = areaList(<span class="keyword">new</span> <span class="type">Random</span>().nextInt(<span class="number">3</span>))</span><br><span class="line">        <span class="keyword">val</span> city = cityList(<span class="keyword">new</span> <span class="type">Random</span>().nextInt(<span class="number">3</span>))</span><br><span class="line">        <span class="keyword">val</span> userid = <span class="keyword">new</span> <span class="type">Random</span>().nextInt(<span class="number">6</span>) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">val</span> adid = <span class="keyword">new</span> <span class="type">Random</span>().nextInt(<span class="number">6</span>) + <span class="number">1</span></span><br><span class="line">        list.append(<span class="string">s"<span class="subst">$&#123;System.currentTimeMillis()&#125;</span> <span class="subst">$&#123;area&#125;</span> <span class="subst">$&#123;city&#125;</span> <span class="subst">$&#123;userid&#125;</span> <span class="subst">$&#123;adid&#125;</span>"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      list</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="7-3-需求一：广告黑名单"><a href="#7-3-需求一：广告黑名单" class="headerlink" title="7.3 需求一：广告黑名单"></a>7.3 需求一：广告黑名单</h2><p>实现实时的动态黑名单机制：将每天对某个广告点击超过 100 次的用户拉黑。<br>注：黑名单保存到 MySQL 中。  </p>
<h3 id="7-3-1-思路分析"><a href="#7-3-1-思路分析" class="headerlink" title="7.3.1 思路分析"></a>7.3.1 思路分析</h3><p>1）读取 Kafka 数据之后，并对 MySQL 中存储的黑名单数据做校验；<br>2）校验通过则对给用户点击广告次数累加一并存入 MySQL；<br>3）在存入 MySQL 之后对数据做校验，如果单日超过 100 次则将该用户加入黑名单。  </p>
<p><img src="/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240725181435513.png" alt="image-20240725181435513"></p>
<h3 id="7-3-2-MySQL-建表"><a href="#7-3-2-MySQL-建表" class="headerlink" title="7.3.2 MySQL 建表"></a>7.3.2 MySQL 建表</h3><p>创建库 spark2020  </p>
<p>1）存放黑名单用户的表  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> black_list (userid <span class="built_in">CHAR</span>(<span class="number">1</span>) PRIMARY <span class="keyword">KEY</span>);</span><br></pre></td></tr></table></figure>

<p>2）存放单日各用户点击每个广告的次数  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_ad_count (</span><br><span class="line">    dt <span class="built_in">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    userid <span class="built_in">CHAR</span> (<span class="number">1</span>),</span><br><span class="line">    adid <span class="built_in">CHAR</span> (<span class="number">1</span>),</span><br><span class="line">    <span class="keyword">count</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">    PRIMARY <span class="keyword">KEY</span> (dt, userid, adid)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h3 id="7-3-3-环境准备"><a href="#7-3-3-环境准备" class="headerlink" title="7.3.3 环境准备"></a>7.3.3 环境准备</h3><p>绝大部分时候都是对接的 Kafka 数据源，创建一个 SparkStreaming 读取 Kafka 数据的工具类。  接下来开始实时需求的分析，需要用到 SparkStreaming 来做实时数据的处理，在生产环境中，</p>
<ul>
<li><p>MyKafkaUtil  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerRecord</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.<span class="type">StreamingContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">InputDStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyKafkaUtil</span> </span>&#123;</span><br><span class="line">    <span class="comment">//1.创建配置信息对象</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> properties: <span class="type">Properties</span> = <span class="type">PropertiesUtil</span>.load(<span class="string">"config.properties"</span>)</span><br><span class="line">    <span class="comment">//2.用于初始化链接到集群的地址</span></span><br><span class="line">    <span class="keyword">val</span> broker_list: <span class="type">String</span> = properties.getProperty(<span class="string">"kafka.broker.list"</span>)</span><br><span class="line">    <span class="comment">//3.kafka 消费者配置</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParam = <span class="type">Map</span>(</span><br><span class="line">        <span class="string">"bootstrap.servers"</span> -&gt; broker_list,</span><br><span class="line">        <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">        <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">        <span class="comment">//消费者组</span></span><br><span class="line">        <span class="string">"group.id"</span> -&gt; <span class="string">"commerce-consumer-group"</span>,</span><br><span class="line">        <span class="comment">//如果没有初始化偏移量或者当前的偏移量不存在任何服务器上，可以使用这个配置属性</span></span><br><span class="line">        <span class="comment">//可以使用这个配置， latest 自动重置偏移量为最新的偏移量</span></span><br><span class="line">        <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"latest"</span>,</span><br><span class="line">        <span class="comment">//如果是 true，则这个消费者的偏移量会在后台自动提交,但是 kafka 宕机容易丢失数据</span></span><br><span class="line">        <span class="comment">//如果是 false，会需要手动维护 kafka 偏移量</span></span><br><span class="line">        <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">true</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// 创建 DStream，返回接收到的输入数据</span></span><br><span class="line">    <span class="comment">// LocationStrategies：根据给定的主题和集群地址创建 consumer</span></span><br><span class="line">    <span class="comment">// LocationStrategies.PreferConsistent：持续的在所有 Executor 之间分配分区</span></span><br><span class="line">    <span class="comment">// ConsumerStrategies：选择如何在 Driver 和 Executor 上创建和配置 Kafka Consumer</span></span><br><span class="line">    <span class="comment">// ConsumerStrategies.Subscribe：订阅一系列主题</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getKafkaStream</span></span>(topic: <span class="type">String</span>, ssc: <span class="type">StreamingContext</span>): <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = &#123;</span><br><span class="line">        <span class="keyword">val</span> dStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</span><br><span class="line">        <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>, <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Array</span>(topic), kafkaParam))</span><br><span class="line">        dStream</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>JdbcUtil</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.&#123;<span class="type">Connection</span>, <span class="type">PreparedStatement</span>, <span class="type">ResultSet</span>&#125;</span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">import</span> javax.sql.<span class="type">DataSource</span></span><br><span class="line"><span class="keyword">import</span> com.alibaba.druid.pool.<span class="type">DruidDataSourceFactory</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JdbcUtil</span> </span>&#123;</span><br><span class="line">    <span class="comment">//初始化连接池</span></span><br><span class="line">    <span class="keyword">var</span> dataSource: <span class="type">DataSource</span> = init()</span><br><span class="line">    <span class="comment">//初始化连接池方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init</span></span>(): <span class="type">DataSource</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">        <span class="keyword">val</span> config: <span class="type">Properties</span> = <span class="type">PropertiesUtil</span>.load(<span class="string">"config.properties"</span>)</span><br><span class="line">        properties.setProperty(<span class="string">"driverClassName"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">        properties.setProperty(<span class="string">"url"</span>, config.getProperty(<span class="string">"jdbc.url"</span>))</span><br><span class="line">        properties.setProperty(<span class="string">"username"</span>, config.getProperty(<span class="string">"jdbc.user"</span>))</span><br><span class="line">        properties.setProperty(<span class="string">"password"</span>, config.getProperty(<span class="string">"jdbc.password"</span>))</span><br><span class="line">        properties.setProperty(<span class="string">"maxActive"</span>,</span><br><span class="line">        config.getProperty(<span class="string">"jdbc.datasource.size"</span>))</span><br><span class="line">        <span class="type">DruidDataSourceFactory</span>.createDataSource(properties)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//获取 MySQL 连接</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getConnection</span></span>: <span class="type">Connection</span> = &#123;</span><br><span class="line">    	dataSource.getConnection</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//执行 SQL 语句,单条数据插入</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">executeUpdate</span></span>(connection: <span class="type">Connection</span>, sql: <span class="type">String</span>, params: <span class="type">Array</span>[<span class="type">Any</span>]): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="keyword">var</span> rtn = <span class="number">0</span></span><br><span class="line">        <span class="keyword">var</span> pstmt: <span class="type">PreparedStatement</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            connection.setAutoCommit(<span class="literal">false</span>)</span><br><span class="line">            pstmt = connection.prepareStatement(sql)</span><br><span class="line">            <span class="keyword">if</span> (params != <span class="literal">null</span> &amp;&amp; params.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (i &lt;- params.indices) &#123;</span><br><span class="line">                	pstmt.setObject(i + <span class="number">1</span>, params(i))</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            rtn = pstmt.executeUpdate()</span><br><span class="line">            connection.commit()</span><br><span class="line">            pstmt.close()</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        	<span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; e.printStackTrace()</span><br><span class="line">        &#125;</span><br><span class="line">        rtn</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//执行 SQL 语句,批量数据插入</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">executeBatchUpdate</span></span>(connection: <span class="type">Connection</span>, sql: <span class="type">String</span>, paramsList: <span class="type">Iterable</span>[<span class="type">Array</span>[<span class="type">Any</span>]]): <span class="type">Array</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">        <span class="keyword">var</span> rtn: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> pstmt: <span class="type">PreparedStatement</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            connection.setAutoCommit(<span class="literal">false</span>)</span><br><span class="line">            pstmt = connection.prepareStatement(sql)</span><br><span class="line">            <span class="keyword">for</span> (params &lt;- paramsList) &#123;</span><br><span class="line">                <span class="keyword">if</span> (params != <span class="literal">null</span> &amp;&amp; params.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="keyword">for</span> (i &lt;- params.indices) &#123;</span><br><span class="line">                    	pstmt.setObject(i + <span class="number">1</span>, params(i))</span><br><span class="line">                    &#125;</span><br><span class="line">                    pstmt.addBatch()</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            rtn = pstmt.executeBatch()</span><br><span class="line">            connection.commit()</span><br><span class="line">            pstmt.close()</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        	<span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; e.printStackTrace()</span><br><span class="line">        &#125;</span><br><span class="line">        rtn</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//判断一条数据是否存在</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isExist</span></span>(connection: <span class="type">Connection</span>, sql: <span class="type">String</span>, params: <span class="type">Array</span>[<span class="type">Any</span>]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">        <span class="keyword">var</span> flag: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line">        <span class="keyword">var</span> pstmt: <span class="type">PreparedStatement</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            pstmt = connection.prepareStatement(sql)</span><br><span class="line">            <span class="keyword">for</span> (i &lt;- params.indices) &#123;</span><br><span class="line">            	pstmt.setObject(i + <span class="number">1</span>, params(i))</span><br><span class="line">            &#125;</span><br><span class="line">            flag = pstmt.executeQuery().next()</span><br><span class="line">            pstmt.close()</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        	<span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; e.printStackTrace()</span><br><span class="line">        &#125;</span><br><span class="line">        flag</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//获取 MySQL 的一条数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getDataFromMysql</span></span>(connection: <span class="type">Connection</span>, sql: <span class="type">String</span>, params: <span class="type">Array</span>[<span class="type">Any</span>]): <span class="type">Long</span> = &#123;</span><br><span class="line">        <span class="keyword">var</span> result: <span class="type">Long</span> = <span class="number">0</span>L</span><br><span class="line">        <span class="keyword">var</span> pstmt: <span class="type">PreparedStatement</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            pstmt = connection.prepareStatement(sql)</span><br><span class="line">            <span class="keyword">for</span> (i &lt;- params.indices) &#123;</span><br><span class="line">            	pstmt.setObject(i + <span class="number">1</span>, params(i))</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">val</span> resultSet: <span class="type">ResultSet</span> = pstmt.executeQuery()</span><br><span class="line">            <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">            	result = resultSet.getLong(<span class="number">1</span>)</span><br><span class="line">            &#125;</span><br><span class="line">            resultSet.close()</span><br><span class="line">            pstmt.close()</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        	<span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; e.printStackTrace()</span><br><span class="line">        &#125;</span><br><span class="line">        result</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//主方法,用于测试上述方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="7-3-4-代码实现"><a href="#7-3-4-代码实现" class="headerlink" title="7.3.4 代码实现"></a>7.3.4 代码实现</h3><ul>
<li><p>Ads_log  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Ads_log</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    timestamp: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    area: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    city: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">	userid: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">	adid: <span class="type">String</span></span></span></span><br><span class="line"><span class="class"><span class="params"></span>)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>BlackListHandler  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.<span class="type">Connection</span></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Date</span></span><br><span class="line"><span class="keyword">import</span> com.atguigu.bean.<span class="type">Ads_log</span></span><br><span class="line"><span class="keyword">import</span> com.atguigu.utils.<span class="type">JdbcUtil</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">DStream</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BlackListHandler</span> </span>&#123;</span><br><span class="line">    <span class="comment">//时间格式化对象</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addBlackList</span></span>(filterAdsLogDSteam: <span class="type">DStream</span>[<span class="type">Ads_log</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//统计当前批次中单日每个用户点击每个广告的总次数</span></span><br><span class="line">        <span class="comment">//1.将数据接转换结构 ads_log=&gt;((date,user,adid),1)</span></span><br><span class="line">        <span class="keyword">val</span> dateUserAdToOne: <span class="type">DStream</span>[((<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>), <span class="type">Long</span>)] =</span><br><span class="line">        filterAdsLogDSteam.map(adsLog =&gt; &#123;</span><br><span class="line">            <span class="comment">//a.将时间戳转换为日期字符串</span></span><br><span class="line">            <span class="keyword">val</span> date: <span class="type">String</span> = sdf.format(<span class="keyword">new</span> <span class="type">Date</span>(adsLog.timestamp))</span><br><span class="line">            <span class="comment">//b.返回值</span></span><br><span class="line">            ((date, adsLog.userid, adsLog.adid), <span class="number">1</span>L)&#125;)</span><br><span class="line">        <span class="comment">//2.统计单日每个用户点击每个广告的总次数 ((date,user,adid),1)=&gt;((date,user,adid),count)</span></span><br><span class="line">        <span class="keyword">val</span> dateUserAdToCount: <span class="type">DStream</span>[((<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>), <span class="type">Long</span>)] = dateUserAdToOne.reduceByKey(_ + _)</span><br><span class="line">        dateUserAdToCount.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">            rdd.foreachPartition(iter =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> connection: <span class="type">Connection</span> = <span class="type">JdbcUtil</span>.getConnection</span><br><span class="line">            iter.foreach &#123; <span class="keyword">case</span> ((dt, user, ad), count) =&gt;</span><br><span class="line">                <span class="type">JdbcUtil</span>.executeUpdate(connection,</span><br><span class="line">                <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">                |INSERT INTO user_ad_count (dt,userid,adid,count)</span></span><br><span class="line"><span class="string">                |VALUES (?,?,?,?)</span></span><br><span class="line"><span class="string">                |ON DUPLICATE KEY</span></span><br><span class="line"><span class="string">                |UPDATE count=count+?</span></span><br><span class="line"><span class="string">                "</span><span class="string">""</span>.stripMargin, <span class="type">Array</span>(dt, user, ad, count, count))</span><br><span class="line">                <span class="keyword">val</span> ct: <span class="type">Long</span> = <span class="type">JdbcUtil</span>.getDataFromMysql(connection, <span class="string">"select count from user_ad_count where dt=? and userid=? and adid =?"</span>, <span class="type">Array</span>(dt, user, ad))</span><br><span class="line">                <span class="keyword">if</span> (ct &gt;= <span class="number">30</span>) &#123;</span><br><span class="line">                    <span class="type">JdbcUtil</span>.executeUpdate(connection, <span class="string">"INSERT INTO black_list (userid) VALUES (?) ON DUPLICATE KEY update userid=?"</span>, <span class="type">Array</span>(user, user))</span><br><span class="line">                &#125;</span><br><span class="line">             &#125;</span><br><span class="line">            connection.close()</span><br><span class="line">            &#125;)</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">filterByBlackList</span></span>(adsLogDStream: <span class="type">DStream</span>[<span class="type">Ads_log</span>]): <span class="type">DStream</span>[<span class="type">Ads_log</span>] = &#123;</span><br><span class="line">        adsLogDStream.transform(rdd =&gt; &#123;</span><br><span class="line">            rdd.filter(adsLog =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> connection: <span class="type">Connection</span> = <span class="type">JdbcUtil</span>.getConnection</span><br><span class="line">                <span class="keyword">val</span> bool: <span class="type">Boolean</span> = <span class="type">JdbcUtil</span>.isExist(connection, <span class="string">"select * from black_list where userid=?"</span>, <span class="type">Array</span>(adsLog.userid))</span><br><span class="line">                connection.close()</span><br><span class="line">                !bool</span><br><span class="line">            &#125;)</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>RealtimeApp</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.atguigu.bean.<span class="type">Ads_log</span></span><br><span class="line"><span class="keyword">import</span> com.atguigu.handler.<span class="type">BlackListHandler</span></span><br><span class="line"><span class="keyword">import</span> com.atguigu.utils.<span class="type">MyKafkaUtil</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerRecord</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RealTimeApp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//1.创建 SparkConf</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"RealTimeApp"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">        <span class="comment">//2.创建 StreamingContext</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">        <span class="comment">//3.读取数据</span></span><br><span class="line">        <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">MyKafkaUtil</span>.getKafkaStream(<span class="string">"ads_log"</span>, ssc)</span><br><span class="line">        <span class="comment">//4.将从 Kafka 读出的数据转换为样例类对象</span></span><br><span class="line">        <span class="keyword">val</span> adsLogDStream: <span class="type">DStream</span>[<span class="type">Ads_log</span>] = kafkaDStream.map(record =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> value: <span class="type">String</span> = record.value()</span><br><span class="line">            <span class="keyword">val</span> arr: <span class="type">Array</span>[<span class="type">String</span>] = value.split(<span class="string">" "</span>)</span><br><span class="line">            <span class="type">Ads_log</span>(arr(<span class="number">0</span>).toLong, arr(<span class="number">1</span>), arr(<span class="number">2</span>), arr(<span class="number">3</span>), arr(<span class="number">4</span>))&#125;)</span><br><span class="line">        <span class="comment">//5.需求一：根据 MySQL 中的黑名单过滤当前数据集</span></span><br><span class="line">        <span class="keyword">val</span> filterAdsLogDStream: <span class="type">DStream</span>[<span class="type">Ads_log</span>] = <span class="type">BlackListHandler2</span>.filterByBlackList(adsLogDStream)</span><br><span class="line">        <span class="comment">//6.需求一：将满足要求的用户写入黑名单</span></span><br><span class="line">        <span class="type">BlackListHandler2</span>.addBlackList(filterAdsLogDStream)</span><br><span class="line">        <span class="comment">//测试打印</span></span><br><span class="line">        filterAdsLogDStream.cache()</span><br><span class="line">        filterAdsLogDStream.count().print()</span><br><span class="line">        <span class="comment">//启动任务</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.spark.streaming</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.&#123;<span class="type">ConsumerConfig</span>, <span class="type">ConsumerRecord</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">InputDStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming11_Req1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 案例实操-需求一</span></span><br><span class="line">    <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">    <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">// 定义 Kafka 参数</span></span><br><span class="line">    <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"bigdata1:6667,bigdata2:6667,bigdata3:6667"</span>,</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"test1212"</span>,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>,</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> kafkaDataDS: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</span><br><span class="line">      <span class="comment">// 配置自动选择采集节点和执行节点的最优选择机制</span></span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">"test1212"</span>), kafkaPara)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    kafkaDataDS.map(_.value()).print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.spark.streaming</span><br><span class="line"><span class="keyword">import</span> com.atguigu.spark.util.<span class="type">JdbcUtil</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.&#123;<span class="type">ConsumerConfig</span>, <span class="type">ConsumerRecord</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">ResultSet</span></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Date</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming11_Req1_BlackList</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 案例实操-需求一</span></span><br><span class="line">    <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">    <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">// 定义 Kafka 参数</span></span><br><span class="line">    <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"bigdata1:6667,bigdata2:6667,bigdata3:6667"</span>,</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"test1212"</span>,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>,</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> kafkaDataDS: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</span><br><span class="line">      <span class="comment">// 配置自动选择采集节点和执行节点的最优选择机制</span></span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">"test1212"</span>), kafkaPara)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> adClickData: <span class="type">DStream</span>[<span class="type">AdClickData</span>] = kafkaDataDS.map(</span><br><span class="line">      kafkaData =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> data = kafkaData.value()</span><br><span class="line">        <span class="keyword">val</span> datas = data.split(<span class="string">" "</span>)</span><br><span class="line">        <span class="type">AdClickData</span>(datas(<span class="number">0</span>), datas(<span class="number">1</span>), datas(<span class="number">2</span>), datas(<span class="number">3</span>), datas(<span class="number">4</span>))</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">DStream</span>[((<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = adClickData.transform(</span><br><span class="line">      rdd =&gt; &#123;</span><br><span class="line">        <span class="comment">// TODO 通过JDBC周期性获取黑名单数据</span></span><br><span class="line">        <span class="keyword">val</span> blackList = <span class="type">ListBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">        <span class="keyword">val</span> conn = <span class="type">JdbcUtil</span>.getConnection</span><br><span class="line">        <span class="keyword">val</span> pstat = conn.prepareStatement(<span class="string">"select userid from black_list"</span>)</span><br><span class="line">        <span class="keyword">val</span> rs: <span class="type">ResultSet</span> = pstat.executeQuery()</span><br><span class="line">        <span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">          blackList.append(rs.getString(<span class="number">1</span>))</span><br><span class="line">        &#125;</span><br><span class="line">        rs.close()</span><br><span class="line">        pstat.close()</span><br><span class="line">        conn.close()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 判断点击用户是否在黑名单中</span></span><br><span class="line">        <span class="keyword">val</span> filterRDD: <span class="type">RDD</span>[<span class="type">AdClickData</span>] = rdd.filter(</span><br><span class="line">          data =&gt; &#123;</span><br><span class="line">            !blackList.contains(data.user)</span><br><span class="line">          &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 如果用户不在黑名单中，那么进行统计数量（每个采集周期）</span></span><br><span class="line">        filterRDD.map(</span><br><span class="line">          data =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd"</span>)</span><br><span class="line">            <span class="keyword">val</span> day = sdf.format(<span class="keyword">new</span> <span class="type">Date</span>(data.ts.toLong))</span><br><span class="line">            <span class="keyword">val</span> user = data.user</span><br><span class="line">            <span class="keyword">val</span> ad = data.ad</span><br><span class="line">            ((day, user, ad), <span class="number">1</span>)</span><br><span class="line">          &#125;</span><br><span class="line">        ).reduceByKey(_ + _)</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    ds.foreachRDD(</span><br><span class="line">      rdd =&gt;&#123;</span><br><span class="line">        rdd.foreach&#123;</span><br><span class="line">          <span class="keyword">case</span> ((day,user,ad),count) =&gt; &#123;</span><br><span class="line">            println(<span class="string">s"<span class="subst">$&#123;day&#125;</span> <span class="subst">$&#123;user&#125;</span> <span class="subst">$&#123;ad&#125;</span> <span class="subst">$&#123;count&#125;</span>"</span>)</span><br><span class="line">            <span class="keyword">if</span>(count &gt;= <span class="number">30</span>)&#123;</span><br><span class="line">              <span class="comment">// TODO 如果统计数量超过点击阈值(30)，那么将用户拉入黑名单</span></span><br><span class="line">              <span class="keyword">val</span> conn = <span class="type">JdbcUtil</span>.getConnection</span><br><span class="line">              <span class="keyword">val</span> pstat = conn.prepareStatement(</span><br><span class="line">                <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">                  |insert into black_list (userid) values (?)</span></span><br><span class="line"><span class="string">                  |on DUPLICATE KEY</span></span><br><span class="line"><span class="string">                  |UPDATE userid = ?</span></span><br><span class="line"><span class="string">                  |"</span><span class="string">""</span>.stripMargin)</span><br><span class="line">              pstat.setString(<span class="number">1</span>, user)</span><br><span class="line">              pstat.setString(<span class="number">2</span>, user)</span><br><span class="line">              pstat.executeUpdate()</span><br><span class="line">              pstat.close()</span><br><span class="line">              conn.close()</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              <span class="comment">// TODO 如果没有超过阈值，那么需要将当天的广告数量进行更新</span></span><br><span class="line">              <span class="keyword">val</span> conn = <span class="type">JdbcUtil</span>.getConnection</span><br><span class="line">              <span class="keyword">val</span> pstat = conn.prepareStatement(</span><br><span class="line">                <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">                  |select *</span></span><br><span class="line"><span class="string">                  |from user_ad_count</span></span><br><span class="line"><span class="string">                  |where dt =? and userid =? and adid =?</span></span><br><span class="line"><span class="string">                  |"</span><span class="string">""</span>.stripMargin)</span><br><span class="line">              pstat.setString(<span class="number">1</span>,day)</span><br><span class="line">              pstat.setString(<span class="number">2</span>,user)</span><br><span class="line">              pstat.setString(<span class="number">3</span>,ad)</span><br><span class="line">              <span class="keyword">val</span> rs = pstat.executeQuery()</span><br><span class="line">              <span class="comment">// 查询统计表数据</span></span><br><span class="line">              <span class="keyword">if</span>(rs.next())&#123;</span><br><span class="line">                <span class="comment">// 如果存在数据，则更新</span></span><br><span class="line">                <span class="keyword">val</span> pstat1 = conn.prepareStatement(</span><br><span class="line">                  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">                    |update user_ad_count</span></span><br><span class="line"><span class="string">                    |set count = count + ?</span></span><br><span class="line"><span class="string">                    |where dt =? and userid =? and adid =?</span></span><br><span class="line"><span class="string">                    |"</span><span class="string">""</span>.stripMargin)</span><br><span class="line">                pstat1.setInt(<span class="number">1</span>,count)</span><br><span class="line">                pstat1.setString(<span class="number">2</span>,day)</span><br><span class="line">                pstat1.setString(<span class="number">3</span>,user)</span><br><span class="line">                pstat1.setString(<span class="number">4</span>,ad)</span><br><span class="line">                pstat1.executeUpdate()</span><br><span class="line">                pstat1.close()</span><br><span class="line">                <span class="comment">// TODO 判断更新后的点击数据是否超过阈值，如果超过，那么将用户拉入黑名单</span></span><br><span class="line">                <span class="keyword">val</span> pstat2 = conn.prepareStatement(</span><br><span class="line">                  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">                    |select * from user_ad_count</span></span><br><span class="line"><span class="string">                    |where dt =? and userid =? and adid =? and count &gt;= 30</span></span><br><span class="line"><span class="string">                    |"</span><span class="string">""</span>.stripMargin)</span><br><span class="line">                pstat2.setString(<span class="number">1</span>, day)</span><br><span class="line">                pstat2.setString(<span class="number">2</span>, user)</span><br><span class="line">                pstat2.setString(<span class="number">3</span>, ad)</span><br><span class="line">                <span class="keyword">val</span> rs2 = pstat2.executeQuery()</span><br><span class="line">                <span class="keyword">if</span>(rs2.next()) &#123;</span><br><span class="line">                  <span class="keyword">val</span> pstat3 = conn.prepareStatement(</span><br><span class="line">                    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">                      |insert into black_list (userid) values (?)</span></span><br><span class="line"><span class="string">                      |on DUPLICATE KEY</span></span><br><span class="line"><span class="string">                      |UPDATE userid = ?</span></span><br><span class="line"><span class="string">                      |"</span><span class="string">""</span>.stripMargin)</span><br><span class="line">                  pstat3.setString(<span class="number">1</span>, user)</span><br><span class="line">                  pstat3.setString(<span class="number">2</span>, user)</span><br><span class="line">                  pstat3.executeUpdate()</span><br><span class="line">                  pstat3.close()</span><br><span class="line">                &#125;</span><br><span class="line">                rs2.close()</span><br><span class="line">                pstat2.close()</span><br><span class="line">              &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 如果不存在数据，则新增</span></span><br><span class="line">                <span class="keyword">val</span> pstat1 = conn.prepareStatement(</span><br><span class="line">                  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">                    |insert into user_ad_count (dt, userid,adid,count) values (?,?,?,?)</span></span><br><span class="line"><span class="string">                    |"</span><span class="string">""</span>.stripMargin)</span><br><span class="line">                pstat1.setString(<span class="number">1</span>,day)</span><br><span class="line">                pstat1.setString(<span class="number">2</span>,user)</span><br><span class="line">                pstat1.setString(<span class="number">3</span>,ad)</span><br><span class="line">                pstat1.setInt(<span class="number">4</span>,count)</span><br><span class="line">                pstat1.executeUpdate()</span><br><span class="line">                pstat1.close()</span><br><span class="line">              &#125;</span><br><span class="line">              rs.close()</span><br><span class="line">              pstat.close()</span><br><span class="line">              conn.close()</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">AdClickData</span>(<span class="params">ts: <span class="type">String</span>, area: <span class="type">String</span>, city: <span class="type">String</span>, user: <span class="type">String</span>, ad: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.spark.streaming</span><br><span class="line"><span class="keyword">import</span> com.atguigu.spark.util.<span class="type">JdbcUtil</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.&#123;<span class="type">ConsumerConfig</span>, <span class="type">ConsumerRecord</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">ResultSet</span></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Date</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming11_Req1_BlackList1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 案例实操-需求一简化</span></span><br><span class="line">    <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">    <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">// 定义 Kafka 参数</span></span><br><span class="line">    <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"bigdata1:6667,bigdata2:6667,bigdata3:6667"</span>,</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"test1212"</span>,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>,</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> kafkaDataDS: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</span><br><span class="line">      <span class="comment">// 配置自动选择采集节点和执行节点的最优选择机制</span></span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">"test1212"</span>), kafkaPara)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> adClickData: <span class="type">DStream</span>[<span class="type">AdClickData</span>] = kafkaDataDS.map(</span><br><span class="line">      kafkaData =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> data = kafkaData.value()</span><br><span class="line">        <span class="keyword">val</span> datas = data.split(<span class="string">" "</span>)</span><br><span class="line">        <span class="type">AdClickData</span>(datas(<span class="number">0</span>), datas(<span class="number">1</span>), datas(<span class="number">2</span>), datas(<span class="number">3</span>), datas(<span class="number">4</span>))</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">DStream</span>[((<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = adClickData.transform(</span><br><span class="line">      rdd =&gt; &#123;</span><br><span class="line">        <span class="comment">// TODO 通过JDBC周期性获取黑名单数据</span></span><br><span class="line">        <span class="keyword">val</span> blackList = <span class="type">ListBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">        <span class="keyword">val</span> conn = <span class="type">JdbcUtil</span>.getConnection</span><br><span class="line">        <span class="keyword">val</span> pstat = conn.prepareStatement(<span class="string">"select userid from black_list"</span>)</span><br><span class="line">        <span class="keyword">val</span> rs: <span class="type">ResultSet</span> = pstat.executeQuery()</span><br><span class="line">        <span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">          blackList.append(rs.getString(<span class="number">1</span>))</span><br><span class="line">        &#125;</span><br><span class="line">        rs.close()</span><br><span class="line">        pstat.close()</span><br><span class="line">        conn.close()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 判断点击用户是否在黑名单中</span></span><br><span class="line">        <span class="keyword">val</span> filterRDD: <span class="type">RDD</span>[<span class="type">AdClickData</span>] = rdd.filter(</span><br><span class="line">          data =&gt; &#123;</span><br><span class="line">            !blackList.contains(data.user)</span><br><span class="line">          &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 如果用户不在黑名单中，那么进行统计数量（每个采集周期）</span></span><br><span class="line">        filterRDD.map(</span><br><span class="line">          data =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd"</span>)</span><br><span class="line">            <span class="keyword">val</span> day = sdf.format(<span class="keyword">new</span> java.util.<span class="type">Date</span>(data.ts.toLong))</span><br><span class="line">            <span class="keyword">val</span> user = data.user</span><br><span class="line">            <span class="keyword">val</span> ad = data.ad</span><br><span class="line">            ((day, user, ad), <span class="number">1</span>)</span><br><span class="line">          &#125;</span><br><span class="line">        ).reduceByKey(_ + _)</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    ds.foreachRDD(</span><br><span class="line">      rdd =&gt;&#123;</span><br><span class="line">        <span class="comment">// rdd.foreach方法会灭一条数据创建连接</span></span><br><span class="line">        <span class="comment">// foreach方法是RDD的算子，算子之外的代码是在Driver端执行，算子内的代码是在Executor端执行</span></span><br><span class="line">        <span class="comment">// 这样就会涉及闭包操作，Driver端的数据就需要传递到Executor端，需要将数据进行序列化</span></span><br><span class="line">        <span class="comment">// 数据库的连接对象是不能序列化的传递的，因此连接对象不能提出来放到外面，只能放到里面</span></span><br><span class="line">        <span class="comment">// RDD 提供了一个算子可以有效提升效率 rdd.foreachPartition(),以一个分区为单位进行数据处理</span></span><br><span class="line">        rdd.foreachPartition(</span><br><span class="line">          iter =&gt; &#123;</span><br><span class="line">            <span class="comment">// 此处就是一个分区创建一个连接，而不是一条数据创建一个连接，可以大幅度减少连接对象的数量，提升效率</span></span><br><span class="line">            <span class="keyword">val</span> conn = <span class="type">JdbcUtil</span>.getConnection</span><br><span class="line">            <span class="comment">// 此处的foreach只是一个集合中的方法，不是算子了，这里也不涉及数据的传递和序列化</span></span><br><span class="line">            iter.foreach&#123;</span><br><span class="line">              <span class="keyword">case</span> ((day, user, ad), count) =&gt; &#123;</span><br><span class="line">                println(<span class="string">s"<span class="subst">$&#123;day&#125;</span> <span class="subst">$&#123;user&#125;</span> <span class="subst">$&#123;ad&#125;</span> <span class="subst">$&#123;count&#125;</span>"</span>)</span><br><span class="line">                <span class="keyword">if</span> (count &gt;= <span class="number">30</span>) &#123;</span><br><span class="line">                  <span class="comment">// TODO 如果统计数量超过点击阈值(30)，那么将用户拉入黑名单</span></span><br><span class="line">                  <span class="keyword">val</span> sql =</span><br><span class="line">                    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">                      |insert into black_list (userid) values (?)</span></span><br><span class="line"><span class="string">                      |on DUPLICATE KEY</span></span><br><span class="line"><span class="string">                      |UPDATE userid = ?</span></span><br><span class="line"><span class="string">                      |"</span><span class="string">""</span>.stripMargin</span><br><span class="line">                  <span class="type">JdbcUtil</span>.executeUpdate(conn, sql, <span class="type">Array</span>(user, user))</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                  <span class="comment">// TODO 如果没有超过阈值，那么需要将当天的广告数量进行更新</span></span><br><span class="line">                  <span class="keyword">val</span> sql =</span><br><span class="line">                    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">                      |select *</span></span><br><span class="line"><span class="string">                      |from user_ad_count</span></span><br><span class="line"><span class="string">                      |where dt =? and userid =? and adid =?</span></span><br><span class="line"><span class="string">                      |"</span><span class="string">""</span>.stripMargin</span><br><span class="line">                  <span class="keyword">val</span> flag = <span class="type">JdbcUtil</span>.isExist(conn, sql, <span class="type">Array</span>(day, user, ad))</span><br><span class="line">                  <span class="comment">// 查询统计表数据</span></span><br><span class="line">                  <span class="keyword">if</span> (flag) &#123;</span><br><span class="line">                    <span class="comment">// 如果存在数据，则更新</span></span><br><span class="line">                    <span class="keyword">val</span> sql =</span><br><span class="line">                      <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">                        |update user_ad_count</span></span><br><span class="line"><span class="string">                        |set count = count + ?</span></span><br><span class="line"><span class="string">                        |where dt =? and userid =? and adid =?</span></span><br><span class="line"><span class="string">                        |"</span><span class="string">""</span>.stripMargin</span><br><span class="line">                    <span class="type">JdbcUtil</span>.executeUpdate(conn, sql, <span class="type">Array</span>(count, day, user, ad))</span><br><span class="line">                    <span class="comment">// TODO 判断更新后的点击数据是否超过阈值，如果超过，那么将用户拉入黑名单</span></span><br><span class="line">                    <span class="keyword">val</span> sql1 =</span><br><span class="line">                      <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">                        |select * from user_ad_count</span></span><br><span class="line"><span class="string">                        |where dt =? and userid =? and adid =? and count &gt;= 30</span></span><br><span class="line"><span class="string">                        |"</span><span class="string">""</span>.stripMargin</span><br><span class="line">                    <span class="keyword">val</span> flag = <span class="type">JdbcUtil</span>.isExist(conn, sql1, <span class="type">Array</span>(day, user, ad))</span><br><span class="line">                    <span class="keyword">if</span> (flag) &#123;</span><br><span class="line">                      <span class="keyword">val</span> sql2 =</span><br><span class="line">                        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">                          |insert into black_list (userid) values (?)</span></span><br><span class="line"><span class="string">                          |on DUPLICATE KEY</span></span><br><span class="line"><span class="string">                          |UPDATE userid = ?</span></span><br><span class="line"><span class="string">                          |"</span><span class="string">""</span>.stripMargin</span><br><span class="line">                      <span class="type">JdbcUtil</span>.executeUpdate(conn, sql2, <span class="type">Array</span>(user, user))</span><br><span class="line">                    &#125;</span><br><span class="line">                  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// 如果不存在数据，则新增</span></span><br><span class="line">                    <span class="keyword">val</span> sql =</span><br><span class="line">                      <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">                        |insert into user_ad_count (dt, userid, adid, count) values (?,?,?,?)</span></span><br><span class="line"><span class="string">                        |"</span><span class="string">""</span>.stripMargin</span><br><span class="line">                    <span class="type">JdbcUtil</span>.executeUpdate(conn, sql, <span class="type">Array</span>(day, user, ad, count))</span><br><span class="line">                  &#125;</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            conn.close()</span><br><span class="line">          &#125;</span><br><span class="line">        )</span><br><span class="line"><span class="comment">//        rdd.foreach&#123;</span></span><br><span class="line"><span class="comment">//          case ((day,user,ad),count) =&gt; &#123;</span></span><br><span class="line"><span class="comment">//            println(s"$&#123;day&#125; $&#123;user&#125; $&#123;ad&#125; $&#123;count&#125;")</span></span><br><span class="line"><span class="comment">//            if(count &gt;= 30)&#123;</span></span><br><span class="line"><span class="comment">//              // TODO 如果统计数量超过点击阈值(30)，那么将用户拉入黑名单</span></span><br><span class="line"><span class="comment">//              val conn = JdbcUtil.getConnection</span></span><br><span class="line"><span class="comment">//              val sql =</span></span><br><span class="line"><span class="comment">//                """</span></span><br><span class="line"><span class="comment">//                  |insert into black_list (userid) values (?)</span></span><br><span class="line"><span class="comment">//                  |on DUPLICATE KEY</span></span><br><span class="line"><span class="comment">//                  |UPDATE userid = ?</span></span><br><span class="line"><span class="comment">//                  |""".stripMargin</span></span><br><span class="line"><span class="comment">//              JdbcUtil.executeUpdate(conn,sql,Array(user, user))</span></span><br><span class="line"><span class="comment">//              conn.close()</span></span><br><span class="line"><span class="comment">//            &#125; else &#123;</span></span><br><span class="line"><span class="comment">//              // TODO 如果没有超过阈值，那么需要将当天的广告数量进行更新</span></span><br><span class="line"><span class="comment">//              val conn = JdbcUtil.getConnection</span></span><br><span class="line"><span class="comment">//              val sql =</span></span><br><span class="line"><span class="comment">//                """</span></span><br><span class="line"><span class="comment">//                  |select *</span></span><br><span class="line"><span class="comment">//                  |from user_ad_count</span></span><br><span class="line"><span class="comment">//                  |where dt =? and userid =? and adid =?</span></span><br><span class="line"><span class="comment">//                  |""".stripMargin</span></span><br><span class="line"><span class="comment">//              val flag = JdbcUtil.isExist(conn,sql, Array(day,user,ad))</span></span><br><span class="line"><span class="comment">//              // 查询统计表数据</span></span><br><span class="line"><span class="comment">//              if(flag)&#123;</span></span><br><span class="line"><span class="comment">//                // 如果存在数据，则更新</span></span><br><span class="line"><span class="comment">//                val sql =</span></span><br><span class="line"><span class="comment">//                  """</span></span><br><span class="line"><span class="comment">//                    |update user_ad_count</span></span><br><span class="line"><span class="comment">//                    |set count = count + ?</span></span><br><span class="line"><span class="comment">//                    |where dt =? and userid =? and adid =?</span></span><br><span class="line"><span class="comment">//                    |""".stripMargin</span></span><br><span class="line"><span class="comment">//                JdbcUtil.executeUpdate(conn, sql, Array(count,day,user,ad))</span></span><br><span class="line"><span class="comment">//                // TODO 判断更新后的点击数据是否超过阈值，如果超过，那么将用户拉入黑名单</span></span><br><span class="line"><span class="comment">//                val sql1 =</span></span><br><span class="line"><span class="comment">//                  """</span></span><br><span class="line"><span class="comment">//                    |select * from user_ad_count</span></span><br><span class="line"><span class="comment">//                    |where dt =? and userid =? and adid =? and count &gt;= 30</span></span><br><span class="line"><span class="comment">//                    |""".stripMargin</span></span><br><span class="line"><span class="comment">//                val flag = JdbcUtil.isExist(conn, sql1, Array(day, user, ad))</span></span><br><span class="line"><span class="comment">//                if(flag) &#123;</span></span><br><span class="line"><span class="comment">//                  val sql2 =</span></span><br><span class="line"><span class="comment">//                    """</span></span><br><span class="line"><span class="comment">//                      |insert into black_list (userid) values (?)</span></span><br><span class="line"><span class="comment">//                      |on DUPLICATE KEY</span></span><br><span class="line"><span class="comment">//                      |UPDATE userid = ?</span></span><br><span class="line"><span class="comment">//                      |""".stripMargin</span></span><br><span class="line"><span class="comment">//                  JdbcUtil.executeUpdate(conn,sql2,Array(user, user))</span></span><br><span class="line"><span class="comment">//                &#125;</span></span><br><span class="line"><span class="comment">//              &#125; else &#123;</span></span><br><span class="line"><span class="comment">//                // 如果不存在数据，则新增</span></span><br><span class="line"><span class="comment">//                val sql =</span></span><br><span class="line"><span class="comment">//                  """</span></span><br><span class="line"><span class="comment">//                    |insert into user_ad_count (dt, userid, adid, count) values (?,?,?,?)</span></span><br><span class="line"><span class="comment">//                    |""".stripMargin</span></span><br><span class="line"><span class="comment">//                JdbcUtil.executeUpdate(conn, sql ,Array(day,user,ad,count))</span></span><br><span class="line"><span class="comment">//              &#125;</span></span><br><span class="line"><span class="comment">//              conn.close()</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//          &#125;</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">AdClickData</span>(<span class="params">ts: <span class="type">String</span>, area: <span class="type">String</span>, city: <span class="type">String</span>, user: <span class="type">String</span>, ad: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>

<h2 id="7-4-需求二：广告点击量实时统计"><a href="#7-4-需求二：广告点击量实时统计" class="headerlink" title="7.4 需求二：广告点击量实时统计"></a>7.4 需求二：广告点击量实时统计</h2><p>描述： 实时统计每天各地区各城市各广告的点击总流量，并将其存入 MySQL。  </p>
<h3 id="7-4-1-思路分析"><a href="#7-4-1-思路分析" class="headerlink" title="7.4.1 思路分析"></a>7.4.1 思路分析</h3><p>1）单个批次内对数据进行按照天维度的聚合统计;<br>2）结合 MySQL 数据跟当前批次数据更新原有的数据。  </p>
<h3 id="7-4-2-MySQL-建表"><a href="#7-4-2-MySQL-建表" class="headerlink" title="7.4.2 MySQL 建表"></a>7.4.2 MySQL 建表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> area_city_ad_count (</span><br><span class="line">    dt <span class="built_in">VARCHAR</span>(<span class="number">255</span>),</span><br><span class="line">    area <span class="built_in">VARCHAR</span>(<span class="number">255</span>),</span><br><span class="line">    city <span class="built_in">VARCHAR</span>(<span class="number">255</span>),</span><br><span class="line">    adid <span class="built_in">VARCHAR</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="keyword">count</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">    PRIMARY <span class="keyword">KEY</span> (dt,area,city,adid)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h3 id="7-4-3-代码实现"><a href="#7-4-3-代码实现" class="headerlink" title="7.4.3 代码实现"></a>7.4.3 代码实现</h3><ul>
<li><p>DateAreaCityAdCountHandler  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.<span class="type">Connection</span></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Date</span></span><br><span class="line"><span class="keyword">import</span> com.atguigu.bean.<span class="type">Ads_log</span></span><br><span class="line"><span class="keyword">import</span> com.atguigu.utils.<span class="type">JdbcUtil</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">DStream</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DateAreaCityAdCountHandler</span> </span>&#123;</span><br><span class="line">    <span class="comment">//时间格式化对象</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> sdf: <span class="type">SimpleDateFormat</span> = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd"</span>)</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 统计每天各大区各个城市广告点击总数并保存至 MySQL 中</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param filterAdsLogDStream 根据黑名单过滤后的数据集</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">saveDateAreaCityAdCountToMysql</span></span>(filterAdsLogDStream: <span class="type">DStream</span>[<span class="type">Ads_log</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//1.统计每天各大区各个城市广告点击总数</span></span><br><span class="line">        <span class="keyword">val</span> dateAreaCityAdToCount: <span class="type">DStream</span>[((<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>), <span class="type">Long</span>)] =</span><br><span class="line">        filterAdsLogDStream.map(ads_log =&gt; &#123;</span><br><span class="line">            <span class="comment">//a.取出时间戳</span></span><br><span class="line">            <span class="keyword">val</span> timestamp: <span class="type">Long</span> = ads_log.timestamp</span><br><span class="line">            <span class="comment">//b.格式化为日期字符串</span></span><br><span class="line">            <span class="keyword">val</span> dt: <span class="type">String</span> = sdf.format(<span class="keyword">new</span> <span class="type">Date</span>(timestamp))</span><br><span class="line">            <span class="comment">//c.组合,返回</span></span><br><span class="line">            ((dt, ads_log.area, ads_log.city, ads_log.adid), <span class="number">1</span>L)</span><br><span class="line">            &#125;).reduceByKey(_ + _)</span><br><span class="line">        <span class="comment">//2.将单个批次统计之后的数据集合 MySQL 数据对原有的数据更新</span></span><br><span class="line">        dateAreaCityAdToCount.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">            <span class="comment">//对每个分区单独处理</span></span><br><span class="line">            rdd.foreachPartition(iter =&gt; &#123;</span><br><span class="line">                <span class="comment">//a.获取连接</span></span><br><span class="line">                <span class="keyword">val</span> connection: <span class="type">Connection</span> = <span class="type">JdbcUtil</span>.getConnection</span><br><span class="line">                <span class="comment">//b.写库</span></span><br><span class="line">                iter.foreach &#123; <span class="keyword">case</span> ((dt, area, city, adid), count) =&gt;</span><br><span class="line">                    <span class="type">JdbcUtil</span>.executeUpdate(connection,</span><br><span class="line">                    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">                    |INSERT INTO area_city_ad_count (dt,area,city,adid,count)</span></span><br><span class="line"><span class="string">                    |VALUES(?,?,?,?,?)</span></span><br><span class="line"><span class="string">                    |ON DUPLICATE KEY</span></span><br><span class="line"><span class="string">                    |UPDATE count=count+?;</span></span><br><span class="line"><span class="string">                    "</span><span class="string">""</span>.stripMargin,</span><br><span class="line">                    <span class="type">Array</span>(dt, area, city, adid, count, count))</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//c.释放连接</span></span><br><span class="line">                connection.close()</span><br><span class="line">            &#125;)</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>RealTimeApp</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.<span class="type">Connection</span></span><br><span class="line"><span class="keyword">import</span> com.atguigu.bean.<span class="type">Ads_log</span></span><br><span class="line"><span class="keyword">import</span> com.atguigu.handler.&#123;<span class="type">BlackListHandler</span>, <span class="type">DateAreaCityAdCountHandler</span>,</span><br><span class="line"><span class="type">LastHourAdCountHandler</span>&#125;</span><br><span class="line"><span class="keyword">import</span> com.atguigu.utils.&#123;<span class="type">JdbcUtil</span>, <span class="type">MyKafkaUtil</span>, <span class="type">PropertiesUtil</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerRecord</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RealTimeApp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//1.创建 SparkConf</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"RealTimeApp"</span>)</span><br><span class="line">        <span class="comment">//2.创建 StreamingContext</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">        <span class="comment">//3.读取 Kafka 数据 1583288137305 华南 深圳 4 3</span></span><br><span class="line">        <span class="keyword">val</span> topic: <span class="type">String</span> = <span class="type">PropertiesUtil</span>.load(<span class="string">"config.properties"</span>).getProperty(<span class="string">"kafka.topic"</span>)</span><br><span class="line">        <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">MyKafkaUtil</span>.getKafkaStream(topic, ssc)</span><br><span class="line">        <span class="comment">//4.将每一行数据转换为样例类对象</span></span><br><span class="line">        <span class="keyword">val</span> adsLogDStream: <span class="type">DStream</span>[<span class="type">Ads_log</span>] = kafkaDStream.map(record =&gt; &#123;</span><br><span class="line">            <span class="comment">//a.取出 value 并按照" "切分</span></span><br><span class="line">            <span class="keyword">val</span> arr: <span class="type">Array</span>[<span class="type">String</span>] = record.value().split(<span class="string">" "</span>)</span><br><span class="line">            <span class="comment">//b.封装为样例类对象</span></span><br><span class="line">            <span class="type">Ads_log</span>(arr(<span class="number">0</span>).toLong, arr(<span class="number">1</span>), arr(<span class="number">2</span>), arr(<span class="number">3</span>), arr(<span class="number">4</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//5.根据 MySQL 中的黑名单表进行数据过滤</span></span><br><span class="line">        <span class="keyword">val</span> filterAdsLogDStream: <span class="type">DStream</span>[<span class="type">Ads_log</span>] = adsLogDStream.filter(adsLog =&gt; &#123;</span><br><span class="line">            <span class="comment">//查询 MySQL,查看当前用户是否存在。</span></span><br><span class="line">            <span class="keyword">val</span> connection: <span class="type">Connection</span> = <span class="type">JdbcUtil</span>.getConnection</span><br><span class="line">            <span class="keyword">val</span> bool: <span class="type">Boolean</span> = <span class="type">JdbcUtil</span>.isExist(connection, <span class="string">"select * from black_list where userid=?"</span>, <span class="type">Array</span>(adsLog.userid))</span><br><span class="line">            connection.close()</span><br><span class="line">            !bool</span><br><span class="line">        &#125;)</span><br><span class="line">        filterAdsLogDStream.cache()</span><br><span class="line">        <span class="comment">//6.对没有被加入黑名单的用户统计当前批次单日各个用户对各个广告点击的总次数,</span></span><br><span class="line">        <span class="comment">// 并更新至 MySQL</span></span><br><span class="line">        <span class="comment">// 之后查询更新之后的数据,判断是否超过 100 次。</span></span><br><span class="line">        <span class="comment">// 如果超过则将给用户加入黑名单</span></span><br><span class="line">        <span class="type">BlackListHandler</span>.saveBlackListToMysql(filterAdsLogDStream)</span><br><span class="line">        <span class="comment">//7.统计每天各大区各个城市广告点击总数并保存至 MySQL 中</span></span><br><span class="line">        dateAreaCityAdCountHandler.saveDateAreaCityAdCountToMysql(filterAdsLogDStream)</span><br><span class="line">        <span class="comment">//10.开启任务</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.spark.streaming</span><br><span class="line"><span class="keyword">import</span> com.atguigu.spark.util.<span class="type">JdbcUtil</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.&#123;<span class="type">ConsumerConfig</span>, <span class="type">ConsumerRecord</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Date</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming12_Req2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 案例实操-需求二</span></span><br><span class="line">    <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">    <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    <span class="comment">// 定义 Kafka 参数</span></span><br><span class="line">    <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"bigdata1:6667,bigdata2:6667,bigdata3:6667"</span>,</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"test1212"</span>,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>,</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> kafkaDataDS: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</span><br><span class="line">      <span class="comment">// 配置自动选择采集节点和执行节点的最优选择机制</span></span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">"test1212"</span>), kafkaPara)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> adClickData: <span class="type">DStream</span>[<span class="type">AdClickData</span>] = kafkaDataDS.map(</span><br><span class="line">      kafkaData =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> data = kafkaData.value()</span><br><span class="line">        <span class="keyword">val</span> datas = data.split(<span class="string">" "</span>)</span><br><span class="line">        <span class="type">AdClickData</span>(datas(<span class="number">0</span>), datas(<span class="number">1</span>), datas(<span class="number">2</span>), datas(<span class="number">3</span>), datas(<span class="number">4</span>))</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> reduceDS = adClickData.map(</span><br><span class="line">      data =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd"</span>)</span><br><span class="line">        <span class="keyword">val</span> day = sdf.format(<span class="keyword">new</span> <span class="type">Date</span>(data.ts.toLong))</span><br><span class="line">        <span class="keyword">val</span> area = data.area</span><br><span class="line">        <span class="keyword">val</span> city = data.city</span><br><span class="line">        <span class="keyword">val</span> ad = data.ad</span><br><span class="line">        ((day, area, city, ad), <span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    reduceDS.foreachRDD(</span><br><span class="line">      rdd =&gt; &#123;</span><br><span class="line">        rdd.foreachPartition(</span><br><span class="line">          iter =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> conn = <span class="type">JdbcUtil</span>.getConnection</span><br><span class="line">            <span class="keyword">val</span> sql =</span><br><span class="line">              <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">                |insert into area_city_ad_count(dt, area,city,adid,count)</span></span><br><span class="line"><span class="string">                |values(?,?,?,?,?)</span></span><br><span class="line"><span class="string">                |on DUPLICATE KEY</span></span><br><span class="line"><span class="string">                |UPDATE count = count + ?</span></span><br><span class="line"><span class="string">                |"</span><span class="string">""</span>.stripMargin</span><br><span class="line">            iter.foreach&#123;</span><br><span class="line">              <span class="keyword">case</span> ((day, area, city, ad), sum) =&gt; &#123;</span><br><span class="line">                <span class="type">JdbcUtil</span>.executeUpdate(conn,sql,<span class="type">Array</span>(day,area,city,ad,sum,sum))</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            conn.close()</span><br><span class="line">          &#125;</span><br><span class="line">        )</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">AdClickData</span>(<span class="params">ts: <span class="type">String</span>, area: <span class="type">String</span>, city: <span class="type">String</span>, user: <span class="type">String</span>, ad: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>

<h2 id="7-5-需求三：-最近一小时广告点击量"><a href="#7-5-需求三：-最近一小时广告点击量" class="headerlink" title="7.5 需求三： 最近一小时广告点击量"></a>7.5 需求三： 最近一小时广告点击量</h2><p>结果展示：  </p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>： List [<span class="number">15</span>:<span class="number">50</span>-&gt;<span class="number">10</span>,<span class="number">15</span>:<span class="number">51</span>-&gt;<span class="number">25</span>,<span class="number">15</span>:<span class="number">52</span>-&gt;<span class="number">30</span>]</span><br><span class="line"><span class="number">2</span>： List [<span class="number">15</span>:<span class="number">50</span>-&gt;<span class="number">10</span>,<span class="number">15</span>:<span class="number">51</span>-&gt;<span class="number">25</span>,<span class="number">15</span>:<span class="number">52</span>-&gt;<span class="number">30</span>]</span><br><span class="line"><span class="number">3</span>： List [<span class="number">15</span>:<span class="number">50</span>-&gt;<span class="number">10</span>,<span class="number">15</span>:<span class="number">51</span>-&gt;<span class="number">25</span>,<span class="number">15</span>:<span class="number">52</span>-&gt;<span class="number">30</span>]</span><br></pre></td></tr></table></figure>

<h3 id="7-5-1-思路分析"><a href="#7-5-1-思路分析" class="headerlink" title="7.5.1 思路分析"></a>7.5.1 思路分析</h3><p>1）开窗确定时间范围；<br>2）在窗口内将数据转换数据结构为((adid,hm),count);<br>3）按照广告 id 进行分组处理，组内按照时分排序</p>
<p><img src="/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/image-20240725181522164.png" alt="image-20240725181522164"></p>
<h3 id="7-5-2-代码实现"><a href="#7-5-2-代码实现" class="headerlink" title="7.5.2 代码实现"></a>7.5.2 代码实现</h3><ul>
<li><p>LastHourAdCountHandler</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Date</span></span><br><span class="line"><span class="keyword">import</span> com.atguigu.bean.<span class="type">Ads_log</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.<span class="type">Minutes</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">DStream</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LastHourAdCountHandler</span> </span>&#123;</span><br><span class="line">    <span class="comment">//时间格式化对象</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> sdf: <span class="type">SimpleDateFormat</span> = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"HH:mm"</span>)</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 统计最近一小时(2 分钟)广告分时点击总数</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param filterAdsLogDStream 过滤后的数据集</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getAdHourMintToCount</span></span>(filterAdsLogDStream: <span class="type">DStream</span>[<span class="type">Ads_log</span>]): <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Long</span>)])] = &#123;</span><br><span class="line">        <span class="comment">//1.开窗 =&gt; 时间间隔为 1 个小时 window()</span></span><br><span class="line">        <span class="keyword">val</span> windowAdsLogDStream: <span class="type">DStream</span>[<span class="type">Ads_log</span>] = filterAdsLogDStream.window(<span class="type">Minutes</span>(<span class="number">2</span>))</span><br><span class="line">        <span class="comment">//2.转换数据结构 ads_log =&gt;((adid,hm),1L) map()</span></span><br><span class="line">        <span class="keyword">val</span> adHmToOneDStream: <span class="type">DStream</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Long</span>)] = windowAdsLogDStream.map(adsLog =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> timestamp: <span class="type">Long</span> = adsLog.timestamp</span><br><span class="line">            <span class="keyword">val</span> hm: <span class="type">String</span> = sdf.format(<span class="keyword">new</span> <span class="type">Date</span>(timestamp))</span><br><span class="line">            ((adsLog.adid, hm), <span class="number">1</span>L)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//3.统计总数 ((adid,hm),1L)=&gt;((adid,hm),sum) reduceBykey(_+_)</span></span><br><span class="line">        <span class="keyword">val</span> adHmToCountDStream: <span class="type">DStream</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Long</span>)] = adHmToOneDStream.reduceByKey(_ + _)</span><br><span class="line">        <span class="comment">//4.转换数据结构 ((adid,hm),sum)=&gt;(adid,(hm,sum)) map()</span></span><br><span class="line">        <span class="keyword">val</span> adToHmCountDStream: <span class="type">DStream</span>[(<span class="type">String</span>, (<span class="type">String</span>, <span class="type">Long</span>))] = adHmToCountDStream.map &#123; <span class="keyword">case</span> ((adid, hm), count) =&gt;</span><br><span class="line">        	(adid, (hm, count))</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//5.按照 adid 分组 (adid,(hm,sum))=&gt;(adid,Iter[(hm,sum),...]) groupByKey</span></span><br><span class="line">        adToHmCountDStream.groupByKey().mapValues(iter =&gt;</span><br><span class="line">        	iter.toList.sortWith(_._1 &lt; _._1)</span><br><span class="line">        )</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>RealTimeApp</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.<span class="type">Connection</span></span><br><span class="line"><span class="keyword">import</span> com.atguigu.bean.<span class="type">Ads_log</span></span><br><span class="line"><span class="keyword">import</span> com.atguigu.handler.&#123;<span class="type">BlackListHandler</span>, <span class="type">DateAreaCityAdCountHandler</span>,</span><br><span class="line"><span class="type">LastHourAdCountHandler</span>&#125;</span><br><span class="line"><span class="keyword">import</span> com.atguigu.utils.&#123;<span class="type">JdbcUtil</span>, <span class="type">MyKafkaUtil</span>, <span class="type">PropertiesUtil</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerRecord</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RealTimeApp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//1.创建 SparkConf</span></span><br><span class="line">        <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"RealTimeApp"</span>)</span><br><span class="line">        <span class="comment">//2.创建 StreamingContext</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">        <span class="comment">//3.读取 Kafka 数据 1583288137305 华南 深圳 4 3</span></span><br><span class="line">        <span class="keyword">val</span> topic: <span class="type">String</span> = <span class="type">PropertiesUtil</span>.load(<span class="string">"config.properties"</span>).getProperty(<span class="string">"kafka.topic"</span>)</span><br><span class="line">        <span class="keyword">val</span> kafkaDStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">MyKafkaUtil</span>.getKafkaStream(topic, ssc)</span><br><span class="line">        <span class="comment">//4.将每一行数据转换为样例类对象</span></span><br><span class="line">        <span class="keyword">val</span> adsLogDStream: <span class="type">DStream</span>[<span class="type">Ads_log</span>] = kafkaDStream.map(record =&gt; &#123;</span><br><span class="line">            <span class="comment">//a.取出 value 并按照" "切分</span></span><br><span class="line">            <span class="keyword">val</span> arr: <span class="type">Array</span>[<span class="type">String</span>] = record.value().split(<span class="string">" "</span>)</span><br><span class="line">            <span class="comment">//b.封装为样例类对象</span></span><br><span class="line">            <span class="type">Ads_log</span>(arr(<span class="number">0</span>).toLong, arr(<span class="number">1</span>), arr(<span class="number">2</span>), arr(<span class="number">3</span>), arr(<span class="number">4</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//5.根据 MySQL 中的黑名单表进行数据过滤</span></span><br><span class="line">        <span class="keyword">val</span> filterAdsLogDStream: <span class="type">DStream</span>[<span class="type">Ads_log</span>] = adsLogDStream.filter(adsLog =&gt; &#123;</span><br><span class="line">            <span class="comment">//查询 MySQL,查看当前用户是否存在。</span></span><br><span class="line">            <span class="keyword">val</span> connection: <span class="type">Connection</span> = <span class="type">JdbcUtil</span>.getConnection</span><br><span class="line">            <span class="keyword">val</span> bool: <span class="type">Boolean</span> = <span class="type">JdbcUtil</span>.isExist(connection, <span class="string">"select * from black_list</span></span><br><span class="line"><span class="string">            where userid=?"</span>, <span class="type">Array</span>(adsLog.userid))</span><br><span class="line">            connection.close()</span><br><span class="line">            !bool</span><br><span class="line">        &#125;)</span><br><span class="line">        filterAdsLogDStream.cache()</span><br><span class="line">        <span class="comment">//6.对没有被加入黑名单的用户统计当前批次单日各个用户对各个广告点击的总次数,</span></span><br><span class="line">        <span class="comment">// 并更新至 MySQL</span></span><br><span class="line">        <span class="comment">// 之后查询更新之后的数据,判断是否超过 100 次。</span></span><br><span class="line">        <span class="comment">// 如果超过则将给用户加入黑名单</span></span><br><span class="line">        <span class="type">BlackListHandler</span>.saveBlackListToMysql(filterAdsLogDStream)</span><br><span class="line">        <span class="comment">//7.统计每天各大区各个城市广告点击总数并保存至 MySQL 中</span></span><br><span class="line">        <span class="type">DateAreaCityAdCountHandler</span>.saveDateAreaCityAdCountToMysql(filterAdsLogDStream)</span><br><span class="line">        <span class="comment">//8.统计最近一小时(2 分钟)广告分时点击总数</span></span><br><span class="line">        <span class="keyword">val</span> adToHmCountListDStream: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Long</span>)])] =</span><br><span class="line">        <span class="type">LastHourAdCountHandler</span>.getAdHourMintToCount(filterAdsLogDStream)</span><br><span class="line">        <span class="comment">//9.打印</span></span><br><span class="line">        adToHmCountListDStream.print()</span><br><span class="line">        <span class="comment">//10.开启任务</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>练习与测试</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.spark.streaming</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.&#123;<span class="type">ConsumerConfig</span>, <span class="type">ConsumerRecord</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming13_Req3</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 案例实操-需求三</span></span><br><span class="line">    <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">    <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="comment">// 定义 Kafka 参数</span></span><br><span class="line">    <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"bigdata1:6667,bigdata2:6667,bigdata3:6667"</span>,</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"test1212"</span>,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>,</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> kafkaDataDS: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</span><br><span class="line">      <span class="comment">// 配置自动选择采集节点和执行节点的最优选择机制</span></span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">"test1212"</span>), kafkaPara)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> adClickData: <span class="type">DStream</span>[<span class="type">AdClickData</span>] = kafkaDataDS.map(</span><br><span class="line">      kafkaData =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> data = kafkaData.value()</span><br><span class="line">        <span class="keyword">val</span> datas = data.split(<span class="string">" "</span>)</span><br><span class="line">        <span class="type">AdClickData</span>(datas(<span class="number">0</span>), datas(<span class="number">1</span>), datas(<span class="number">2</span>), datas(<span class="number">3</span>), datas(<span class="number">4</span>))</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 最新一分钟，每10秒计算一次</span></span><br><span class="line">    <span class="comment">// 00:01 =&gt; 00:00</span></span><br><span class="line">    <span class="comment">// 00:11 =&gt; 00:10</span></span><br><span class="line">    <span class="comment">// 00:19 =&gt; 00:10</span></span><br><span class="line">    <span class="comment">// 00:25 =&gt; 00:20</span></span><br><span class="line">    <span class="comment">// 00:59 =&gt; 00:50</span></span><br><span class="line">    <span class="comment">// 55 /10 * 10 =&gt; 50</span></span><br><span class="line">    <span class="comment">// 这里涉及窗口的计算</span></span><br><span class="line">    <span class="keyword">val</span> reduceDS = adClickData.map(</span><br><span class="line">      data =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> ts = data.ts.toLong</span><br><span class="line">        <span class="keyword">val</span> newTS = ts / <span class="number">10000</span> * <span class="number">10000</span></span><br><span class="line">        (newTS, <span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    ).reduceByKeyAndWindow((x:<span class="type">Int</span>,y:<span class="type">Int</span>) =&gt; &#123;x + y&#125;, <span class="type">Seconds</span>(<span class="number">60</span>), <span class="type">Seconds</span>(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    reduceDS.print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">AdClickData</span>(<span class="params">ts: <span class="type">String</span>, area: <span class="type">String</span>, city: <span class="type">String</span>, user: <span class="type">String</span>, ad: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.spark.streaming</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.&#123;<span class="type">ConsumerConfig</span>, <span class="type">ConsumerRecord</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> java.io.&#123;<span class="type">File</span>, <span class="type">FileWriter</span>, <span class="type">PrintWriter</span>&#125;</span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming13_Req31</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 案例实操-需求三可视化</span></span><br><span class="line">    <span class="comment">// StreamingContext创建时，需要传递两个参数</span></span><br><span class="line">    <span class="comment">// 第一个参数表示环境配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SparkStreaming"</span>)</span><br><span class="line">    <span class="comment">// 第二个参数表示批量处理的周期（采集周期）</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="comment">// 定义 Kafka 参数</span></span><br><span class="line">    <span class="keyword">val</span> kafkaPara: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; <span class="string">"bigdata1:6667,bigdata2:6667,bigdata3:6667"</span>,</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; <span class="string">"test1212"</span>,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>,</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> kafkaDataDS: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</span><br><span class="line">      <span class="comment">// 配置自动选择采集节点和执行节点的最优选择机制</span></span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">Set</span>(<span class="string">"test1212"</span>), kafkaPara)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> adClickData: <span class="type">DStream</span>[<span class="type">AdClickData</span>] = kafkaDataDS.map(</span><br><span class="line">      kafkaData =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> data = kafkaData.value()</span><br><span class="line">        <span class="keyword">val</span> datas = data.split(<span class="string">" "</span>)</span><br><span class="line">        <span class="type">AdClickData</span>(datas(<span class="number">0</span>), datas(<span class="number">1</span>), datas(<span class="number">2</span>), datas(<span class="number">3</span>), datas(<span class="number">4</span>))</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 最新一分钟，每10秒计算一次</span></span><br><span class="line">    <span class="comment">// 00:01 =&gt; 00:00</span></span><br><span class="line">    <span class="comment">// 00:11 =&gt; 00:10</span></span><br><span class="line">    <span class="comment">// 00:19 =&gt; 00:10</span></span><br><span class="line">    <span class="comment">// 00:25 =&gt; 00:20</span></span><br><span class="line">    <span class="comment">// 00:59 =&gt; 00:50</span></span><br><span class="line">    <span class="comment">// 55 /10 * 10 =&gt; 50</span></span><br><span class="line">    <span class="comment">// 这里涉及窗口的计算</span></span><br><span class="line">    <span class="keyword">val</span> reduceDS = adClickData.map(</span><br><span class="line">      data =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> ts = data.ts.toLong</span><br><span class="line">        <span class="keyword">val</span> newTS = ts / <span class="number">10000</span> * <span class="number">10000</span></span><br><span class="line">        (newTS, <span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    ).reduceByKeyAndWindow((x:<span class="type">Int</span>,y:<span class="type">Int</span>) =&gt; &#123;x + y&#125;, <span class="type">Seconds</span>(<span class="number">60</span>), <span class="type">Seconds</span>(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    reduceDS.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> list = <span class="type">ListBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">      <span class="keyword">val</span> datas: <span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Int</span>)] = rdd.sortByKey(<span class="literal">true</span>).collect()</span><br><span class="line">      datas.foreach&#123;</span><br><span class="line">        <span class="keyword">case</span> (time, count) =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> timeString = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"mm:ss"</span>).format(<span class="keyword">new</span> java.util.<span class="type">Date</span>(time.toLong))</span><br><span class="line">          list.append(</span><br><span class="line">            <span class="string">s""</span><span class="string">"</span></span><br><span class="line"><span class="string">              |&#123; "</span><span class="string">xtime":"</span>$&#123;timeString&#125;<span class="string">", "</span><span class="string">yval":"</span>$&#123;count&#125;<span class="string">" &#125;</span></span><br><span class="line"><span class="string">              |"</span><span class="string">""</span>.stripMargin)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 输出文件</span></span><br><span class="line">      <span class="keyword">val</span> out = <span class="keyword">new</span> <span class="type">PrintWriter</span>(<span class="keyword">new</span> <span class="type">FileWriter</span>(<span class="keyword">new</span> <span class="type">File</span>(<span class="string">"D:\\learn\\spark_test\\datas\\adclick\\adclick.json"</span>)))</span><br><span class="line">      out.println(<span class="string">"["</span>+list.mkString(<span class="string">","</span>)+<span class="string">"]"</span>)</span><br><span class="line">      out.flush()</span><br><span class="line">      out.close()</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">AdClickData</span>(<span class="params">ts: <span class="type">String</span>, area: <span class="type">String</span>, city: <span class="type">String</span>, user: <span class="type">String</span>, ad: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>


    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Rui Zhang
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://yoursite.com/2024/07/04/SparkStreaming%E7%AC%94%E8%AE%B0/" title="SparkStreaming笔记">http://yoursite.com/2024/07/04/SparkStreaming笔记/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Spark/" rel="tag"># Spark</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/05/17/SparkSQL%E7%AC%94%E8%AE%B0/" rel="prev" title="SparkSQL笔记">
      <i class="fa fa-chevron-left"></i> SparkSQL笔记
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/07/29/Spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/" rel="next" title="Spark性能调优">
      Spark性能调优 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#第1章-SparkStreaming-概述"><span class="nav-number">1.</span> <span class="nav-text">第1章 SparkStreaming 概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-Spark-Streaming-是什么"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 Spark Streaming 是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-Spark-Streaming-的特点"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 Spark Streaming 的特点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-Spark-Streaming-架构"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 Spark Streaming 架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-1-架构图"><span class="nav-number">1.3.1.</span> <span class="nav-text">1.3.1 架构图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-2-背压机制"><span class="nav-number">1.3.2.</span> <span class="nav-text">1.3.2 背压机制</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第-2-章-Dstream-入门"><span class="nav-number">2.</span> <span class="nav-text">第 2 章 Dstream 入门</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-WordCount-案例实操"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 WordCount 案例实操</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-WordCount-解析"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 WordCount 解析</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第-3-章-DStream-创建"><span class="nav-number">3.</span> <span class="nav-text">第 3 章 DStream 创建</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-RDD-队列"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 RDD 队列</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-用法及说明"><span class="nav-number">3.1.1.</span> <span class="nav-text">3.1.1 用法及说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-案例实操"><span class="nav-number">3.1.2.</span> <span class="nav-text">3.1.2 案例实操</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-自定义数据源"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 自定义数据源</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-用法及说明"><span class="nav-number">3.2.1.</span> <span class="nav-text">3.2.1 用法及说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-案例实操"><span class="nav-number">3.2.2.</span> <span class="nav-text">3.2.2 案例实操</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-Kafka-数据源（面试、开发重点）"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 Kafka 数据源（面试、开发重点）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-1-版本选型"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.3.1 版本选型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-2-Kafka-0-8-Receiver-模式（当前版本不适用）"><span class="nav-number">3.3.2.</span> <span class="nav-text">3.3.2 Kafka 0-8 Receiver 模式（当前版本不适用）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-3-Kafka-0-8-Direct-模式（当前版本不适用）"><span class="nav-number">3.3.3.</span> <span class="nav-text">3.3.3 Kafka 0-8 Direct 模式（当前版本不适用）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-4-Kafka-0-10-Direct-模式"><span class="nav-number">3.3.4.</span> <span class="nav-text">3.3.4 Kafka 0-10 Direct 模式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第-4-章-DStream-转换"><span class="nav-number">4.</span> <span class="nav-text">第 4 章 DStream 转换</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-无状态转化操作"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 无状态转化操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-1-Transform"><span class="nav-number">4.1.1.</span> <span class="nav-text">4.1.1 Transform</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-2-join"><span class="nav-number">4.1.2.</span> <span class="nav-text">4.1.2 join</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-有状态转化操作"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 有状态转化操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-1-UpdateStateByKey"><span class="nav-number">4.2.1.</span> <span class="nav-text">4.2.1 UpdateStateByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-2-WindowOperations"><span class="nav-number">4.2.2.</span> <span class="nav-text">4.2.2 WindowOperations</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第-5-章-DStream-输出"><span class="nav-number">5.</span> <span class="nav-text">第 5 章 DStream 输出</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第-6-章-优雅关闭"><span class="nav-number">6.</span> <span class="nav-text">第 6 章 优雅关闭</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第-7-章-SparkStreaming-案例实操"><span class="nav-number">7.</span> <span class="nav-text">第 7 章 SparkStreaming 案例实操</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#7-1-环境准备"><span class="nav-number">7.1.</span> <span class="nav-text">7.1 环境准备</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-1-pom-文件"><span class="nav-number">7.1.1.</span> <span class="nav-text">7.1.1 pom 文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-2-工具类"><span class="nav-number">7.1.2.</span> <span class="nav-text">7.1.2 工具类</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-2-实时数据生成模块"><span class="nav-number">7.2.</span> <span class="nav-text">7.2 实时数据生成模块</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-3-需求一：广告黑名单"><span class="nav-number">7.3.</span> <span class="nav-text">7.3 需求一：广告黑名单</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-1-思路分析"><span class="nav-number">7.3.1.</span> <span class="nav-text">7.3.1 思路分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-2-MySQL-建表"><span class="nav-number">7.3.2.</span> <span class="nav-text">7.3.2 MySQL 建表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-3-环境准备"><span class="nav-number">7.3.3.</span> <span class="nav-text">7.3.3 环境准备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-4-代码实现"><span class="nav-number">7.3.4.</span> <span class="nav-text">7.3.4 代码实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-4-需求二：广告点击量实时统计"><span class="nav-number">7.4.</span> <span class="nav-text">7.4 需求二：广告点击量实时统计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-1-思路分析"><span class="nav-number">7.4.1.</span> <span class="nav-text">7.4.1 思路分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-2-MySQL-建表"><span class="nav-number">7.4.2.</span> <span class="nav-text">7.4.2 MySQL 建表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-3-代码实现"><span class="nav-number">7.4.3.</span> <span class="nav-text">7.4.3 代码实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-5-需求三：-最近一小时广告点击量"><span class="nav-number">7.5.</span> <span class="nav-text">7.5 需求三： 最近一小时广告点击量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-5-1-思路分析"><span class="nav-number">7.5.1.</span> <span class="nav-text">7.5.1 思路分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-5-2-代码实现"><span class="nav-number">7.5.2.</span> <span class="nav-text">7.5.2 代码实现</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Rui Zhang"
      src="/images/head.jpg">
  <p class="site-author-name" itemprop="name">Rui Zhang</p>
  <div class="site-description" itemprop="description">不在沉默中爆发，就在沉默中灭亡</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">52</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rui Zhang</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">1.7m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">25:10</span>
</div>




        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":120,"height":230},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>
