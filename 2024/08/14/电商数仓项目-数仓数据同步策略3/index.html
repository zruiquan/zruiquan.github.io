<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="第1章 实时数仓同步数据实时数仓由Flink源源不断从Kafka当中读数据计算，所以不需要手动同步数据到实时数仓。 第2章 离线数仓同步数据2.1 用户行为数据同步2.1.1 数据通道">
<meta property="og:type" content="article">
<meta property="og:title" content="电商数仓项目-数仓数据同步策略3">
<meta property="og:url" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/index.html">
<meta property="og:site_name" content="没有尾巴的小驴">
<meta property="og:description" content="第1章 实时数仓同步数据实时数仓由Flink源源不断从Kafka当中读数据计算，所以不需要手动同步数据到实时数仓。 第2章 离线数仓同步数据2.1 用户行为数据同步2.1.1 数据通道">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps41.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps43.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps47.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps48.jpg">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps49.jpg">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps51.jpg">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps52.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps53.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps64.jpg">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps65.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps66.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps67.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps68.jpg">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps69.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps70.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps71.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps72.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps73.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps74.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps75.jpg">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps77.jpg">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps55.jpg">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps56.jpg">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps57.jpg">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps58.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/image-20240826171739719.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps59.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps60.png">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps61.jpg">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps62.jpg">
<meta property="og:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/image-20240822110329349.png">
<meta property="article:published_time" content="2024-08-14T06:37:02.000Z">
<meta property="article:modified_time" content="2025-06-25T08:08:46.322Z">
<meta property="article:author" content="Rui Zhang">
<meta property="article:tag" content="数仓">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps41.png">

<link rel="canonical" href="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true
  };
</script>

  <title>电商数仓项目-数仓数据同步策略3 | 没有尾巴的小驴</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">没有尾巴的小驴</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">记录生活</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">22</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">22</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">52</span></a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.jpg">
      <meta itemprop="name" content="Rui Zhang">
      <meta itemprop="description" content="不在沉默中爆发，就在沉默中灭亡">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="没有尾巴的小驴">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          电商数仓项目-数仓数据同步策略3
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-08-14 14:37:02" itemprop="dateCreated datePublished" datetime="2024-08-14T14:37:02+08:00">2024-08-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-06-25 16:08:46" itemprop="dateModified" datetime="2025-06-25T16:08:46+08:00">2025-06-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E4%BB%93/" itemprop="url" rel="index"><span itemprop="name">数仓</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>61k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>55 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="第1章-实时数仓同步数据"><a href="#第1章-实时数仓同步数据" class="headerlink" title="第1章 实时数仓同步数据"></a>第1章 实时数仓同步数据</h1><p>实时数仓由Flink源源不断从Kafka当中读数据计算，所以不需要手动同步数据到实时数仓。</p>
<h1 id="第2章-离线数仓同步数据"><a href="#第2章-离线数仓同步数据" class="headerlink" title="第2章 离线数仓同步数据"></a>第2章 离线数仓同步数据</h1><h2 id="2-1-用户行为数据同步"><a href="#2-1-用户行为数据同步" class="headerlink" title="2.1 用户行为数据同步"></a>2.1 用户行为数据同步</h2><h3 id="2-1-1-数据通道"><a href="#2-1-1-数据通道" class="headerlink" title="2.1.1 数据通道"></a>2.1.1 数据通道</h3><p>用户行为数据由Flume从Kafka直接同步到HDFS，由于离线数仓采用Hive的分区表按天统计，所以目标路径要包含一层日期。具体数据流向如下图所示。</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps41.png" alt="img"></p>
<h3 id="2-1-2-日志消费Flume配置概述"><a href="#2-1-2-日志消费Flume配置概述" class="headerlink" title="2.1.2 日志消费Flume配置概述"></a>2.1.2 日志消费Flume配置概述</h3><p>按照规划，该Flume需将Kafka中topic_log的数据发往HDFS。并且对每天产生的用户行为日志进行区分，将不同天的数据发往HDFS不同天的路径。</p>
<p>此处选择KafkaSource、FileChannel、HDFSSink。</p>
<p>关键配置如下：</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps43.png" alt="img"></p>
<h3 id="2-1-3-日志消费Flume配置实操"><a href="#2-1-3-日志消费Flume配置实操" class="headerlink" title="2.1.3 日志消费Flume配置实操"></a>2.1.3 日志消费Flume配置实操</h3><p>1）创建Flume配置文件</p>
<p>在hadoop104节点的Flume家目录下创建job目录，在job下创建kafka_to_hdfs_log.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 ~]$ cd /opt/module/flume/</span><br><span class="line">[atguigu@hadoop104 flume]$ mkdir job </span><br><span class="line">[atguigu@hadoop104 flume]$ vim job/kafka_to_hdfs_log.conf</span><br></pre></td></tr></table></figure>

<p>2）配置文件内容如下</p>
<p>#定义组件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">定义组件</span></span><br><span class="line">a1.sources=r1</span><br><span class="line">a1.channels=c1</span><br><span class="line">a1.sinks=k1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">配置source1</span></span><br><span class="line">a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">a1.sources.r1.batchSize = 5000</span><br><span class="line">a1.sources.r1.batchDurationMillis = 2000</span><br><span class="line">a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br><span class="line">a1.sources.r1.kafka.topics=topic_log</span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = com.atguigu.gmall.flume.interceptor.TimestampInterceptor$Builder</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">配置channel</span></span><br><span class="line">a1.channels.c1.type = file</span><br><span class="line">a1.channels.c1.checkpointDir = /opt/module/flume/checkpoint/behavior1</span><br><span class="line">a1.channels.c1.dataDirs = /opt/module/flume/data/behavior1</span><br><span class="line">a1.channels.c1.maxFileSize = 2146435071</span><br><span class="line">a1.channels.c1.capacity = 1000000</span><br><span class="line">a1.channels.c1.keep-alive = 6</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">配置sink</span></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_log/%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = log</span><br><span class="line">a1.sinks.k1.hdfs.round = false</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 10</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">控制输出文件类型</span></span><br><span class="line">a1.sinks.k1.hdfs.fileType = CompressedStream</span><br><span class="line">a1.sinks.k1.hdfs.codeC = gzip</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">组装 </span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>3）FileChannel优化</p>
<p>通过配置dataDirs指向多个路径，每个路径对应不同的硬盘，增大Flume吞吐量。</p>
<p>官方说明如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Comma separated list of directories for storing log files. Using multiple directories on separate disks can improve file channel peformance</span><br></pre></td></tr></table></figure>

<p>checkpointDir和backupCheckpointDir也尽量配置在不同硬盘对应的目录中，保证checkpoint坏掉后，可以快速使用backupCheckpointDir恢复数据</p>
<p>4）HDFS Sink优化</p>
<p>（1）HDFS存入大量小文件，有什么影响？</p>
<p>元数据层面：每个小文件都有一份元数据，其中包括文件路径，文件名，所有者，所属组，权限，创建时间等，这些信息都保存在Namenode内存中。所以小文件过多，会占用Namenode服务器大量内存，影响Namenode性能和使用寿命</p>
<p>计算层面：默认情况下MR会对每个小文件启用一个Map任务计算，非常影响计算性能。同时也影响磁盘寻址时间。</p>
<p>​    （2）HDFS小文件处理</p>
<p>官方默认的这三个参数配置写入HDFS后会产生小文件，hdfs.rollInterval、hdfs.rollSize、hdfs.rollCount</p>
<p>基于以上hdfs.rollInterval=3600，hdfs.rollSize=134217728，hdfs.rollCount =0几个参数综合作用，效果如下：</p>
<p>Ø 文件在达到128M时会滚动生成新文件</p>
<p>Ø 文件创建超3600秒时会滚动生成新文件</p>
<p>5）编写Flume拦截器</p>
<p>（1）零点漂移问题</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps47.png" alt="img"></p>
<p>（2）在idea里创建名为gmall的项目</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps48.jpg" alt="img"> </p>
<p>（3）在pom.xml文件中添加如下配置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.62<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（4）在com.atguigu.gmall.flume.interceptor包下创建TimestampInterceptor类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.gmall.flume.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.interceptor.Interceptor;</span><br><span class="line"><span class="keyword">import</span> java.nio.charset.StandardCharsets;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimestampInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//1、获取header和body的数据</span></span><br><span class="line">    Map&lt;String, String&gt; headers = event.getHeaders();</span><br><span class="line">    String log = <span class="keyword">new</span> String(event.getBody(), StandardCharsets.UTF_8);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//2、将body的数据类型转成jsonObject类型（方便获取数据）</span></span><br><span class="line">        JSONObject jsonObject = JSONObject.parseObject(log);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3、header中timestamp时间字段替换成日志生成的时间戳（解决数据漂移问题）</span></span><br><span class="line">        String ts = jsonObject.getString(<span class="string">"ts"</span>);</span><br><span class="line">        headers.put(<span class="string">"timestamp"</span>, ts);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;Event&gt; <span class="title">intercept</span><span class="params">(List&lt;Event&gt; list)</span> </span>&#123;</span><br><span class="line">    Iterator&lt;Event&gt; iterator = list.iterator();</span><br><span class="line">    <span class="keyword">while</span> (iterator.hasNext()) &#123;</span><br><span class="line">        Event event = iterator.next();</span><br><span class="line">        <span class="keyword">if</span> (intercept(event) == <span class="keyword">null</span>) &#123;</span><br><span class="line">            iterator.remove();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> list;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Builder</span> <span class="keyword">implements</span> <span class="title">Interceptor</span>.<span class="title">Builder</span> </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Interceptor <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> TimestampInterceptor();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（5）打包</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps49.jpg" alt="img"> </p>
<p>（6）需要先将打好的包放入到hadoop104的/opt/module/flume/lib文件夹下面。</p>
<h3 id="2-1-4-日志消费Flume测试"><a href="#2-1-4-日志消费Flume测试" class="headerlink" title="2.1.4 日志消费Flume测试"></a>2.1.4 日志消费Flume测试</h3><p>1）启动Zookeeper、Kafka、HDFS</p>
<p>2）启动日志采集Flume</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ f1.sh start</span><br></pre></td></tr></table></figure>

<p>3）启动hadoop104的日志消费Flume</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 flume]$ bin/flume-ng agent -n a1 -c conf/ -f job/kafka_to_hdfs_log.conf</span><br></pre></td></tr></table></figure>

<p>4）生成模拟数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ lg.sh</span><br></pre></td></tr></table></figure>

<p>5）观察HDFS是否出现数据</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps51.jpg" alt="img"> </p>
<h3 id="2-1-5-日志消费Flume启停脚本"><a href="#2-1-5-日志消费Flume启停脚本" class="headerlink" title="2.1.5 日志消费Flume启停脚本"></a>2.1.5 日志消费Flume启停脚本</h3><p>若上述测试通过，为方便，此处创建一个Flume的启停脚本。</p>
<p>1）在hadoop102节点的/home/atguigu/bin目录下创建脚本f2.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ vim f2.sh</span><br></pre></td></tr></table></figure>

<p>​    在脚本中填写如下内容。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">"start")</span><br><span class="line">        echo " --------启动 hadoop104 日志数据flume-------"</span><br><span class="line">        ssh hadoop104 "nohup /opt/module/flume/bin/flume-ng agent -n a1 -c /opt/module/flume/conf -f /opt/module/flume/job/kafka_to_hdfs_log.conf &gt;/dev/null 2&gt;&amp;1 &amp;"</span><br><span class="line">;;</span><br><span class="line">"stop")</span><br><span class="line"></span><br><span class="line">        echo " --------停止 hadoop104 日志数据flume-------"</span><br><span class="line">        ssh hadoop104 "ps -ef | grep kafka_to_hdfs_log | grep -v grep |awk '&#123;print \$2&#125;' | xargs -n1 kill"</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>2）增加脚本执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ chmod 777 f2.sh</span><br></pre></td></tr></table></figure>

<p>3）f2启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ f2.sh start</span><br></pre></td></tr></table></figure>

<p>4）f2停止</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ f2.sh stop</span><br></pre></td></tr></table></figure>

<h2 id="2-2-业务数据同步"><a href="#2-2-业务数据同步" class="headerlink" title="2.2 业务数据同步"></a>2.2 业务数据同步</h2><h3 id="2-2-1-数据同步策略概述"><a href="#2-2-1-数据同步策略概述" class="headerlink" title="2.2.1 数据同步策略概述"></a>2.2.1 数据同步策略概述</h3><p>业务数据是数据仓库的重要数据来源，我们需要每日定时从业务数据库中抽取数据，传输到数据仓库中，之后再对数据进行分析统计。</p>
<p>为保证统计结果的正确性，需要保证数据仓库中的数据与业务数据库是同步的，离线数仓的计算周期通常为天，所以数据同步周期也通常为天，即每天同步一次即可。</p>
<p>数据的同步策略有全量同步和增量同步。</p>
<p>全量同步，就是每天都将业务数据库中的全部数据同步一份到数据仓库，这是保证两侧数据同步的最简单的方式。</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps52.png" alt="img"></p>
<p>增量同步，就是每天只将业务数据中的新增及变化数据同步到数据仓库。采用每日增量同步的表，通常需要在首日先进行一次全量同步。</p>
<h3 id="2-2-2-数据同步策略选择"><a href="#2-2-2-数据同步策略选择" class="headerlink" title="2.2.2 数据同步策略选择"></a>2.2.2 数据同步策略选择</h3><p>两种策略都能保证数据仓库和业务数据库的数据同步，那应该如何选择呢？下面对两种策略进行简要对比。根据上述对比，可以得出以下结论：<br>通常情况，业务表数据量比较大，优先考虑增量，数据量比较小，优先考虑全量；具体选择由数仓模型决定，此处暂不详解。</p>
<table>
<thead>
<tr>
<th>同步策略</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>全量同步</td>
<td>逻辑简单</td>
<td>在某些情况下效率较低。例如某张表数据量较大，但是每天数据的变化比例很低，若对其采用每日全量同步，则会重复同步和存储大量相同的数据。</td>
</tr>
<tr>
<td>增量同步</td>
<td>效率高，无需同步和存储重复数据</td>
<td>逻辑复杂，需要将每日的新增及变化数据同原来的数据进行整合，才能使用</td>
</tr>
</tbody></table>
<p>下图为各表同步策略：</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps53.png" alt="img"></p>
<h3 id="2-2-3-数据同步工具概述"><a href="#2-2-3-数据同步工具概述" class="headerlink" title="2.2.3 数据同步工具概述"></a>2.2.3 数据同步工具概述</h3><p>数据同步工具种类繁多，大致可分为两类，一类是以DataX、Sqoop为代表的基于Select查询的离线、批量同步工具，另一类是以Maxwell、Canal为代表的基于数据库数据变更日志（例如MySQL的binlog，其会实时记录所有的insert、update以及delete操作）的实时流式同步工具。</p>
<p>全量同步通常使用DataX、Sqoop等基于查询的离线同步工具。而增量同步既可以使用DataX、Sqoop等工具，也可使用Maxwell、Canal等工具，下面对增量同步不同方案进行简要对比。</p>
<table>
<thead>
<tr>
<th>增量同步方案</th>
<th>DataX/Sqoop</th>
<th>Maxwell/Canal</th>
</tr>
</thead>
<tbody><tr>
<td>对数据库的要求</td>
<td>原理是基于查询，故若想通过select查询获取新增及变化数据，就要求数据表中存在create_time、update_time等字段，然后根据这些字段获取变更数据。</td>
<td>要求数据库记录变更操作，例如MySQL需开启binlog。</td>
</tr>
<tr>
<td>数据的中间状态</td>
<td>由于是离线批量同步，故若一条数据在一天中变化多次，该方案只能获取最后一个状态，中间状态无法获取。</td>
<td>由于是实时获取所有的数据变更操作，所以可以获取变更数据的所有中间状态。</td>
</tr>
</tbody></table>
<p>本项目中，全量同步采用DataX，增量同步采用Maxwell。</p>
<h3 id="2-2-4-全量表数据同步"><a href="#2-2-4-全量表数据同步" class="headerlink" title="2.2.4 全量表数据同步"></a>2.2.4 全量表数据同步</h3><h4 id="2-2-4-1-数据同步工具DataX部署"><a href="#2-2-4-1-数据同步工具DataX部署" class="headerlink" title="2.2.4.1 数据同步工具DataX部署"></a>2.2.4.1 数据同步工具DataX部署</h4><h5 id="2-2-4-1-1-DataX简介"><a href="#2-2-4-1-1-DataX简介" class="headerlink" title="2.2.4.1.1 DataX简介"></a>2.2.4.1.1 DataX简介</h5><h6 id="2-2-1-1-1-DataX概述"><a href="#2-2-1-1-1-DataX概述" class="headerlink" title="2.2.1.1.1 DataX概述"></a>2.2.1.1.1 DataX概述</h6><p>​    DataX 是阿里巴巴开源的一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。</p>
<p>源码地址：<a href="https://github.com/alibaba/DataX" target="_blank" rel="noopener">https://github.com/alibaba/DataX</a></p>
<h6 id="2-2-1-1-2-DataX支持的数据源"><a href="#2-2-1-1-2-DataX支持的数据源" class="headerlink" title="2.2.1.1.2 DataX支持的数据源"></a>2.2.1.1.2 DataX支持的数据源</h6><p>DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入，目前支持数据如下图。</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>数据源</th>
<th>Reader(读)</th>
<th>Writer(写)</th>
</tr>
</thead>
<tbody><tr>
<td>RDBMS 关系型数据库</td>
<td>MySQL</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>Oracle</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>OceanBase</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>SQLServer</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>PostgreSQL</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>DRDS</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>通用RDBMS</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>阿里云数仓数据存储</td>
<td>ODPS</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>ADS</td>
<td></td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>OSS</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>OCS</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>NoSQL数据存储</td>
<td>OTS</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>Hbase0.94</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>Hbase1.1</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>Phoenix4.x</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>Phoenix5.x</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>MongoDB</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>Hive</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>Cassandra</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>无结构化数据存储</td>
<td>TxtFile</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>FTP</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>HDFS</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>Elasticsearch</td>
<td></td>
<td>√</td>
</tr>
<tr>
<td>时间序列数据库</td>
<td>OpenTSDB</td>
<td>√</td>
<td></td>
</tr>
<tr>
<td></td>
<td>TSDB</td>
<td>√</td>
<td>√</td>
</tr>
</tbody></table>
<h5 id="2-2-4-1-2-DataX架构原理"><a href="#2-2-4-1-2-DataX架构原理" class="headerlink" title="2.2.4.1.2 DataX架构原理"></a>2.2.4.1.2 DataX架构原理</h5><h6 id="2-2-4-1-2-1-DataX设计理念"><a href="#2-2-4-1-2-1-DataX设计理念" class="headerlink" title="2.2.4.1.2.1 DataX设计理念"></a>2.2.4.1.2.1 DataX设计理念</h6><p>为了解决异构数据源同步问题，DataX将复杂的网状的同步链路变成了星型数据链路，DataX作为中间传输载体负责连接各种数据源。当需要接入一个新的数据源的时候，只需要将此数据源对接到DataX，便能跟已有的数据源做到无缝数据同步。</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps64.jpg" alt="img"> </p>
<h6 id="2-2-4-1-2-2-DataX框架设计"><a href="#2-2-4-1-2-2-DataX框架设计" class="headerlink" title="2.2.4.1.2.2 DataX框架设计"></a>2.2.4.1.2.2 DataX框架设计</h6><p>DataX本身作为离线数据同步框架，采用Framework + plugin架构构建。将数据源读取和写入抽象成为Reader/Writer插件，纳入到整个同步框架中。</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps65.png" alt="img"></p>
<h6 id="2-2-4-1-2-3-DataX运行流程"><a href="#2-2-4-1-2-3-DataX运行流程" class="headerlink" title="2.2.4.1.2.3 DataX运行流程"></a>2.2.4.1.2.3 DataX运行流程</h6><p>下面用一个DataX作业生命周期的时序图说明DataX的运行流程、核心概念以及每个概念之间的关系。</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps66.png" alt="img"></p>
<h6 id="2-2-4-1-2-4-DataX调度决策思路"><a href="#2-2-4-1-2-4-DataX调度决策思路" class="headerlink" title="2.2.4.1.2.4 DataX调度决策思路"></a>2.2.4.1.2.4 DataX调度决策思路</h6><p>举例来说，用户提交了一个DataX作业，并且配置了总的并发度为20，目的是对一个有100张分表的mysql数据源进行同步。DataX的调度决策思路是：</p>
<p>（1）DataX Job根据分库分表切分策略，将同步工作分成100个Task。</p>
<p>（2）根据配置的总的并发度20，以及每个Task Group的并发度5，DataX计算共需要分配4个TaskGroup。</p>
<p>（3）4个TaskGroup平分100个Task，每一个TaskGroup负责运行25个Task。</p>
<h6 id="2-2-4-1-2-5-DataX与Sqoop对比"><a href="#2-2-4-1-2-5-DataX与Sqoop对比" class="headerlink" title="2.2.4.1.2.5 DataX与Sqoop对比"></a>2.2.4.1.2.5 DataX与Sqoop对比</h6><table>
<thead>
<tr>
<th>功能</th>
<th>DataX</th>
<th>Sqoop</th>
</tr>
</thead>
<tbody><tr>
<td>运行模式</td>
<td>单进程多线程</td>
<td>MR</td>
</tr>
<tr>
<td>分布式</td>
<td>不支持，可以通过调度系统规避</td>
<td>支持</td>
</tr>
<tr>
<td>流控</td>
<td>有流控功能</td>
<td>需要定制</td>
</tr>
<tr>
<td>统计信息</td>
<td>已有一些统计，上报需定制</td>
<td>没有，分布式的数据收集不方便</td>
</tr>
<tr>
<td>数据校验</td>
<td>在core部分有校验功能</td>
<td>没有，分布式的数据收集不方便</td>
</tr>
<tr>
<td>监控</td>
<td>需要定制</td>
<td>需要定制</td>
</tr>
</tbody></table>
<h5 id="2-2-4-1-3-DataX部署"><a href="#2-2-4-1-3-DataX部署" class="headerlink" title="2.2.4.1.3 DataX部署"></a>2.2.4.1.3 DataX部署</h5><p>1）下载DataX安装包并上传到hadoop102的/opt/software</p>
<p>下载地址：<a href="http://datax-opensource.oss-cn-hangzhou.aliyuncs.com/datax.tar.gz" target="_blank" rel="noopener">http://datax-opensource.oss-cn-hangzhou.aliyuncs.com/datax.tar.gz</a></p>
<p>2）解压datax.tar.gz到/opt/module</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf datax.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>3）自检，执行如下命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ python /opt/module/datax/bin/datax.py /opt/module/datax/job/job.json</span><br></pre></td></tr></table></figure>

<p>出现如下内容，则表明安装成功</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">……</span><br><span class="line">2021-10-12 21:51:12.335 [job-0] INFO  JobContainer - </span><br><span class="line">任务启动时刻                    : 2021-10-12 21:51:02</span><br><span class="line">任务结束时刻                    : 2021-10-12 21:51:12</span><br><span class="line">任务总计耗时                    :                 10s</span><br><span class="line">任务平均流量                    :          253.91KB/s</span><br><span class="line">记录写入速度                    :          10000rec/s</span><br><span class="line">读出记录总数                    :              100000</span><br><span class="line">读写失败总数                    :                   0</span><br></pre></td></tr></table></figure>

<h5 id="2-2-4-1-4-DataX使用"><a href="#2-2-4-1-4-DataX使用" class="headerlink" title="2.2.4.1.4 DataX使用"></a>2.2.4.1.4 DataX使用</h5><h6 id="2-2-4-1-4-1-DataX使用概述"><a href="#2-2-4-1-4-1-DataX使用概述" class="headerlink" title="2.2.4.1.4.1 DataX使用概述"></a>2.2.4.1.4.1 DataX使用概述</h6><p><strong>DataX任务提交命令</strong></p>
<p>DataX的使用十分简单，用户只需根据自己同步数据的数据源和目的地选择相应的Reader和Writer，并将Reader和Writer的信息配置在一个json文件中，然后执行如下命令提交数据同步任务即可。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datax]$ python bin/datax.py path/to/your/job.json</span><br></pre></td></tr></table></figure>

<p> <strong>DataX配置文件格式</strong></p>
<p>可以使用如下命名查看DataX配置文件模板。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ cd /opt/module/datax/</span><br><span class="line">[atguigu@hadoop102 datax]$ python bin/datax.py -r mysqlreader -w hdfswriter</span><br></pre></td></tr></table></figure>

<p>配置文件模板如下，json最外层是一个job，job包含setting和content两部分，其中setting用于对整个job进行配置，content用户配置数据源和目的地。</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps67.png" alt="img"> </p>
<p>Reader和Writer的具体参数可参考官方文档，地址如下：</p>
<p><a href="https://github.com/alibaba/DataX/blob/master/README.md" target="_blank" rel="noopener">https://github.com/alibaba/DataX/blob/master/README.md</a> </p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps68.jpg" alt="img"> </p>
<h6 id="2-2-4-1-4-2-同步MySQL数据到HDFS案例"><a href="#2-2-4-1-4-2-同步MySQL数据到HDFS案例" class="headerlink" title="2.2.4.1.4.2 同步MySQL数据到HDFS案例"></a>2.2.4.1.4.2 同步MySQL数据到HDFS案例</h6><p>案例要求：同步gmall数据库中base_province表数据到HDFS的/base_province目录</p>
<p>需求分析：要实现该功能，需选用MySQLReader和HDFSWriter，MySQLReader具有两种模式分别是TableMode和QuerySQLMode，前者使用table，column，where等属性声明需要同步的数据；后者使用一条SQL查询语句声明需要同步的数据。</p>
<p>下面分别使用两种模式进行演示。</p>
<p><strong>MySQLReader之TableMode</strong></p>
<p>1）编写配置文件</p>
<p>（1）创建配置文件base_province.json</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ vim /opt/module/datax/job/base_province.json</span><br></pre></td></tr></table></figure>

<p>（2）配置文件内容如下</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"job"</span>: &#123;</span><br><span class="line">        <span class="attr">"content"</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"reader"</span>: &#123;</span><br><span class="line">                    <span class="attr">"name"</span>: <span class="string">"mysqlreader"</span>,</span><br><span class="line">                    <span class="attr">"parameter"</span>: &#123;</span><br><span class="line">                        <span class="attr">"column"</span>: [</span><br><span class="line">                            <span class="string">"id"</span>,</span><br><span class="line">                            <span class="string">"name"</span>,</span><br><span class="line">                            <span class="string">"region_id"</span>,</span><br><span class="line">                            <span class="string">"area_code"</span>,</span><br><span class="line">                            <span class="string">"iso_code"</span>,</span><br><span class="line">                            <span class="string">"iso_3166_2"</span>,</span><br><span class="line">							<span class="string">"create_time"</span>,</span><br><span class="line">							<span class="string">"operate_time"</span></span><br><span class="line">                        ],</span><br><span class="line">                        <span class="attr">"where"</span>: <span class="string">"id&gt;=3"</span>,</span><br><span class="line">                        <span class="attr">"connection"</span>: [</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"jdbcUrl"</span>: [</span><br><span class="line">                                    <span class="string">"jdbc:mysql://hadoop102:3306/gmall?useUnicode=true&amp;allowPublicKeyRetrieval=true&amp;characterEncoding=utf-8"</span></span><br><span class="line">                                ],</span><br><span class="line">                                <span class="attr">"table"</span>: [</span><br><span class="line">                                    <span class="string">"base_province"</span></span><br><span class="line">                                ]</span><br><span class="line">                            &#125;</span><br><span class="line">                        ],</span><br><span class="line">                        <span class="attr">"password"</span>: <span class="string">"000000"</span>,</span><br><span class="line">                        <span class="attr">"splitPk"</span>: <span class="string">""</span>,</span><br><span class="line">                        <span class="attr">"username"</span>: <span class="string">"root"</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="attr">"writer"</span>: &#123;</span><br><span class="line">                    <span class="attr">"name"</span>: <span class="string">"hdfswriter"</span>,</span><br><span class="line">                    <span class="attr">"parameter"</span>: &#123;</span><br><span class="line">                        <span class="attr">"column"</span>: [</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"id"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"bigint"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"name"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"region_id"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"area_code"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"iso_code"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"iso_3166_2"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"create_time"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"operate_time"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;</span><br><span class="line">                        ],</span><br><span class="line">                        <span class="attr">"compress"</span>: <span class="string">"gzip"</span>,</span><br><span class="line">                        <span class="attr">"defaultFS"</span>: <span class="string">"hdfs://hadoop102:8020"</span>,</span><br><span class="line">                        <span class="attr">"fieldDelimiter"</span>: <span class="string">"\t"</span>,</span><br><span class="line">                        <span class="attr">"fileName"</span>: <span class="string">"base_province"</span>,</span><br><span class="line">                        <span class="attr">"fileType"</span>: <span class="string">"text"</span>,</span><br><span class="line">                        <span class="attr">"path"</span>: <span class="string">"/base_province"</span>,</span><br><span class="line">                        <span class="attr">"writeMode"</span>: <span class="string">"append"</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"setting"</span>: &#123;</span><br><span class="line">            <span class="attr">"speed"</span>: &#123;</span><br><span class="line">                <span class="attr">"channel"</span>: <span class="number">1</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）配置文件说明</p>
<p>（1）Reader参数说明</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps69.png" alt="img"></p>
<p>（2）Writer参数说明</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps70.png" alt="img">注意事项：</p>
<p>HFDS Writer并未提供nullFormat参数：也就是用户并不能自定义null值写到HFDS文件中的存储格式。默认情况下，HFDS Writer会将null值存储为空字符串（’’），而Hive默认的null值存储格式为N。所以后期将DataX同步的文件导入Hive表就会出现问题。</p>
<p>解决该问题的方案有两个：</p>
<p>一是修改DataX HDFS Writer的源码，增加自定义null值存储格式的逻辑，可参考<a href="https://blog.csdn.net/u010834071/article/details/105506580。" target="_blank" rel="noopener">https://blog.csdn.net/u010834071/article/details/105506580。</a></p>
<p>二是在Hive中建表时指定null值存储格式为空字符串（’’），例如：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> base_province;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> base_province</span><br><span class="line">(</span><br><span class="line">    <span class="string">`id`</span>         <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'编号'</span>,</span><br><span class="line">    <span class="string">`name`</span>       <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'省份名称'</span>,</span><br><span class="line">    <span class="string">`region_id`</span>  <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'地区ID'</span>,</span><br><span class="line">    <span class="string">`area_code`</span>  <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'地区编码'</span>,</span><br><span class="line">    <span class="string">`iso_code`</span>   <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'旧版ISO-3166-2编码，供可视化使用'</span>,</span><br><span class="line">    <span class="string">`iso_3166_2`</span> <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'新版IOS-3166-2编码，供可视化使用'</span></span><br><span class="line">) <span class="keyword">COMMENT</span> <span class="string">'省份表'</span></span><br><span class="line">    <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line">    <span class="literal">NULL</span> DEFINED <span class="keyword">AS</span> <span class="string">''</span></span><br><span class="line">LOCATION <span class="string">'/base_province/'</span>;</span><br></pre></td></tr></table></figure>

<p>Hive部署下文详解，此处关注空值格式定义语法即可。</p>
<p>（3）Setting参数说明</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps71.png" alt="img"></p>
<p>3）提交任务</p>
<p>（1）在HDFS创建/base_province目录</p>
<p>使用DataX向HDFS同步数据时，需确保目标路径已存在</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datax]$ hadoop fs -mkdir /base_province</span><br></pre></td></tr></table></figure>

<p>（2）进入DataX根目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datax]$ cd /opt/module/datax</span><br></pre></td></tr></table></figure>

<p>（3）执行如下命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datax]$ python bin/datax.py job/base_province.json</span><br></pre></td></tr></table></figure>

<p>4）查看结果</p>
<p>（1）DataX打印日志</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2021-10-13 11:13:14.930 [job-0] INFO  JobContainer - </span><br><span class="line">任务启动时刻                    : 2021-10-13 11:13:03</span><br><span class="line">任务结束时刻                    : 2021-10-13 11:13:14</span><br><span class="line">任务总计耗时                    :                 11s</span><br><span class="line">任务平均流量                    :               66B/s</span><br><span class="line">记录写入速度                    :              3rec/s</span><br><span class="line">读出记录总数                    :                  32</span><br><span class="line">读写失败总数                    :                   0</span><br></pre></td></tr></table></figure>

<p>（2）查看HDFS文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datax]$ hadoop fs -cat /base_province/* | zcat</span><br><span class="line"></span><br><span class="line">3	山西	1	140000	CN-14	CN-SX</span><br><span class="line">4	内蒙古	1	150000	CN-15	CN-NM</span><br><span class="line">5	河北	1	130000	CN-13	CN-HE</span><br><span class="line">6	上海	2	310000	CN-31	CN-SH</span><br><span class="line">7	江苏	2	320000	CN-32	CN-JS</span><br><span class="line">8	浙江	2	330000	CN-33	CN-ZJ</span><br><span class="line">9	安徽	2	340000	CN-34	CN-AH</span><br><span class="line">10	福建	2	350000	CN-35	CN-FJ</span><br><span class="line">11	江西	2	360000	CN-36	CN-JX</span><br><span class="line">12	山东	2	370000	CN-37	CN-SD</span><br><span class="line">14	台湾	2	710000	CN-71	CN-TW</span><br><span class="line">15	黑龙江	3	230000	CN-23	CN-HL</span><br><span class="line">16	吉林	3	220000	CN-22	CN-JL</span><br><span class="line">17	辽宁	3	210000	CN-21	CN-LN</span><br><span class="line">18	陕西	7	610000	CN-61	CN-SN</span><br><span class="line">19	甘肃	7	620000	CN-62	CN-GS</span><br><span class="line">20	青海	7	630000	CN-63	CN-QH</span><br><span class="line">21	宁夏	7	640000	CN-64	CN-NX</span><br><span class="line">22	新疆	7	650000	CN-65	CN-XJ</span><br><span class="line">23	河南	4	410000	CN-41	CN-HA</span><br><span class="line">24	湖北	4	420000	CN-42	CN-HB</span><br><span class="line">25	湖南	4	430000	CN-43	CN-HN</span><br><span class="line">26	广东	5	440000	CN-44	CN-GD</span><br><span class="line">27	广西	5	450000	CN-45	CN-GX</span><br><span class="line">28	海南	5	460000	CN-46	CN-HI</span><br><span class="line">29	香港	5	810000	CN-91	CN-HK</span><br><span class="line">30	澳门	5	820000	CN-92	CN-MO</span><br><span class="line">31	四川	6	510000	CN-51	CN-SC</span><br><span class="line">32	贵州	6	520000	CN-52	CN-GZ</span><br><span class="line">33	云南	6	530000	CN-53	CN-YN</span><br><span class="line">13	重庆	6	500000	CN-50	CN-CQ</span><br><span class="line">34	西藏	6	540000	CN-54	CN-XZ</span><br></pre></td></tr></table></figure>

<p><strong>MySQLReader之QuerySQLMode</strong></p>
<p>1）编写配置文件</p>
<p>（1）修改配置文件base_province.json</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ vim /opt/module/datax/job/base_province_sql.json</span><br></pre></td></tr></table></figure>

<p>（2）配置文件内容如下</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"job"</span>: &#123;</span><br><span class="line">        <span class="attr">"content"</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"reader"</span>: &#123;</span><br><span class="line">                    <span class="attr">"name"</span>: <span class="string">"mysqlreader"</span>,</span><br><span class="line">                    <span class="attr">"parameter"</span>: &#123;</span><br><span class="line">                        <span class="attr">"connection"</span>: [</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"jdbcUrl"</span>: [</span><br><span class="line">                                    <span class="string">"jdbc:mysql://hadoop102:3306/gmall?useUnicode=true&amp;allowPublicKeyRetrieval=true&amp;characterEncoding=utf-8"</span></span><br><span class="line">                                ],</span><br><span class="line">                                <span class="attr">"querySql"</span>: [</span><br><span class="line">                                    <span class="string">"select id,name,region_id,area_code,iso_code,iso_3166_2,create_time,operate_time from base_province where id&gt;=3"</span></span><br><span class="line">                                ]</span><br><span class="line">                            &#125;</span><br><span class="line">                        ],</span><br><span class="line">                        <span class="attr">"password"</span>: <span class="string">"000000"</span>,</span><br><span class="line">                        <span class="attr">"username"</span>: <span class="string">"root"</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="attr">"writer"</span>: &#123;</span><br><span class="line">                    <span class="attr">"name"</span>: <span class="string">"hdfswriter"</span>,</span><br><span class="line">                    <span class="attr">"parameter"</span>: &#123;</span><br><span class="line">                        <span class="attr">"column"</span>: [</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"id"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"bigint"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"name"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"region_id"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"area_code"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"iso_code"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"iso_3166_2"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"create_time"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"operate_time"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;</span><br><span class="line">                        ],</span><br><span class="line">                        <span class="attr">"compress"</span>: <span class="string">"gzip"</span>,</span><br><span class="line">                        <span class="attr">"defaultFS"</span>: <span class="string">"hdfs://hadoop102:8020"</span>,</span><br><span class="line">                        <span class="attr">"fieldDelimiter"</span>: <span class="string">"\t"</span>,</span><br><span class="line">                        <span class="attr">"fileName"</span>: <span class="string">"base_province"</span>,</span><br><span class="line">                        <span class="attr">"fileType"</span>: <span class="string">"text"</span>,</span><br><span class="line">                        <span class="attr">"path"</span>: <span class="string">"/base_province"</span>,</span><br><span class="line">                        <span class="attr">"writeMode"</span>: <span class="string">"append"</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"setting"</span>: &#123;</span><br><span class="line">            <span class="attr">"speed"</span>: &#123;</span><br><span class="line">                <span class="attr">"channel"</span>: <span class="number">1</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）配置文件说明</p>
<p>（1）Reader参数说明</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps72.png" alt="img"></p>
<p>3）提交任务</p>
<p>（1）清空历史数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datax]$ hadoop fs -rm -r -f /base_province/*</span><br></pre></td></tr></table></figure>

<p>（2）进入DataX根目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datax]$ cd /opt/module/datax</span><br></pre></td></tr></table></figure>

<p>（3）执行如下命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datax]$ python bin/datax.py job/base_province_sql.json</span><br></pre></td></tr></table></figure>

<p>4）查看结果</p>
<p>（1）DataX打印日志</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2021-10-13 11:13:14.930 [job-0] INFO  JobContainer - </span><br><span class="line">任务启动时刻                    : 2021-10-13 11:13:03</span><br><span class="line">任务结束时刻                    : 2021-10-13 11:13:14</span><br><span class="line">任务总计耗时                    :                 11s</span><br><span class="line">任务平均流量                    :               66B/s</span><br><span class="line">记录写入速度                    :              3rec/s</span><br><span class="line">读出记录总数                    :                  32</span><br><span class="line">读写失败总数                    :                   0</span><br></pre></td></tr></table></figure>

<p>（2）查看HDFS文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datax]$ hadoop fs -cat /base_province/* | zcat</span><br><span class="line"></span><br><span class="line">3	山西	1	140000	CN-14	CN-SX</span><br><span class="line">4	内蒙古	1	150000	CN-15	CN-NM</span><br><span class="line">5	河北	1	130000	CN-13	CN-HE</span><br><span class="line">6	上海	2	310000	CN-31	CN-SH</span><br><span class="line">7	江苏	2	320000	CN-32	CN-JS</span><br><span class="line">8	浙江	2	330000	CN-33	CN-ZJ</span><br><span class="line">9	安徽	2	340000	CN-34	CN-AH</span><br><span class="line">10	福建	2	350000	CN-35	CN-FJ</span><br><span class="line">11	江西	2	360000	CN-36	CN-JX</span><br><span class="line">12	山东	2	370000	CN-37	CN-SD</span><br><span class="line">14	台湾	2	710000	CN-71	CN-TW</span><br><span class="line">15	黑龙江	3	230000	CN-23	CN-HL</span><br><span class="line">16	吉林	3	220000	CN-22	CN-JL</span><br><span class="line">17	辽宁	3	210000	CN-21	CN-LN</span><br><span class="line">18	陕西	7	610000	CN-61	CN-SN</span><br><span class="line">19	甘肃	7	620000	CN-62	CN-GS</span><br><span class="line">20	青海	7	630000	CN-63	CN-QH</span><br><span class="line">21	宁夏	7	640000	CN-64	CN-NX</span><br><span class="line">22	新疆	7	650000	CN-65	CN-XJ</span><br><span class="line">23	河南	4	410000	CN-41	CN-HA</span><br><span class="line">24	湖北	4	420000	CN-42	CN-HB</span><br><span class="line">25	湖南	4	430000	CN-43	CN-HN</span><br><span class="line">26	广东	5	440000	CN-44	CN-GD</span><br><span class="line">27	广西	5	450000	CN-45	CN-GX</span><br><span class="line">28	海南	5	460000	CN-46	CN-HI</span><br><span class="line">29	香港	5	810000	CN-91	CN-HK</span><br><span class="line">30	澳门	5	820000	CN-92	CN-MO</span><br><span class="line">31	四川	6	510000	CN-51	CN-SC</span><br><span class="line">32	贵州	6	520000	CN-52	CN-GZ</span><br><span class="line">33	云南	6	530000	CN-53	CN-YN</span><br><span class="line">13	重庆	6	500000	CN-50	CN-CQ</span><br><span class="line">34	西藏	6	540000	CN-54	CN-XZ</span><br></pre></td></tr></table></figure>

<p><strong>DataX传参</strong></p>
<p>通常情况下，离线数据同步任务需要每日定时重复执行，故HDFS上的目标路径通常会包含一层日期，以对每日同步的数据加以区分，也就是说每日同步数据的目标路径不是固定不变的，因此DataX配置文件中HDFS Writer的path参数的值应该是动态的。为实现这一效果，就需要使用DataX传参的功能。</p>
<p>DataX传参的用法如下，在JSON配置文件中使用${param}引用参数，在提交任务时使用-p”-Dparam=value”传入参数值，具体示例如下。</p>
<p>1）编写配置文件</p>
<p>（1）修改配置文件base_province.json</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ vim /opt/module/datax/job/base_province.json</span><br></pre></td></tr></table></figure>

<p>（2）配置文件内容如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "job": &#123;</span><br><span class="line">        "content": [</span><br><span class="line">            &#123;</span><br><span class="line">                "reader": &#123;</span><br><span class="line">                    "name": "mysqlreader",</span><br><span class="line">                    "parameter": &#123;</span><br><span class="line">                        "connection": [</span><br><span class="line">                            &#123;</span><br><span class="line">                                "jdbcUrl": [</span><br><span class="line">                                    "jdbc:mysql://hadoop102:3306/gmall?useUnicode=true&amp;allowPublicKeyRetrieval=true&amp;characterEncoding=utf-8"</span><br><span class="line">                                ],</span><br><span class="line">                                "querySql": [</span><br><span class="line">                                    "select id,name,region_id,area_code,iso_code,iso_3166_2,create_time,operate_time from base_province where id&gt;=3"</span><br><span class="line">                                ]</span><br><span class="line">                            &#125;</span><br><span class="line">                        ],</span><br><span class="line">                        "password": "000000",</span><br><span class="line">                        "username": "root"</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                "writer": &#123;</span><br><span class="line">                    "name": "hdfswriter",</span><br><span class="line">                    "parameter": &#123;</span><br><span class="line">                        "column": [</span><br><span class="line">                            &#123;</span><br><span class="line">                                "name": "id",</span><br><span class="line">                                "type": "bigint"</span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                "name": "name",</span><br><span class="line">                                "type": "string"</span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                "name": "region_id",</span><br><span class="line">                                "type": "string"</span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                "name": "area_code",</span><br><span class="line">                                "type": "string"</span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                "name": "iso_code",</span><br><span class="line">                                "type": "string"</span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                "name": "iso_3166_2",</span><br><span class="line">                                "type": "string"</span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                "name": "create_time",</span><br><span class="line">                                "type": "string"</span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                "name": "operate_time",</span><br><span class="line">                                "type": "string"</span><br><span class="line">                            &#125;</span><br><span class="line">                        ],</span><br><span class="line">                        "compress": "gzip",</span><br><span class="line">                        "defaultFS": "hdfs://hadoop102:8020",</span><br><span class="line">                        "fieldDelimiter": "\t",</span><br><span class="line">                        "fileName": "base_province",</span><br><span class="line">                        "fileType": "text",</span><br><span class="line">                        "path": "/base_province/$&#123;dt&#125;",</span><br><span class="line">                        "writeMode": "append"</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        "setting": &#123;</span><br><span class="line">            "speed": &#123;</span><br><span class="line">                "channel": 1</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）提交任务</p>
<p>（1）创建目标路径</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datax]$ hadoop fs -mkdir /base_province/2022-06-08</span><br></pre></td></tr></table></figure>

<p>（2）进入DataX根目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datax]$ cd /opt/module/datax</span><br></pre></td></tr></table></figure>

<p>（3）执行如下命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datax]$ python bin/datax.py -p"-Ddt=2022-06-08" job/base_province.json</span><br></pre></td></tr></table></figure>

<p>3）查看结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datax]$ hadoop fs -ls /base_province</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-10-15 21:41 /base_province/2022-06-08</span><br></pre></td></tr></table></figure>

<h6 id="2-2-4-1-4-3-同步HDFS数据到MySQL案例"><a href="#2-2-4-1-4-3-同步HDFS数据到MySQL案例" class="headerlink" title="2.2.4.1.4.3 同步HDFS数据到MySQL案例"></a>2.2.4.1.4.3 同步HDFS数据到MySQL案例</h6><p>案例要求：同步HDFS上的/base_province目录下的数据到MySQL gmall数据库下的test_province表。</p>
<p>需求分析：要实现该功能，需选用HDFSReader和MySQLWriter。</p>
<p>1）编写配置文件</p>
<p>（1）创建配置文件test_province.json</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ vim /opt/module/datax/job/test_province.json</span><br></pre></td></tr></table></figure>

<p>（2）配置文件内容如下</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"job"</span>: &#123;</span><br><span class="line">        <span class="attr">"content"</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"reader"</span>: &#123;</span><br><span class="line">                    <span class="attr">"name"</span>: <span class="string">"hdfsreader"</span>,</span><br><span class="line">                    <span class="attr">"parameter"</span>: &#123;</span><br><span class="line">                        <span class="attr">"defaultFS"</span>: <span class="string">"hdfs://hadoop102:8020"</span>,</span><br><span class="line">                        <span class="attr">"path"</span>: <span class="string">"/base_province"</span>,</span><br><span class="line">                        <span class="attr">"column"</span>: [</span><br><span class="line">                            <span class="string">"*"</span></span><br><span class="line">                        ],</span><br><span class="line">                        <span class="attr">"fileType"</span>: <span class="string">"text"</span>,</span><br><span class="line">                        <span class="attr">"compress"</span>: <span class="string">"gzip"</span>,</span><br><span class="line">                        <span class="attr">"encoding"</span>: <span class="string">"UTF-8"</span>,</span><br><span class="line">                        <span class="attr">"nullFormat"</span>: <span class="string">"\\N"</span>,</span><br><span class="line">                        <span class="attr">"fieldDelimiter"</span>: <span class="string">"\t"</span>,</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="attr">"writer"</span>: &#123;</span><br><span class="line">                    <span class="attr">"name"</span>: <span class="string">"mysqlwriter"</span>,</span><br><span class="line">                    <span class="attr">"parameter"</span>: &#123;</span><br><span class="line">                        <span class="attr">"username"</span>: <span class="string">"root"</span>,</span><br><span class="line">                        <span class="attr">"password"</span>: <span class="string">"000000"</span>,</span><br><span class="line">                        <span class="attr">"connection"</span>: [</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"table"</span>: [</span><br><span class="line">                                    <span class="string">"test_province"</span></span><br><span class="line">                                ],</span><br><span class="line">                                <span class="attr">"jdbcUrl"</span>: <span class="string">"jdbc:mysql://hadoop102:3306/gmall?useUnicode=true&amp;allowPublicKeyRetrieval=true&amp;characterEncoding=utf-8"</span></span><br><span class="line">                            &#125;</span><br><span class="line">                        ],</span><br><span class="line">                        <span class="attr">"column"</span>: [</span><br><span class="line">                            <span class="string">"id"</span>,</span><br><span class="line">                            <span class="string">"name"</span>,</span><br><span class="line">                            <span class="string">"region_id"</span>,</span><br><span class="line">                            <span class="string">"area_code"</span>,</span><br><span class="line">                            <span class="string">"iso_code"</span>,</span><br><span class="line">                            <span class="string">"iso_3166_2"</span>,</span><br><span class="line">							<span class="string">"create_time"</span>,</span><br><span class="line">							<span class="string">"operate_time"</span></span><br><span class="line">                        ],</span><br><span class="line">                        <span class="attr">"writeMode"</span>: <span class="string">"replace"</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"setting"</span>: &#123;</span><br><span class="line">            <span class="attr">"speed"</span>: &#123;</span><br><span class="line">                <span class="attr">"channel"</span>: <span class="number">1</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）配置文件说明</p>
<p>（1）Reader参数说明</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps73.png" alt="img"></p>
<p>（2）Writer参数说明</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps74.png" alt="img"></p>
<p>3）提交任务</p>
<p>（1）在MySQL中创建gmall.test_province表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`test_province`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`test_province`</span>  (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">COLLATE</span> utf8_general_ci <span class="literal">NULL</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`region_id`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">COLLATE</span> utf8_general_ci <span class="literal">NULL</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`area_code`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">COLLATE</span> utf8_general_ci <span class="literal">NULL</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`iso_code`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">COLLATE</span> utf8_general_ci <span class="literal">NULL</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`iso_3166_2`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">COLLATE</span> utf8_general_ci <span class="literal">NULL</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`create_time`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">COLLATE</span> utf8_general_ci <span class="literal">NULL</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`operate_time`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">COLLATE</span> utf8_general_ci <span class="literal">NULL</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span> = <span class="keyword">InnoDB</span> <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> = utf8 <span class="keyword">COLLATE</span> = utf8_general_ci ROW_FORMAT = Dynamic;</span><br></pre></td></tr></table></figure>

<p>（2）进入DataX根目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datax]$ cd /opt/module/datax</span><br></pre></td></tr></table></figure>

<p>（3）执行如下命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datax]$ python bin/datax.py job/test_province.json</span><br></pre></td></tr></table></figure>

<p>4）查看结果</p>
<p>（1）DataX打印日志</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2021-10-13 15:21:35.006 [job-0] INFO  JobContainer - </span><br><span class="line">任务启动时刻                    : 2021-10-13 15:21:23</span><br><span class="line">任务结束时刻                    : 2021-10-13 15:21:35</span><br><span class="line">任务总计耗时                    :                 11s</span><br><span class="line">任务平均流量                    :               70B/s</span><br><span class="line">记录写入速度                    :              3rec/s</span><br><span class="line">读出记录总数                    :                  32</span><br><span class="line">读写失败总数                    :                   0</span><br></pre></td></tr></table></figure>

<p>（2）查看MySQL目标表数据</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps75.jpg" alt="img"> </p>
<p>此处导出的源文件位于hdfs://hadoop102:8020/base_province路径，上文将数据由mysql导入该文件时过滤了id为1和2的记录，因而test_province仅有32条数据。</p>
<h5 id="2-2-4-1-5-DataX优化"><a href="#2-2-4-1-5-DataX优化" class="headerlink" title="2.2.4.1.5 DataX优化"></a>2.2.4.1.5 DataX优化</h5><h6 id="2-2-4-1-5-1-速度控制"><a href="#2-2-4-1-5-1-速度控制" class="headerlink" title="2.2.4.1.5.1 速度控制"></a>2.2.4.1.5.1 速度控制</h6><p>DataX3.0提供了包括通道(并发)、记录流、字节流三种流控模式，可以随意控制你的作业速度，让你的作业在数据库可以承受的范围内达到最佳的同步速度。</p>
<p>关键优化参数如下：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>job.setting.speed.channel</td>
<td>并发数</td>
</tr>
<tr>
<td>job.setting.speed.record</td>
<td>总record限速</td>
</tr>
<tr>
<td>job.setting.speed.byte</td>
<td>总byte限速</td>
</tr>
<tr>
<td>core.transport.channel.speed.record</td>
<td>单个channel的record限速，默认值为10000（10000条/s）</td>
</tr>
<tr>
<td>core.transport.channel.speed.byte</td>
<td>单个channel的byte限速，默认值10241024（1M/s）</td>
</tr>
</tbody></table>
<p>注意事项：</p>
<p>（1）若配置了总record限速，则必须配置单个channel的record限速</p>
<p>（2）若配置了总byte限速，则必须配置单个channel的byte限速</p>
<p>（3）若配置了总record限速和总byte限速，channel并发数参数就会失效。因为配置了总record限速和总byte限速之后，实际channel并发数是通过计算得到的：</p>
<p>计算公式为:</p>
<p>min(总byte限速/单个channel的byte限速，总record限速/单个channel的record限速)</p>
<p>配置示例：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"core"</span>: &#123;</span><br><span class="line">        <span class="attr">"transport"</span>: &#123;</span><br><span class="line">            <span class="attr">"channel"</span>: &#123;</span><br><span class="line">                <span class="attr">"speed"</span>: &#123;</span><br><span class="line">                    <span class="attr">"byte"</span>: <span class="number">1048576</span> <span class="comment">//单个channel byte限速1M/s</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"job"</span>: &#123;</span><br><span class="line">        <span class="attr">"setting"</span>: &#123;</span><br><span class="line">            <span class="attr">"speed"</span>: &#123;</span><br><span class="line">                <span class="attr">"byte"</span> : <span class="number">5242880</span> <span class="comment">//总byte限速5M/s</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="2-2-4-1-5-2-内存调整"><a href="#2-2-4-1-5-2-内存调整" class="headerlink" title="2.2.4.1.5.2 内存调整"></a>2.2.4.1.5.2 内存调整</h6><p>当提升DataX Job内Channel并发数时，内存的占用会显著增加，因为DataX作为数据交换通道，在内存中会缓存较多的数据。例如Channel中会有一个Buffer，作为临时的数据交换的缓冲区，而在部分Reader和Writer的中，也会存在一些Buffer，为了防止OOM等错误，需调大JVM的堆内存。</p>
<p>建议将内存设置为4G或者8G，这个也可以根据实际情况来调整。</p>
<p>调整JVM xms xmx参数的两种方式：一种是直接更改datax.py脚本；另一种是在启动的时候，加上对应的参数，如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python datax/bin/datax.py --jvm="-Xms8G -Xmx8G" /path/to/your/job.json</span><br></pre></td></tr></table></figure>

<h4 id="2-2-4-2-数据通道"><a href="#2-2-4-2-数据通道" class="headerlink" title="2.2.4.2 数据通道"></a>2.2.4.2 数据通道</h4><p>全量表数据由DataX从MySQL业务数据库直接同步到HDFS，具体数据流向，如下图所示。</p>
<h4 id="2-2-4-3-DataX配置文件"><a href="#2-2-4-3-DataX配置文件" class="headerlink" title="2.2.4.3 DataX配置文件"></a>2.2.4.3 DataX配置文件</h4><p>我们需要为每张全量表编写一个DataX的json配置文件，此处以activity_info为例，配置文件内容如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"job"</span>: &#123;</span><br><span class="line">        <span class="attr">"content"</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"reader"</span>: &#123;</span><br><span class="line">                    <span class="attr">"name"</span>: <span class="string">"mysqlreader"</span>,</span><br><span class="line">                    <span class="attr">"parameter"</span>: &#123;</span><br><span class="line">                        <span class="attr">"column"</span>: [</span><br><span class="line">                            <span class="string">"id"</span>,</span><br><span class="line">                            <span class="string">"activity_name"</span>,</span><br><span class="line">                            <span class="string">"activity_type"</span>,</span><br><span class="line">                            <span class="string">"activity_desc"</span>,</span><br><span class="line">                            <span class="string">"start_time"</span>,</span><br><span class="line">                            <span class="string">"end_time"</span>,</span><br><span class="line">                            <span class="string">"create_time"</span></span><br><span class="line">                        ],</span><br><span class="line">                        <span class="attr">"connection"</span>: [</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"jdbcUrl"</span>: [</span><br><span class="line">                                    <span class="string">"jdbc:mysql://hadoop102:3306/gmall?useUnicode=true&amp;allowPublicKeyRetrieval=true&amp;characterEncoding=utf-8"</span></span><br><span class="line">                                ],</span><br><span class="line">                                <span class="attr">"table"</span>: [</span><br><span class="line">                                    <span class="string">"activity_info"</span></span><br><span class="line">                                ]</span><br><span class="line">                            &#125;</span><br><span class="line">                        ],</span><br><span class="line">                        <span class="attr">"password"</span>: <span class="string">"000000"</span>,</span><br><span class="line">                        <span class="attr">"splitPk"</span>: <span class="string">""</span>,</span><br><span class="line">                        <span class="attr">"username"</span>: <span class="string">"root"</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="attr">"writer"</span>: &#123;</span><br><span class="line">                    <span class="attr">"name"</span>: <span class="string">"hdfswriter"</span>,</span><br><span class="line">                    <span class="attr">"parameter"</span>: &#123;</span><br><span class="line">                        <span class="attr">"column"</span>: [</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"id"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"bigint"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"activity_name"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"activity_type"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"activity_desc"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"start_time"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"end_time"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="attr">"name"</span>: <span class="string">"create_time"</span>,</span><br><span class="line">                                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                            &#125;</span><br><span class="line">                        ],</span><br><span class="line">                        <span class="attr">"compress"</span>: <span class="string">"gzip"</span>,</span><br><span class="line">                        <span class="attr">"defaultFS"</span>: <span class="string">"hdfs://hadoop102:8020"</span>,</span><br><span class="line">                        <span class="attr">"fieldDelimiter"</span>: <span class="string">"\t"</span>,</span><br><span class="line">                        <span class="attr">"fileName"</span>: <span class="string">"activity_info"</span>,</span><br><span class="line">                        <span class="attr">"fileType"</span>: <span class="string">"text"</span>,</span><br><span class="line">                        <span class="attr">"path"</span>: <span class="string">"$&#123;targetdir&#125;"</span>,</span><br><span class="line">                        <span class="attr">"writeMode"</span>: <span class="string">"truncate"</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"setting"</span>: &#123;</span><br><span class="line">            <span class="attr">"speed"</span>: &#123;</span><br><span class="line">                <span class="attr">"channel"</span>: <span class="number">1</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注：由于目标路径包含一层日期，用于对不同天的数据加以区分，故path参数并未写死，需在提交任务时通过参数动态传入，参数名称为targetdir。</p>
<h4 id="2-2-4-4-DataX配置文件生成"><a href="#2-2-4-4-DataX配置文件生成" class="headerlink" title="2.2.4.4 DataX配置文件生成"></a>2.2.4.4 DataX配置文件生成</h4><p>1）DataX配置文件生成器使用</p>
<h5 id="2-2-4-4-1-代码"><a href="#2-2-4-4-1-代码" class="headerlink" title="2.2.4.4.1 代码"></a>2.2.4.4.1 代码</h5><h6 id="2-2-4-4-1-1-新建Maven项目"><a href="#2-2-4-4-1-1-新建Maven项目" class="headerlink" title="2.2.4.4.1.1 新建Maven项目"></a>2.2.4.4.1.1 新建Maven项目</h6><p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps77.jpg" alt="img"> </p>
<h6 id="2-2-4-4-1-2-添加如下依赖"><a href="#2-2-4-4-1-2-添加如下依赖" class="headerlink" title="2.2.4.4.1.2 添加如下依赖"></a>2.2.4.4.1.2 添加如下依赖</h6><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span>	</span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>8<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>8<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>cn.hutool<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hutool-json<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.8.11<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>cn.hutool<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hutool-db<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.8.11<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>8.0.31<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!--指定jar包的入口类，UDF用不到主类，因此不用指定--&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.atguigu.datax.Main<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!--将依赖编译到jar包中--&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!--配置执行器--&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="comment">&lt;!--绑定到package执行周期上--&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="comment">&lt;!--只运行一次--&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h6 id="2-2-4-4-1-3-新建com-atguigu-datax-beans-Column类"><a href="#2-2-4-4-1-3-新建com-atguigu-datax-beans-Column类" class="headerlink" title="2.2.4.4.1.3 新建com.atguigu.datax.beans.Column类"></a>2.2.4.4.1.3 新建com.atguigu.datax.beans.Column类</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.datax.beans;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Column</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String type;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String hiveType;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Map&lt;String, String&gt; typeMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        typeMap.put(<span class="string">"bigint"</span>, <span class="string">"bigint"</span>);</span><br><span class="line">        typeMap.put(<span class="string">"int"</span>, <span class="string">"bigint"</span>);</span><br><span class="line">        typeMap.put(<span class="string">"smallint"</span>, <span class="string">"bigint"</span>);</span><br><span class="line">        typeMap.put(<span class="string">"tinyint"</span>, <span class="string">"bigint"</span>);</span><br><span class="line">        typeMap.put(<span class="string">"double"</span>, <span class="string">"double"</span>);</span><br><span class="line">        typeMap.put(<span class="string">"float"</span>, <span class="string">"float"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Column</span><span class="params">(String name, String type)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.type = type;</span><br><span class="line">        <span class="keyword">this</span>.hiveType = typeMap.getOrDefault(type, <span class="string">"string"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">name</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">type</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> type;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">hiveType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> hiveType;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h6 id="2-2-4-4-1-4-新建com-atguigu-datax-beans-Table类"><a href="#2-2-4-4-1-4-新建com-atguigu-datax-beans-Table类" class="headerlink" title="2.2.4.4.1.4 新建com.atguigu.datax.beans.Table类"></a>2.2.4.4.1.4 新建com.atguigu.datax.beans.Table类</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.datax.beans;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.stream.Collectors;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Table</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String tableName;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> List&lt;Column&gt; columns;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Table</span><span class="params">(String tableName)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.tableName = tableName;</span><br><span class="line">        <span class="keyword">this</span>.columns = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">name</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> tableName;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addColumn</span><span class="params">(String name, String type)</span> </span>&#123;</span><br><span class="line">        columns.add(<span class="keyword">new</span> Column(name, type));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;String&gt; <span class="title">getColumnNames</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> columns.stream().map(Column::name).collect(Collectors.toList());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> List&lt;Map&lt;String, String&gt;&gt; getColumnNamesAndTypes() &#123;</span><br><span class="line">        List&lt;Map&lt;String, String&gt;&gt; result = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        columns.forEach(column -&gt; &#123;</span><br><span class="line">            Map&lt;String, String&gt; temp = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">            temp.put(<span class="string">"name"</span>, column.name());</span><br><span class="line">            temp.put(<span class="string">"type"</span>, column.hiveType());</span><br><span class="line">            result.add(temp);</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="2-2-4-4-1-5-新建com-atguigu-datax-configuration-Configuration类"><a href="#2-2-4-4-1-5-新建com-atguigu-datax-configuration-Configuration类" class="headerlink" title="2.2.4.4.1.5 新建com.atguigu.datax.configuration.Configuration类"></a>2.2.4.4.1.5 新建com.atguigu.datax.configuration.Configuration类</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.datax.configuration;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.nio.file.Files;</span><br><span class="line"><span class="keyword">import</span> java.nio.file.Path;</span><br><span class="line"><span class="keyword">import</span> java.nio.file.Paths;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Configuration</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String MYSQL_USER;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String MYSQL_PASSWORD;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String MYSQL_HOST;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String MYSQL_PORT;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String MYSQL_DATABASE_IMPORT;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String MYSQL_DATABASE_EXPORT;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String MYSQL_URL_IMPORT;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String MYSQL_URL_EXPORT;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String MYSQL_TABLES_IMPORT;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String MYSQL_TABLES_EXPORT;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String IS_SEPERATED_TABLES;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String HDFS_URI;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String IMPORT_OUT_DIR;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String EXPORT_OUT_DIR;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String IMPORT_MIGRATION_TYPE = <span class="string">"import"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String EXPORT_MIGRATION_TYPE = <span class="string">"export"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        Path path = Paths.get(<span class="string">"configuration.properties"</span>);</span><br><span class="line">        Properties configuration = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            configuration.load(Files.newBufferedReader(path));</span><br><span class="line">            MYSQL_USER = configuration.getProperty(<span class="string">"mysql.username"</span>, <span class="string">"root"</span>);</span><br><span class="line">            MYSQL_PASSWORD = configuration.getProperty(<span class="string">"mysql.password"</span>, <span class="string">"000000"</span>);</span><br><span class="line">            MYSQL_HOST = configuration.getProperty(<span class="string">"mysql.host"</span>, <span class="string">"hadoop102"</span>);</span><br><span class="line">            MYSQL_PORT = configuration.getProperty(<span class="string">"mysql.port"</span>, <span class="string">"3306"</span>);</span><br><span class="line">            MYSQL_DATABASE_IMPORT = configuration.getProperty(<span class="string">"mysql.database.import"</span>, <span class="string">"gmall"</span>);</span><br><span class="line">            MYSQL_DATABASE_EXPORT = configuration.getProperty(<span class="string">"mysql.database.export"</span>, <span class="string">"gmall"</span>);</span><br><span class="line">            MYSQL_URL_IMPORT = <span class="string">"jdbc:mysql://"</span> + MYSQL_HOST + <span class="string">":"</span> + MYSQL_PORT + <span class="string">"/"</span> + MYSQL_DATABASE_IMPORT + <span class="string">"?useSSL=false&amp;allowPublicKeyRetrieval=true&amp;useUnicode=true&amp;characterEncoding=utf-8"</span>;</span><br><span class="line">            MYSQL_URL_EXPORT = <span class="string">"jdbc:mysql://"</span> + MYSQL_HOST + <span class="string">":"</span> + MYSQL_PORT + <span class="string">"/"</span> + MYSQL_DATABASE_EXPORT + <span class="string">"?useSSL=false&amp;allowPublicKeyRetrieval=true&amp;useUnicode=true&amp;characterEncoding=utf-8"</span>;</span><br><span class="line">            MYSQL_TABLES_IMPORT = configuration.getProperty(<span class="string">"mysql.tables.import"</span>, <span class="string">""</span>);</span><br><span class="line">            MYSQL_TABLES_EXPORT = configuration.getProperty(<span class="string">"mysql.tables.export"</span>, <span class="string">""</span>);</span><br><span class="line">            IS_SEPERATED_TABLES = configuration.getProperty(<span class="string">"is.seperated.tables"</span>, <span class="string">"0"</span>);</span><br><span class="line">            HDFS_URI = configuration.getProperty(<span class="string">"hdfs.uri"</span>, <span class="string">"hdfs://hadoop102:8020"</span>);</span><br><span class="line">            IMPORT_OUT_DIR = configuration.getProperty(<span class="string">"import_out_dir"</span>);</span><br><span class="line">            EXPORT_OUT_DIR = configuration.getProperty(<span class="string">"export_out_dir"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            MYSQL_USER = <span class="string">"root"</span>;</span><br><span class="line">            MYSQL_PASSWORD = <span class="string">"000000"</span>;</span><br><span class="line">            MYSQL_HOST = <span class="string">"hadoop102"</span>;</span><br><span class="line">            MYSQL_PORT = <span class="string">"3306"</span>;</span><br><span class="line">            MYSQL_DATABASE_IMPORT = <span class="string">"gmall"</span>;</span><br><span class="line">            MYSQL_DATABASE_EXPORT = <span class="string">"gmall"</span>;</span><br><span class="line">            MYSQL_URL_IMPORT = <span class="string">"jdbc:mysql://"</span> + MYSQL_HOST + <span class="string">":"</span> + MYSQL_PORT + <span class="string">"/"</span> + MYSQL_DATABASE_IMPORT + <span class="string">"?useSSL=false&amp;allowPublicKeyRetrieval=true&amp;useUnicode=true&amp;characterEncoding=utf-8"</span>;</span><br><span class="line">            MYSQL_URL_EXPORT = <span class="string">"jdbc:mysql://"</span> + MYSQL_HOST + <span class="string">":"</span> + MYSQL_PORT + <span class="string">"/"</span> + MYSQL_DATABASE_EXPORT + <span class="string">"?useSSL=false&amp;allowPublicKeyRetrieval=true&amp;useUnicode=true&amp;characterEncoding=utf-8"</span>;</span><br><span class="line">            MYSQL_TABLES_IMPORT = <span class="string">""</span>;</span><br><span class="line">            MYSQL_TABLES_EXPORT = <span class="string">""</span>;</span><br><span class="line">            IS_SEPERATED_TABLES = <span class="string">"0"</span>;</span><br><span class="line">            HDFS_URI = <span class="string">"hdfs://hadoop102:8020"</span>;</span><br><span class="line">            IMPORT_OUT_DIR = <span class="keyword">null</span>;</span><br><span class="line">            EXPORT_OUT_DIR = <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(MYSQL_DATABASE_EXPORT);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="2-2-4-4-1-6-新建com-atguigu-datax-helper-DataxJsonHelper类"><a href="#2-2-4-4-1-6-新建com-atguigu-datax-helper-DataxJsonHelper类" class="headerlink" title="2.2.4.4.1.6 新建com.atguigu.datax.helper.DataxJsonHelper类"></a>2.2.4.4.1.6 新建com.atguigu.datax.helper.DataxJsonHelper类</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.datax.helper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cn.hutool.json.JSONObject;</span><br><span class="line"><span class="keyword">import</span> cn.hutool.json.JSONUtil;</span><br><span class="line"><span class="keyword">import</span> com.atguigu.datax.beans.Table;</span><br><span class="line"><span class="keyword">import</span> com.atguigu.datax.configuration.Configuration;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataxJsonHelper</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 解析 inputConfig 和 outputConfig 模板</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Hadoop 单点集群</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> JSONObject inputConfig = JSONUtil.parseObj(<span class="string">"&#123;\"job\":&#123;\"content\":[&#123;\"reader\":&#123;\"name\":\"mysqlreader\",\"parameter\":&#123;\"column\":[],\"connection\":[&#123;\"jdbcUrl\":[],\"table\":[]&#125;],\"password\":\"\",\"splitPk\":\"\",\"username\":\"\"&#125;&#125;,\"writer\":&#123;\"name\":\"hdfswriter\",\"parameter\":&#123;\"column\":[],\"compress\":\"gzip\",\"defaultFS\":\"\",\"fieldDelimiter\":\"\\t\",\"fileName\":\"content\",\"fileType\":\"text\",\"path\":\"$&#123;targetdir&#125;\",\"writeMode\":\"truncate\",\"nullFormat\":\"\"&#125;&#125;&#125;],\"setting\":&#123;\"speed\":&#123;\"channel\":1&#125;&#125;&#125;&#125;"</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> JSONObject outputConfig = JSONUtil.parseObj(<span class="string">"&#123;\"job\":&#123;\"setting\":&#123;\"speed\":&#123;\"channel\":1&#125;&#125;,\"content\":[&#123;\"reader\":&#123;\"name\":\"hdfsreader\",\"parameter\":&#123;\"path\":\"$&#123;exportdir&#125;\",\"defaultFS\":\"\",\"column\":[\"*\"],\"fileType\":\"text\",\"encoding\":\"UTF-8\",\"fieldDelimiter\":\"\\t\",\"nullFormat\":\"\\\\N\"&#125;&#125;,\"writer\":&#123;\"name\":\"mysqlwriter\",\"parameter\":&#123;\"writeMode\":\"replace\",\"username\":\"\",\"password\":\"\",\"column\":[],\"connection\":[&#123;\"jdbcUrl\":\"\",\"table\":[]&#125;]&#125;&#125;&#125;]&#125;&#125;"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Hadoop HA 集群</span></span><br><span class="line"><span class="comment">//    private final JSONObject inputConfig = JSONUtil.parseObj("&#123;\"job\": &#123;\"content\": [&#123;\"reader\": &#123;\"name\": \"mysqlreader\",\"parameter\": &#123;\"column\": [],\"connection\": [&#123;\"jdbcUrl\": [],\"table\": []&#125;],\"password\": \"\",\"splitPk\": \"\",\"username\": \"\"&#125;&#125;,\"writer\": &#123;\"name\": \"hdfswriter\",\"parameter\": &#123;\"column\": [],\"compress\": \"gzip\",\"defaultFS\": \"hdfs://mycluster\",\"dfs.nameservices\": \"mycluster\",\"dfs.ha.namenodes.mycluster\": \"namenode1,namenode2\",\"dfs.namenode.rpc-address.aliDfs.namenode1\": \"hdfs://com.tstzyls-hadoop101:8020\",\"dfs.namenode.rpc-address.aliDfs.namenode2\": \"hdfs://com.tstzyls-hadoop102:8020\",\"dfs.client.failover.proxy.provider.mycluster\": \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\",\"fieldDelimiter\": \"\\t\",\"fileName\": \"content\",\"fileType\": \"text\",\"path\": \"$&#123;targetdir&#125;\",\"writeMode\": \"truncate\",\"nullFormat\": \"\"&#125;&#125;&#125;],\"setting\": &#123;\"speed\": &#123;\"channel\": 1&#125;&#125;&#125;&#125;");</span></span><br><span class="line"><span class="comment">//    private final JSONObject outputConfig = JSONUtil.parseObj("&#123;\"job\": &#123;\"setting\": &#123;\"speed\": &#123;\"channel\": 1&#125;&#125;,\"content\": [&#123;\"reader\": &#123;\"name\": \"hdfsreader\",\"parameter\": &#123;\"path\": \"$&#123;exportdir&#125;\",\"defaultFS\": \"\",\"dfs.nameservices\": \"mycluster\",\"dfs.ha.namenodes.mycluster\": \"namenode1,namenode2\",\"dfs.namenode.rpc-address.aliDfs.namenode1\": \"hdfs://com.tstzyls-hadoop101:8020\",\"dfs.namenode.rpc-address.aliDfs.namenode2\": \"hdfs://com.tstzyls-hadoop102:8020\",\"dfs.client.failover.proxy.provider.mycluster\": \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\",\"column\": [\"*\"],\"fileType\": \"text\",\"encoding\": \"UTF-8\",\"fieldDelimiter\": \"\\t\",\"nullFormat\": \"\\\\N\"&#125;&#125;,\"writer\": &#123;\"name\": \"mysqlwriter\",\"parameter\": &#123;\"writeMode\": \"replace\",\"username\": \"\",\"password\": \"\",\"column\": [],\"connection\": [&#123;\"jdbcUrl\": [],\"table\": []&#125;]&#125;&#125;&#125;]&#125;&#125;");</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DataxJsonHelper</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 获取 Reader 和 Writer 配置</span></span><br><span class="line">        JSONObject mysqlReaderPara = inputConfig.getByPath(<span class="string">"job.content[0].reader.parameter"</span>, JSONObject<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        JSONObject hdfsWriterPara = inputConfig.getByPath(<span class="string">"job.content[0].writer.parameter"</span>, JSONObject<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        JSONObject hdfsReaderPara = outputConfig.getByPath(<span class="string">"job.content[0].reader.parameter"</span>, JSONObject<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        JSONObject mysqlWriterPara = outputConfig.getByPath(<span class="string">"job.content[0].writer.parameter"</span>, JSONObject<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 DefaultFS</span></span><br><span class="line">        hdfsReaderPara.set(<span class="string">"defaultFS"</span>, Configuration.HDFS_URI);</span><br><span class="line">        hdfsWriterPara.set(<span class="string">"defaultFS"</span>, Configuration.HDFS_URI);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 MySQL Username</span></span><br><span class="line">        mysqlReaderPara.set(<span class="string">"username"</span>, Configuration.MYSQL_USER);</span><br><span class="line">        mysqlWriterPara.set(<span class="string">"username"</span>, Configuration.MYSQL_USER);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 MySQL Password</span></span><br><span class="line">        mysqlReaderPara.set(<span class="string">"password"</span>, Configuration.MYSQL_PASSWORD);</span><br><span class="line">        mysqlWriterPara.set(<span class="string">"password"</span>, Configuration.MYSQL_PASSWORD);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 JDBC URL</span></span><br><span class="line">        mysqlReaderPara.putByPath(<span class="string">"connection[0].jdbcUrl[0]"</span>, Configuration.MYSQL_URL_IMPORT);</span><br><span class="line">        mysqlWriterPara.putByPath(<span class="string">"connection[0].jdbcUrl"</span>, Configuration.MYSQL_URL_EXPORT);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 写回Reader和Writer配置</span></span><br><span class="line">        inputConfig.putByPath(<span class="string">"job.content[0].reader.parameter"</span>, mysqlReaderPara);</span><br><span class="line">        inputConfig.putByPath(<span class="string">"job.content[0].writer.parameter"</span>, hdfsWriterPara);</span><br><span class="line">        outputConfig.putByPath(<span class="string">"job.content[0].reader.parameter"</span>, hdfsReaderPara);</span><br><span class="line">        outputConfig.putByPath(<span class="string">"job.content[0].writer.parameter"</span>, mysqlWriterPara);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setTableAndColumns</span><span class="params">(Table table, <span class="keyword">int</span> index, String migrationType)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 设置表名</span></span><br><span class="line">        setTable(table, index, migrationType);</span><br><span class="line">        <span class="comment">// 设置列名及路径</span></span><br><span class="line">        setColumns(table, migrationType);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setColumns</span><span class="params">(Table table, String migrationType)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (migrationType.equals(<span class="string">"import"</span>)) &#123;</span><br><span class="line">            <span class="comment">// 设置 hdfswriter 文件名</span></span><br><span class="line">            inputConfig.putByPath(<span class="string">"job.content[0].writer.parameter.fileName"</span>, table.name());</span><br><span class="line">            <span class="comment">// 设置列名</span></span><br><span class="line">            inputConfig.putByPath(<span class="string">"job.content[0].reader.parameter.column"</span>, table.getColumnNames());</span><br><span class="line">            inputConfig.putByPath(<span class="string">"job.content[0].writer.parameter.column"</span>, table.getColumnNamesAndTypes());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 设置列名</span></span><br><span class="line">            outputConfig.putByPath(<span class="string">"job.content[0].writer.parameter.column"</span>, table.getColumnNames());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setTable</span><span class="params">(Table table, <span class="keyword">int</span> index, String migrationType)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (migrationType.equals(<span class="string">"import"</span>)) &#123;</span><br><span class="line">            <span class="comment">// 设置表名</span></span><br><span class="line">            inputConfig.putByPath(<span class="string">"job.content[0].reader.parameter.connection[0].table["</span> + index + <span class="string">"]"</span>, table.name());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            outputConfig.putByPath(<span class="string">"job.content[0].writer.parameter.connection[0].table["</span> + index + <span class="string">"]"</span>, table.name());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> JSONObject <span class="title">getInputConfig</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> inputConfig;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> JSONObject <span class="title">getOutputConfig</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> outputConfig;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="2-2-4-4-1-7-新建com-atguigu-datax-helper-MysqlHelper类"><a href="#2-2-4-4-1-7-新建com-atguigu-datax-helper-MysqlHelper类" class="headerlink" title="2.2.4.4.1.7 新建com.atguigu.datax.helper.MysqlHelper类"></a>2.2.4.4.1.7 新建com.atguigu.datax.helper.MysqlHelper类</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.datax.helper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cn.hutool.db.Db;</span><br><span class="line"><span class="keyword">import</span> cn.hutool.db.Entity;</span><br><span class="line"><span class="keyword">import</span> cn.hutool.db.ds.DSFactory;</span><br><span class="line"><span class="keyword">import</span> cn.hutool.setting.Setting;</span><br><span class="line"><span class="keyword">import</span> com.atguigu.datax.beans.Table;</span><br><span class="line"><span class="keyword">import</span> com.atguigu.datax.configuration.Configuration;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.SQLException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Comparator;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MysqlHelper</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> List&lt;Table&gt; tables;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Table&gt; <span class="title">getTables</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> tables;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">MysqlHelper</span><span class="params">(String url, String database, String mysqlTables)</span> </span>&#123;</span><br><span class="line">        tables = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        Db db = Db.use(DSFactory.create(</span><br><span class="line">                Setting.create()</span><br><span class="line">                        .set(<span class="string">"url"</span>, url)</span><br><span class="line">                        .set(<span class="string">"user"</span>, Configuration.MYSQL_USER)</span><br><span class="line">                        .set(<span class="string">"pass"</span>, Configuration.MYSQL_PASSWORD)</span><br><span class="line">                        .set(<span class="string">"showSql"</span>, <span class="string">"false"</span>)</span><br><span class="line">                        .set(<span class="string">"showParams"</span>, <span class="string">"false"</span>)</span><br><span class="line">                        .set(<span class="string">"sqlLevel"</span>, <span class="string">"info"</span>)</span><br><span class="line">        ).getDataSource());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取设置的表格，如未设置，查询数据库下面所有表格</span></span><br><span class="line">        <span class="keyword">if</span> (mysqlTables != <span class="keyword">null</span> &amp;&amp; !<span class="string">""</span>.equals(mysqlTables)) &#123;</span><br><span class="line">            <span class="keyword">for</span> (String mysqlTable : mysqlTables.split(<span class="string">","</span>)) &#123;</span><br><span class="line">                tables.add(<span class="keyword">new</span> Table(mysqlTable));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                db.findAll(Entity.create(<span class="string">"information_schema.TABLES"</span>)</span><br><span class="line">                                .set(<span class="string">"TABLE_SCHEMA"</span>, database))</span><br><span class="line">                        .forEach(entity -&gt;</span><br><span class="line">                                tables.add(<span class="keyword">new</span> Table(entity.getStr(<span class="string">"TABLE_NAME"</span>))));</span><br><span class="line">            &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取所有表格的列</span></span><br><span class="line">        <span class="keyword">for</span> (Table table : tables) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                db.findAll(Entity.create(<span class="string">"information_schema.COLUMNS"</span>)</span><br><span class="line">                                .set(<span class="string">"TABLE_SCHEMA"</span>, database)</span><br><span class="line">                                .set(<span class="string">"TABLE_NAME"</span>, table.name())</span><br><span class="line">                        ).stream()</span><br><span class="line">                        .sorted(Comparator.comparingInt(o -&gt; o.getInt(<span class="string">"ORDINAL_POSITION"</span>)))</span><br><span class="line">                        .forEach(entity -&gt; table.addColumn(</span><br><span class="line">                                entity.getStr(<span class="string">"COLUMN_NAME"</span>),</span><br><span class="line">                                entity.getStr(<span class="string">"DATA_TYPE"</span>)</span><br><span class="line">                        ));</span><br><span class="line">            &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="2-2-4-4-1-8-新建com-atguigu-datax-Main类"><a href="#2-2-4-4-1-8-新建com-atguigu-datax-Main类" class="headerlink" title="2.2.4.4.1.8 新建com.atguigu.datax.Main类"></a>2.2.4.4.1.8 新建com.atguigu.datax.Main类</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.datax;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cn.hutool.json.JSONUtil;</span><br><span class="line"><span class="keyword">import</span> com.atguigu.datax.beans.Table;</span><br><span class="line"><span class="keyword">import</span> com.atguigu.datax.configuration.Configuration;</span><br><span class="line"><span class="keyword">import</span> com.atguigu.datax.helper.DataxJsonHelper;</span><br><span class="line"><span class="keyword">import</span> com.atguigu.datax.helper.MysqlHelper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.FileWriter;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.nio.file.Files;</span><br><span class="line"><span class="keyword">import</span> java.nio.file.Paths;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 生成 HDFS 入方向配置文件</span></span><br><span class="line">        <span class="keyword">if</span> (Configuration.IMPORT_OUT_DIR != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                !Configuration.IMPORT_OUT_DIR.equals(<span class="string">""</span>)) &#123;</span><br><span class="line">            MysqlHelper mysqlHelper = <span class="keyword">new</span> MysqlHelper(</span><br><span class="line">                    Configuration.MYSQL_URL_IMPORT,</span><br><span class="line">                    Configuration.MYSQL_DATABASE_IMPORT,</span><br><span class="line">                    Configuration.MYSQL_TABLES_IMPORT);</span><br><span class="line">            DataxJsonHelper dataxJsonHelper = <span class="keyword">new</span> DataxJsonHelper();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取迁移操作类型</span></span><br><span class="line">            String migrationType = Configuration.IMPORT_MIGRATION_TYPE;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 创建父文件夹</span></span><br><span class="line">            Files.createDirectories(Paths.get(Configuration.IMPORT_OUT_DIR));</span><br><span class="line">            List&lt;Table&gt; tables = mysqlHelper.getTables();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 判断传入的表是否为分表，根据判断结果采用不同的处理策略</span></span><br><span class="line">            <span class="keyword">if</span> (Configuration.IS_SEPERATED_TABLES.equals(<span class="string">"1"</span>)) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; tables.size(); i++) &#123;</span><br><span class="line">                    Table table = tables.get(i);</span><br><span class="line">                    dataxJsonHelper.setTable(table, i, migrationType);</span><br><span class="line">                &#125;</span><br><span class="line">                dataxJsonHelper.setColumns(tables.get(<span class="number">0</span>), migrationType);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 输出最终Json配置</span></span><br><span class="line">                FileWriter inputWriter = <span class="keyword">new</span> FileWriter(Configuration.IMPORT_OUT_DIR + <span class="string">"/"</span> + Configuration.MYSQL_DATABASE_IMPORT + <span class="string">"."</span> + tables.get(<span class="number">0</span>).name() + <span class="string">".json"</span>);</span><br><span class="line">                JSONUtil.toJsonStr(dataxJsonHelper.getInputConfig(), inputWriter);</span><br><span class="line">                inputWriter.close();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">for</span> (Table table : tables) &#123;</span><br><span class="line">                    <span class="comment">// 设置表信息</span></span><br><span class="line">                    dataxJsonHelper.setTableAndColumns(table, <span class="number">0</span>, migrationType);</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 输出最终Json配置</span></span><br><span class="line">                    FileWriter inputWriter = <span class="keyword">new</span> FileWriter(Configuration.IMPORT_OUT_DIR + <span class="string">"/"</span> + Configuration.MYSQL_DATABASE_IMPORT + <span class="string">"."</span> + table.name() + <span class="string">".json"</span>);</span><br><span class="line">                    JSONUtil.toJsonStr(dataxJsonHelper.getInputConfig(), inputWriter);</span><br><span class="line">                    inputWriter.close();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 生成 HDFS 出方向配置文件</span></span><br><span class="line">        <span class="keyword">if</span> (Configuration.EXPORT_OUT_DIR != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                !<span class="string">""</span>.equals(Configuration.EXPORT_OUT_DIR)) &#123;</span><br><span class="line">            MysqlHelper mysqlHelper = <span class="keyword">new</span> MysqlHelper(</span><br><span class="line">                    Configuration.MYSQL_URL_EXPORT,</span><br><span class="line">                    Configuration.MYSQL_DATABASE_EXPORT,</span><br><span class="line">                    Configuration.MYSQL_TABLES_EXPORT);</span><br><span class="line">            DataxJsonHelper dataxJsonHelper = <span class="keyword">new</span> DataxJsonHelper();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取迁移操作类型</span></span><br><span class="line">            String migrationType = Configuration.EXPORT_MIGRATION_TYPE;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 创建父文件夹</span></span><br><span class="line">            Files.createDirectories(Paths.get(Configuration.EXPORT_OUT_DIR));</span><br><span class="line">            List&lt;Table&gt; tables = mysqlHelper.getTables();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (Configuration.IS_SEPERATED_TABLES.equals(<span class="string">"1"</span>)) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; tables.size(); i++) &#123;</span><br><span class="line">                    Table table = tables.get(i);</span><br><span class="line">                    dataxJsonHelper.setTable(table, i, migrationType);</span><br><span class="line">                &#125;</span><br><span class="line">                dataxJsonHelper.setColumns(tables.get(<span class="number">0</span>), migrationType);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 输出最终Json配置</span></span><br><span class="line">                FileWriter outputWriter = <span class="keyword">new</span> FileWriter(Configuration.EXPORT_OUT_DIR + <span class="string">"/"</span> + Configuration.MYSQL_DATABASE_EXPORT + <span class="string">"."</span> + tables.get(<span class="number">0</span>).name() + <span class="string">".json"</span>);</span><br><span class="line">                JSONUtil.toJsonStr(dataxJsonHelper.getOutputConfig(), outputWriter);</span><br><span class="line">                outputWriter.close();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (Table table : tables) &#123;</span><br><span class="line">                <span class="comment">// 设置表信息</span></span><br><span class="line">                dataxJsonHelper.setTableAndColumns(table, <span class="number">0</span>, migrationType);</span><br><span class="line">                <span class="comment">// 输出最终Json配置</span></span><br><span class="line">                FileWriter outputWriter = <span class="keyword">new</span> FileWriter(Configuration.EXPORT_OUT_DIR + <span class="string">"/"</span> + Configuration.MYSQL_DATABASE_EXPORT + <span class="string">"."</span> + table.name() + <span class="string">".json"</span>);</span><br><span class="line">                JSONUtil.toJsonStr(dataxJsonHelper.getOutputConfig(), outputWriter);</span><br><span class="line">                outputWriter.close();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="2-2-4-4-2-如何使用"><a href="#2-2-4-4-2-如何使用" class="headerlink" title="2.2.4.4.2 如何使用"></a>2.2.4.4.2 如何使用</h5><h6 id="2-2-4-4-2-1-修改配置文件"><a href="#2-2-4-4-2-1-修改配置文件" class="headerlink" title="2.2.4.4.2.1 修改配置文件"></a>2.2.4.4.2.1 修改配置文件</h6><p>在项目根目录新建配置文件：configuration.properties</p>
<table>
<thead>
<tr>
<th>key</th>
<th>default</th>
<th>description</th>
</tr>
</thead>
<tbody><tr>
<td>mysql.username</td>
<td>root</td>
<td>MySQL用户名</td>
</tr>
<tr>
<td>mysql.password</td>
<td>000000</td>
<td>MySQL密码</td>
</tr>
<tr>
<td>mysql.host</td>
<td>hadoop102</td>
<td>MySQL所在Host</td>
</tr>
<tr>
<td>mysql.port</td>
<td>3306</td>
<td>MySQL端口号</td>
</tr>
<tr>
<td>mysql.database.import</td>
<td>gmall</td>
<td>导入HDFS的数据库</td>
</tr>
<tr>
<td>mysql.database.export</td>
<td>gmall</td>
<td>从HDFS导出的数据库</td>
</tr>
<tr>
<td>mysql.tables.import</td>
<td>“”</td>
<td>需要导入的表，空字符串表示全部导入</td>
</tr>
<tr>
<td>mysql.tables.export</td>
<td>“”</td>
<td>需要导出的表，空字符串表示全部导出</td>
</tr>
<tr>
<td>is.seperated.tables</td>
<td>0</td>
<td>是否为分表，0为否</td>
</tr>
<tr>
<td>hdfs.uri</td>
<td>hdfs://hadoop102:8020</td>
<td>HDFS Namenode 地址</td>
</tr>
<tr>
<td>import_out_dir</td>
<td>null</td>
<td>导入HDFS的配置文件输出地址</td>
</tr>
<tr>
<td>export_out_dir</td>
<td>null</td>
<td>从HDFS导出的配置文件输出地址</td>
</tr>
</tbody></table>
<p>文件内容如下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mysql.username=root</span><br><span class="line">mysql.password=000000</span><br><span class="line">mysql.host=hadoop102</span><br><span class="line">mysql.port=3306</span><br><span class="line">mysql.database.import=gmall</span><br><span class="line">mysql.database.export=gmall</span><br><span class="line">mysql.tables.import=</span><br><span class="line">mysql.tables.export=</span><br><span class="line">is.seperated.tables=0</span><br><span class="line">hdfs.uri=hdfs://hadoop102:8020</span><br><span class="line">import_out_dir=d:/output/import</span><br><span class="line">export_out_dir=d:/output/export</span><br></pre></td></tr></table></figure>

<h6 id="2-2-4-4-2-2-运行"><a href="#2-2-4-4-2-2-运行" class="headerlink" title="2.2.4.4.2.2 运行"></a>2.2.4.4.2.2 运行</h6><p>执行mvn clean package打包，target目录会生成datax-config-generator-1.0-SNAPSHOT-jar-with-dependencies.jar。和configuration.properties文件一并拷入Linux，执行即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ java -jar datax-config-generator-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>

<p>2）将生成器上传到服务器的/opt/module/gen_datax_config目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ mkdir /opt/module/gen_datax_config</span><br><span class="line">[atguigu@hadoop102 ~]$ cd /opt/module/gen_datax_config</span><br></pre></td></tr></table></figure>

<p>3）上传生成器</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps55.jpg" alt="img"> </p>
<p>4）修改configuration.properties配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mysql.username=root</span><br><span class="line">mysql.password=000000</span><br><span class="line">mysql.host=hadoop102</span><br><span class="line">mysql.port=3306</span><br><span class="line">mysql.database.import=gmall</span><br><span class="line"><span class="meta">#</span><span class="bash"> mysql.database.export=gmall</span></span><br><span class="line">mysql.tables.import=activity_info,activity_rule,base_trademark,cart_info,base_category1,base_category2,base_category3,coupon_info,sku_attr_value,sku_sale_attr_value,base_dic,sku_info,base_province,spu_info,base_region,promotion_pos,promotion_refer</span><br><span class="line"><span class="meta">#</span><span class="bash"> mysql.tables.export=</span></span><br><span class="line">is.seperated.tables=0</span><br><span class="line">hdfs.uri=hdfs://hadoop102:8020</span><br><span class="line">import_out_dir=/opt/module/datax/job/import</span><br><span class="line"><span class="meta">#</span><span class="bash"> export_out_dir=</span></span><br></pre></td></tr></table></figure>

<p>5）执行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 gen_datax_config]$ java -jar datax-config-generator-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>

<p>6）观察结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ ll /opt/module/datax/job/import</span><br><span class="line">总用量 68</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu  976 2月   3 21:56 activity_info.json</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 1116 2月   3 21:56 activity_rule.json</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu  747 2月   3 21:56 base_category1.json</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu  802 2月   3 21:56 base_category2.json</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu  802 2月   3 21:56 base_category3.json</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu  814 2月   3 21:56 base_dic.json</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu  942 2月   3 21:56 base_province.json</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu  758 2月   3 21:58 base_region.json</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu  800 2月   3 21:56 base_trademark.json</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 1234 2月   3 21:56 cart_info.json</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 1461 2月   3 21:56 coupon_info.json</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu  868 2月   3 21:56 promotion_pos.json</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu  760 2月   3 21:56 promotion_refer.json</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu  943 2月   3 21:56 sku_attr_value.json</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 1125 2月   3 21:56 sku_info.json</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 1051 2月   3 21:56 sku_sale_attr_value.json</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu  898 2月   3 21:56 spu_info.json</span><br></pre></td></tr></table></figure>

<h4 id="2-2-4-5-测试生成的DataX配置文件"><a href="#2-2-4-5-测试生成的DataX配置文件" class="headerlink" title="2.2.4.5 测试生成的DataX配置文件"></a>2.2.4.5 测试生成的DataX配置文件</h4><p>以activity_info为例，测试用脚本生成的配置文件是否可用。</p>
<p>1）创建目标路径</p>
<p>由于DataX同步任务要求目标路径提前存在，故需手动创建路径，当前activity_info表的目标路径应为/origin_data/gmall/db/activity_info_full/2022-06-08。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ hadoop fs -mkdir -p /origin_data/gmall/db/activity_info_full/2022-06-08</span><br></pre></td></tr></table></figure>

<p>2）执行DataX同步命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ python /opt/module/datax/bin/datax.py -p"-Dtargetdir=/origin_data/gmall/db/activity_info_full/2022-06-08" /opt/module/datax/job/import/gmall.activity_info.json</span><br></pre></td></tr></table></figure>

<p>3）观察同步结果</p>
<p>观察HFDS目标路径是否出现数据。</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps56.jpg" alt="img"> </p>
<h4 id="2-2-4-6-全量表数据同步脚本"><a href="#2-2-4-6-全量表数据同步脚本" class="headerlink" title="2.2.4.6 全量表数据同步脚本"></a>2.2.4.6 全量表数据同步脚本</h4><p>为方便使用以及后续的任务调度，此处编写一个全量表数据同步脚本。</p>
<p>1）在~/bin目录创建mysql_to_hdfs_full.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ vim ~/bin/mysql_to_hdfs_full.sh</span><br></pre></td></tr></table></figure>

<p>脚本内容如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">DATAX_HOME=/opt/module/datax</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果传入日期则do_date等于传入的日期，否则等于前一天日期</span></span><br><span class="line">if [ -n "$2" ] ;then</span><br><span class="line">    do_date=$2</span><br><span class="line">else</span><br><span class="line">    do_date=`date -d "-1 day" +%F`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">处理目标路径，此处的处理逻辑是，如果目标路径不存在，则创建；若存在，则清空，目的是保证同步任务可重复执行</span></span><br><span class="line">handle_targetdir() &#123;</span><br><span class="line">  hadoop fs -test -e $1</span><br><span class="line">  if [[ $? -eq 1 ]]; then</span><br><span class="line">    echo "路径$1不存在，正在创建......"</span><br><span class="line">    hadoop fs -mkdir -p $1</span><br><span class="line">  else</span><br><span class="line">    echo "路径$1已经存在"</span><br><span class="line">    </span><br><span class="line">  fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">数据同步</span></span><br><span class="line">import_data() &#123;</span><br><span class="line">  datax_config=$1</span><br><span class="line">  target_dir=$2</span><br><span class="line"></span><br><span class="line">  handle_targetdir $target_dir</span><br><span class="line">  python $DATAX_HOME/bin/datax.py -p"-Dtargetdir=$target_dir" $datax_config</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">"activity_info")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.activity_info.json /origin_data/gmall/db/activity_info_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">"activity_rule")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.activity_rule.json /origin_data/gmall/db/activity_rule_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">"base_category1")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.base_category1.json /origin_data/gmall/db/base_category1_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">"base_category2")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.base_category2.json /origin_data/gmall/db/base_category2_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">"base_category3")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.base_category3.json /origin_data/gmall/db/base_category3_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">"base_dic")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.base_dic.json /origin_data/gmall/db/base_dic_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">"base_province")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.base_province.json /origin_data/gmall/db/base_province_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">"base_region")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.base_region.json /origin_data/gmall/db/base_region_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">"base_trademark")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.base_trademark.json /origin_data/gmall/db/base_trademark_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">"cart_info")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.cart_info.json /origin_data/gmall/db/cart_info_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">"coupon_info")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.coupon_info.json /origin_data/gmall/db/coupon_info_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">"sku_attr_value")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.sku_attr_value.json /origin_data/gmall/db/sku_attr_value_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">"sku_info")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.sku_info.json /origin_data/gmall/db/sku_info_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">"sku_sale_attr_value")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.sku_sale_attr_value.json /origin_data/gmall/db/sku_sale_attr_value_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">"spu_info")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.spu_info.json /origin_data/gmall/db/spu_info_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">"promotion_pos")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.promotion_pos.json /origin_data/gmall/db/promotion_pos_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">"promotion_refer")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.promotion_refer.json /origin_data/gmall/db/promotion_refer_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">"all")</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.activity_info.json /origin_data/gmall/db/activity_info_full/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.activity_rule.json /origin_data/gmall/db/activity_rule_full/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.base_category1.json /origin_data/gmall/db/base_category1_full/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.base_category2.json /origin_data/gmall/db/base_category2_full/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.base_category3.json /origin_data/gmall/db/base_category3_full/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.base_dic.json /origin_data/gmall/db/base_dic_full/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.base_province.json /origin_data/gmall/db/base_province_full/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.base_region.json /origin_data/gmall/db/base_region_full/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.base_trademark.json /origin_data/gmall/db/base_trademark_full/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.cart_info.json /origin_data/gmall/db/cart_info_full/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.coupon_info.json /origin_data/gmall/db/coupon_info_full/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.sku_attr_value.json /origin_data/gmall/db/sku_attr_value_full/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.sku_info.json /origin_data/gmall/db/sku_info_full/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.sku_sale_attr_value.json /origin_data/gmall/db/sku_sale_attr_value_full/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.spu_info.json /origin_data/gmall/db/spu_info_full/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.promotion_pos.json /origin_data/gmall/db/promotion_pos_full/$do_date</span><br><span class="line">  import_data /opt/module/datax/job/import/gmall.promotion_refer.json /origin_data/gmall/db/promotion_refer_full/$do_date</span><br><span class="line">  ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>2）为mysql_to_hdfs_full.sh增加执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ chmod 777 ~/bin/mysql_to_hdfs_full.sh</span><br></pre></td></tr></table></figure>

<p>3）测试同步脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ mysql_to_hdfs_full.sh all 2022-06-08</span><br></pre></td></tr></table></figure>

<p>4）检查同步结果</p>
<p>查看HDFS目表路径是否出现全量表数据，全量表共17张。</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps57.jpg" alt="img"> </p>
<h3 id="2-2-5-增量表数据同步"><a href="#2-2-5-增量表数据同步" class="headerlink" title="2.2.5 增量表数据同步"></a>2.2.5 增量表数据同步</h3><p>2.2.5.1 数据通道</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps58.png" alt="img"></p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/image-20240826171739719.png" alt="image-20240826171739719"></p>
<p>增量表的数据操作分为2步</p>
<ol>
<li>第一天执行称之为首日执行 - 全量数据同步（maxwell - bootstrap）</li>
<li>除了第一天以后的每一天执行称之为每日执行 - 增量数据同步（maxwell - insert, update, delete）</li>
</ol>
<p>2.2.5.2 Flume配置</p>
<p>1）Flume配置概述</p>
<p>Flume需要将Kafka中topic_db主题的数据传输到HDFS，故其需选用KafkaSource以及HDFSSink，Channel选用FileChannel。</p>
<p>需要注意的是， HDFSSink需要将不同MySQL业务表的数据写到不同的路径，并且路径中应当包含一层日期，用于区分每天的数据。关键配置如下：</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps59.png" alt="img"></p>
<p>具体数据示例如下：</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps60.png" alt="img"></p>
<p>2）Flume配置实操</p>
<p>（1）创建Flume配置文件</p>
<p>在hadoop104节点的Flume的job目录下创建kafka_to_hdfs_db.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 flume]$ vim job/kafka_to_hdfs_db.conf</span><br></pre></td></tr></table></figure>

<p>（2）配置文件内容如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">a1.sources.r1.batchSize = 5000</span><br><span class="line">a1.sources.r1.batchDurationMillis = 2000</span><br><span class="line">a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092</span><br><span class="line">a1.sources.r1.kafka.topics = topic_db</span><br><span class="line">a1.sources.r1.kafka.consumer.group.id = flume</span><br><span class="line">a1.sources.r1.setTopicHeader = true</span><br><span class="line">a1.sources.r1.topicHeader = topic</span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = com.atguigu.gmall.flume.interceptor.TimestampAndTableNameInterceptor$Builder</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type = file</span><br><span class="line">a1.channels.c1.checkpointDir = /opt/module/flume/checkpoint/behavior2</span><br><span class="line">a1.channels.c1.dataDirs = /opt/module/flume/data/behavior2/</span><br><span class="line">a1.channels.c1.maxFileSize = 2146435071</span><br><span class="line">a1.channels.c1.capacity = 1000000</span><br><span class="line">a1.channels.c1.keep-alive = 6</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># sink1</span></span></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = /origin_data/gmall/db/%&#123;tableName&#125;_inc/%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = db</span><br><span class="line">a1.sinks.k1.hdfs.round = false</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 10</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sinks.k1.hdfs.fileType = CompressedStream</span><br><span class="line">a1.sinks.k1.hdfs.codeC = gzip</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 拼装</span></span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel= c1</span><br></pre></td></tr></table></figure>

<p>（3）编写Flume拦截器</p>
<p>在com.atguigu.gmall.flume.interceptor包下创建TimestampAndTableNameInterceptor类。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.gmall.flume.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.interceptor.Interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.nio.charset.StandardCharsets;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimestampAndTableNameInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Map&lt;String, String&gt; headers = event.getHeaders();</span><br><span class="line">        String log = <span class="keyword">new</span> String(event.getBody(), StandardCharsets.UTF_8);</span><br><span class="line"></span><br><span class="line">        JSONObject jsonObject = JSONObject.parseObject(log);</span><br><span class="line"></span><br><span class="line">        Long ts = jsonObject.getLong(<span class="string">"ts"</span>);</span><br><span class="line">        <span class="comment">//Maxwell输出的数据中的ts字段时间戳单位为秒，Flume HDFSSink要求单位为毫秒</span></span><br><span class="line">        String timeMills = String.valueOf(ts * <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">        String tableName = jsonObject.getString(<span class="string">"table"</span>);</span><br><span class="line"></span><br><span class="line">        headers.put(<span class="string">"timestamp"</span>, timeMills);</span><br><span class="line">        headers.put(<span class="string">"tableName"</span>, tableName);</span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Event&gt; <span class="title">intercept</span><span class="params">(List&lt;Event&gt; events)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Event event : events) &#123;</span><br><span class="line">            intercept(event);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> events;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Builder</span> <span class="keyword">implements</span> <span class="title">Interceptor</span>.<span class="title">Builder</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Interceptor <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> TimestampAndTableNameInterceptor ();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（3）重新打包</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps61.jpg" alt="img"> </p>
<p>（4）删除hadoop104的/opt/module/flume/lib目录下的gmall-1.0-SNAPSHOT-jar-with-dependencies.jar文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 lib]$ cd /opt/module/flume/lib/</span><br><span class="line">[atguigu@hadoop104 lib]$ rm gmall-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>

<p>（5）将打好的包放入到hadoop104的/opt/module/flume/lib文件夹下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 lib]$ ls | grep gmall</span><br><span class="line">gmall-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>

<p>3）通道测试</p>
<p>（1）启动Zookeeper、Kafka集群及Maxwell</p>
<p>（2）启动hadoop104的Flume</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 flume]$ bin/flume-ng agent -n a1 -c conf/ -f job/kafka_to_hdfs_db.conf</span><br></pre></td></tr></table></figure>

<p>（3）生成模拟数据</p>
<p>确保Maxwell正在运行，而后生成数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ lg.sh</span><br></pre></td></tr></table></figure>

<p>（4）观察HDFS目标路径</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/wps62.jpg" alt="img"> </p>
<p>增量表目录出现，数据通道已打通。</p>
<p>（5）数据目标路径的日期说明</p>
<p>仔细观察，会发现目标路径中的日期，并非模拟数据的业务日期，而是当前日期。这是由于Maxwell输出的JSON字符串中的ts字段的值，是数据的变动日期。而真实场景下，数据的业务日期与变动日期应当是一致的。教学环境下需要修改时间戳日期，下文详述。</p>
<p>4）编写Flume启停脚本</p>
<p>为方便使用，此处编写一个Flume的启停脚本。</p>
<p>（1）在hadoop102节点的/home/atguigu/bin目录下创建脚本f3.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ vim f3.sh</span><br></pre></td></tr></table></figure>



<p>​    在脚本中填写如下内容。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">"start")</span><br><span class="line">        echo " --------启动 hadoop104 业务数据flume-------"</span><br><span class="line">        ssh hadoop104 "nohup /opt/module/flume/bin/flume-ng agent -n a1 -c /opt/module/flume/conf -f /opt/module/flume/job/kafka_to_hdfs_db.conf &gt;/dev/null 2&gt;&amp;1 &amp;"</span><br><span class="line">;;</span><br><span class="line">"stop")</span><br><span class="line"></span><br><span class="line">        echo " --------停止 hadoop104 业务数据flume-------"</span><br><span class="line">        ssh hadoop104 "ps -ef | grep kafka_to_hdfs_db | grep -v grep |awk '&#123;print \$2&#125;' | xargs -n1 kill"</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>（2）增加脚本执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ chmod 777 f3.sh</span><br></pre></td></tr></table></figure>

<p>（3）f3启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ f3.sh start</span><br></pre></td></tr></table></figure>

<p>（4）f3停止</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ f3.sh stop</span><br></pre></td></tr></table></figure>

<p>2.2.5.3 Maxwell配置</p>
<p>1）Maxwell时间戳问题</p>
<p><img src="/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/image-20240822110329349.png" alt="image-20240822110329349"></p>
<p>为了让Maxwell时间戳日期与模拟的业务日期保持一致，对Maxwell源码进行改动，增加了mock_date参数，在/opt/module/maxwell/config.properties文件中将该参数的值修改为业务日期即可。</p>
<p>2）补充mock.date参数</p>
<p>配置参数如下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">log_level=info</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">Maxwell数据发送目的地，可选配置有stdout|file|kafka|kinesis|pubsub|sqs|rabbitmq|redis</span></span><br><span class="line">producer=kafka</span><br><span class="line"><span class="meta">#</span><span class="bash"> 目标Kafka集群地址</span></span><br><span class="line">kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br><span class="line"><span class="meta">#</span><span class="bash">目标Kafka topic，可静态配置，例如:maxwell，也可动态配置，例如：%&#123;database&#125;_%&#123;table&#125;</span></span><br><span class="line">kafka_topic=topic_db</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> MySQL相关配置</span></span><br><span class="line">host=hadoop102</span><br><span class="line">user=maxwell</span><br><span class="line">password=maxwell</span><br><span class="line">jdbc_options=useSSL=false&amp;serverTimezone=Asia/Shanghai&amp;allowPublicKeyRetrieval=true</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 过滤gmall中的z_log表数据，该表是日志数据的备份，无须采集</span></span><br><span class="line">filter=exclude:gmall.z_log</span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定数据按照主键分组进入Kafka不同分区，避免数据倾斜</span></span><br><span class="line">producer_partition_by=primary_key</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改数据时间戳的日期部分</span></span><br><span class="line">mock_date=2022-06-08</span><br></pre></td></tr></table></figure>

<p>3）重新启动Maxwell</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ mxw.sh restart</span><br></pre></td></tr></table></figure>

<p>4）重新生成模拟数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ lg.sh</span><br></pre></td></tr></table></figure>

<p>5）观察HDFS目标路径日期是否与业务日期保持一致</p>
<p>2.2.5.4 增量表首日全量同步</p>
<p>通常情况下，增量表需要在首日进行一次全量同步，后续每日再进行增量同步，首日全量同步可以使用Maxwell的bootstrap功能，方便起见，下面编写一个增量表首日全量同步脚本。</p>
<p>1）在~/bin目录创建mysql_to_kafka_inc_init.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ vim mysql_to_kafka_inc_init.sh</span><br></pre></td></tr></table></figure>

<p>脚本内容如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 该脚本的作用是初始化所有的增量表，只需执行一次</span></span><br><span class="line"></span><br><span class="line">MAXWELL_HOME=/opt/module/maxwell</span><br><span class="line"></span><br><span class="line">import_data() &#123;</span><br><span class="line">    $MAXWELL_HOME/bin/maxwell-bootstrap --database gmall --table $1 --config $MAXWELL_HOME/config.properties</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">"cart_info")</span><br><span class="line">  import_data cart_info</span><br><span class="line">  ;;</span><br><span class="line">"comment_info")</span><br><span class="line">  import_data comment_info</span><br><span class="line">  ;;</span><br><span class="line">"coupon_use")</span><br><span class="line">  import_data coupon_use</span><br><span class="line">  ;;</span><br><span class="line">"favor_info")</span><br><span class="line">  import_data favor_info</span><br><span class="line">  ;;</span><br><span class="line">"order_detail")</span><br><span class="line">  import_data order_detail</span><br><span class="line">  ;;</span><br><span class="line">"order_detail_activity")</span><br><span class="line">  import_data order_detail_activity</span><br><span class="line">  ;;</span><br><span class="line">"order_detail_coupon")</span><br><span class="line">  import_data order_detail_coupon</span><br><span class="line">  ;;</span><br><span class="line">"order_info")</span><br><span class="line">  import_data order_info</span><br><span class="line">  ;;</span><br><span class="line">"order_refund_info")</span><br><span class="line">  import_data order_refund_info</span><br><span class="line">  ;;</span><br><span class="line">"order_status_log")</span><br><span class="line">  import_data order_status_log</span><br><span class="line">  ;;</span><br><span class="line">"payment_info")</span><br><span class="line">  import_data payment_info</span><br><span class="line">  ;;</span><br><span class="line">"refund_payment")</span><br><span class="line">  import_data refund_payment</span><br><span class="line">  ;;</span><br><span class="line">"user_info")</span><br><span class="line">  import_data user_info</span><br><span class="line">  ;;</span><br><span class="line">"all")</span><br><span class="line">  import_data cart_info</span><br><span class="line">  import_data comment_info</span><br><span class="line">  import_data coupon_use</span><br><span class="line">  import_data favor_info</span><br><span class="line">  import_data order_detail</span><br><span class="line">  import_data order_detail_activity</span><br><span class="line">  import_data order_detail_coupon</span><br><span class="line">  import_data order_info</span><br><span class="line">  import_data order_refund_info</span><br><span class="line">  import_data order_status_log</span><br><span class="line">  import_data payment_info</span><br><span class="line">  import_data refund_payment</span><br><span class="line">  import_data user_info</span><br><span class="line">  ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>2）为mysql_to_kafka_inc_init.sh增加执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ chmod 777 ~/bin/mysql_to_kafka_inc_init.sh</span><br></pre></td></tr></table></figure>

<p>3）测试同步脚本</p>
<p>（1）清理历史数据</p>
<p>为方便查看结果，现将HDFS上之前同步的增量表数据删除。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ hadoop fs -ls /origin_data/gmall/db | grep _inc | awk '&#123;print $8&#125;' | xargs hadoop fs -rm -r -f</span><br></pre></td></tr></table></figure>

<p>（2）执行同步脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ mysql_to_kafka_inc_init.sh all</span><br></pre></td></tr></table></figure>

<p>4）检查同步结果</p>
<p>观察HDFS上是否重新出现增量表数据。</p>
<h2 id="2-3-采集通道启动-停止脚本"><a href="#2-3-采集通道启动-停止脚本" class="headerlink" title="2.3 采集通道启动/停止脚本"></a>2.3 采集通道启动/停止脚本</h2><p>1）在/home/atguigu/bin目录下创建脚本cluster.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ vim cluster.sh</span><br></pre></td></tr></table></figure>

<p>​    在脚本中填写如下内容。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">"start")&#123;</span><br><span class="line">        echo ================== 启动 集群 ==================</span><br><span class="line"></span><br><span class="line">        #启动 Zookeeper集群</span><br><span class="line">        zk.sh start</span><br><span class="line"></span><br><span class="line">        #启动 Hadoop集群</span><br><span class="line">        hdp.sh start</span><br><span class="line"></span><br><span class="line">        #启动 Kafka采集集群</span><br><span class="line">        kf.sh start</span><br><span class="line"></span><br><span class="line">        #启动采集 Flume</span><br><span class="line">        f1.sh start</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">启动日志消费 Flume</span></span><br><span class="line">        f2.sh start</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">启动业务消费 Flume</span></span><br><span class="line">        f3.sh start</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">启动 maxwell</span></span><br><span class="line">        mxw.sh start</span><br><span class="line"></span><br><span class="line">        &#125;;;</span><br><span class="line">"stop")&#123;</span><br><span class="line">        echo ================== 停止 集群 ==================</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">停止 Maxwell</span></span><br><span class="line">        mxw.sh stop</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">停止 业务消费Flume</span></span><br><span class="line">        f3.sh stop</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">停止 日志消费Flume</span></span><br><span class="line">        f2.sh stop</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">停止 日志采集Flume</span></span><br><span class="line">        f1.sh stop</span><br><span class="line"></span><br><span class="line">        #停止 Kafka采集集群</span><br><span class="line">        kf.sh stop</span><br><span class="line"></span><br><span class="line">        #停止 Hadoop集群</span><br><span class="line">        hdp.sh stop</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">循环直至 Kafka 集群进程全部停止</span></span><br><span class="line">		kafka_count=$(xcall jps | grep Kafka | wc -l)</span><br><span class="line">		while [ $kafka_count -gt 0 ]</span><br><span class="line">		do</span><br><span class="line">			sleep 1</span><br><span class="line">			kafka_count=$(jpsall | grep Kafka | wc -l)</span><br><span class="line">            echo "当前未停止的 Kafka 进程数为 $kafka_count"</span><br><span class="line">		done</span><br><span class="line"></span><br><span class="line">        #停止 Zookeeper集群</span><br><span class="line">        zk.sh stop</span><br><span class="line"></span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>2）增加脚本执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ chmod 777 cluster.sh</span><br></pre></td></tr></table></figure>

<p>3）cluster集群启动脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ cluster.sh start</span><br></pre></td></tr></table></figure>

<p>4）cluster集群停止脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ cluster.sh stop</span><br></pre></td></tr></table></figure>

<h1 id="第3章-数仓环境准备"><a href="#第3章-数仓环境准备" class="headerlink" title="第3章 数仓环境准备"></a>第3章 数仓环境准备</h1><h2 id="3-1-Hive安装部署"><a href="#3-1-Hive安装部署" class="headerlink" title="3.1 Hive安装部署"></a>3.1 Hive安装部署</h2><p>1）把hive-3.1.3.tar.gz上传到linux的/opt/software目录下</p>
<p>2）解压hive-3.1.3.tar.gz到/opt/module/目录下面</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf /opt/software/hive-3.1.3.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>3）修改hive-3.1.3-bin.tar.gz的名称为hive</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ mv /opt/module/apache-hive-3.1.3-bin/ /opt/module/hive</span><br></pre></td></tr></table></figure>

<p>4）修改/etc/profile.d/my_env.sh，添加环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>添加内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">HIVE_HOME</span></span><br><span class="line">export HIVE_HOME=/opt/module/hive</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br></pre></td></tr></table></figure>

<p>重启Xshell对话框或者source一下 /etc/profile.d/my_env.sh文件，使环境变量生效。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ source /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>5）解决日志Jar包冲突，进入/opt/module/hive/lib目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 lib]$ mv log4j-slf4j-impl-2.17.1.jar log4j-slf4j-impl-2.17.1.jar.bak</span><br></pre></td></tr></table></figure>

<h2 id="3-2-Hive元数据配置到MySQL"><a href="#3-2-Hive元数据配置到MySQL" class="headerlink" title="3.2 Hive元数据配置到MySQL"></a>3.2 Hive元数据配置到MySQL</h2><h3 id="3-2-1-拷贝驱动"><a href="#3-2-1-拷贝驱动" class="headerlink" title="3.2.1 拷贝驱动"></a>3.2.1 拷贝驱动</h3><p>将MySQL的JDBC驱动拷贝到Hive的lib目录下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 lib]$ cp /opt/software/mysql/mysql-connector-j-8.0.31.jar /opt/module/hive/lib/</span><br></pre></td></tr></table></figure>

<h3 id="3-2-2-配置Metastore到MySQL"><a href="#3-2-2-配置Metastore到MySQL" class="headerlink" title="3.2.2 配置Metastore到MySQL"></a>3.2.2 配置Metastore到MySQL</h3><p>在$HIVE_HOME/conf目录下新建hive-site.xml文件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ vim hive-site.xml</span><br></pre></td></tr></table></figure>

<p>添加如下内容。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置Hive保存元数据信息所需的 MySQL URL地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?useSSL=false<span class="symbol">&amp;amp;</span>useUnicode=true<span class="symbol">&amp;amp;</span>characterEncoding=UTF-8<span class="symbol">&amp;amp;</span>allowPublicKeyRetrieval=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--配置Hive连接MySQL的驱动全类名--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.cj.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--配置Hive连接MySQL的用户名 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--配置Hive连接MySQL的密码 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.db.notification.api.auth<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="3-3-启动Hive"><a href="#3-3-启动Hive" class="headerlink" title="3.3 启动Hive"></a>3.3 启动Hive</h2><h3 id="3-3-1-初始化元数据库"><a href="#3-3-1-初始化元数据库" class="headerlink" title="3.3.1 初始化元数据库"></a>3.3.1 初始化元数据库</h3><p>1）登陆MySQL</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ mysql -uroot -p000000</span><br></pre></td></tr></table></figure>

<p>2）新建Hive元数据库</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create database metastore;</span><br></pre></td></tr></table></figure>

<p>3）初始化Hive元数据库</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ schematool -initSchema -dbType mysql -verbose</span><br></pre></td></tr></table></figure>

<p>4）修改元数据库字符集</p>
<p>Hive元数据库的字符集默认为Latin1，由于其不支持中文字符，所以建表语句中如果包含中文注释，会出现乱码现象。如需解决乱码问题，须做以下修改。</p>
<p>修改Hive元数据库中存储注释的字段的字符集为utf-8。</p>
<p>（1）字段注释</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; use metastore;</span><br><span class="line">mysql&gt; alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;</span><br></pre></td></tr></table></figure>

<p>（2）表注释</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; alter table TABLE_PARAMS modify column PARAM_VALUE mediumtext character set utf8;</span><br></pre></td></tr></table></figure>

<p>4）退出mysql</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; quit;</span><br></pre></td></tr></table></figure>

<h3 id="3-3-2-启动Hive客户端"><a href="#3-3-2-启动Hive客户端" class="headerlink" title="3.3.2 启动Hive客户端"></a>3.3.2 启动Hive客户端</h3><p>1）启动Hive客户端</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ hive</span><br></pre></td></tr></table></figure>

<p>2）查看一下数据库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">database_name</span><br><span class="line">default</span><br><span class="line">Time taken: 0.955 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Rui Zhang
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://yoursite.com/2024/08/14/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E6%95%B0%E4%BB%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A53/" title="电商数仓项目-数仓数据同步策略3">http://yoursite.com/2024/08/14/电商数仓项目-数仓数据同步策略3/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%95%B0%E4%BB%93/" rel="tag"># 数仓</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/08/08/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%B9%B3%E5%8F%B02/" rel="prev" title="电商数仓项目-业务数据采集平台2">
      <i class="fa fa-chevron-left"></i> 电商数仓项目-业务数据采集平台2
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/08/28/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%B3%BB%E7%BB%9F4/" rel="next" title="电商数仓项目-电商数据仓库系统4">
      电商数仓项目-电商数据仓库系统4 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#第1章-实时数仓同步数据"><span class="nav-number">1.</span> <span class="nav-text">第1章 实时数仓同步数据</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第2章-离线数仓同步数据"><span class="nav-number">2.</span> <span class="nav-text">第2章 离线数仓同步数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-用户行为数据同步"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 用户行为数据同步</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-数据通道"><span class="nav-number">2.1.1.</span> <span class="nav-text">2.1.1 数据通道</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-2-日志消费Flume配置概述"><span class="nav-number">2.1.2.</span> <span class="nav-text">2.1.2 日志消费Flume配置概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-3-日志消费Flume配置实操"><span class="nav-number">2.1.3.</span> <span class="nav-text">2.1.3 日志消费Flume配置实操</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-4-日志消费Flume测试"><span class="nav-number">2.1.4.</span> <span class="nav-text">2.1.4 日志消费Flume测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-5-日志消费Flume启停脚本"><span class="nav-number">2.1.5.</span> <span class="nav-text">2.1.5 日志消费Flume启停脚本</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-业务数据同步"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 业务数据同步</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-数据同步策略概述"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1 数据同步策略概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-数据同步策略选择"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.2 数据同步策略选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-数据同步工具概述"><span class="nav-number">2.2.3.</span> <span class="nav-text">2.2.3 数据同步工具概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-4-全量表数据同步"><span class="nav-number">2.2.4.</span> <span class="nav-text">2.2.4 全量表数据同步</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-4-1-数据同步工具DataX部署"><span class="nav-number">2.2.4.1.</span> <span class="nav-text">2.2.4.1 数据同步工具DataX部署</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-4-1-1-DataX简介"><span class="nav-number">2.2.4.1.1.</span> <span class="nav-text">2.2.4.1.1 DataX简介</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-1-1-1-DataX概述"><span class="nav-number">2.2.4.1.1.1.</span> <span class="nav-text">2.2.1.1.1 DataX概述</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-1-1-2-DataX支持的数据源"><span class="nav-number">2.2.4.1.1.2.</span> <span class="nav-text">2.2.1.1.2 DataX支持的数据源</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-4-1-2-DataX架构原理"><span class="nav-number">2.2.4.1.2.</span> <span class="nav-text">2.2.4.1.2 DataX架构原理</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-1-2-1-DataX设计理念"><span class="nav-number">2.2.4.1.2.1.</span> <span class="nav-text">2.2.4.1.2.1 DataX设计理念</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-1-2-2-DataX框架设计"><span class="nav-number">2.2.4.1.2.2.</span> <span class="nav-text">2.2.4.1.2.2 DataX框架设计</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-1-2-3-DataX运行流程"><span class="nav-number">2.2.4.1.2.3.</span> <span class="nav-text">2.2.4.1.2.3 DataX运行流程</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-1-2-4-DataX调度决策思路"><span class="nav-number">2.2.4.1.2.4.</span> <span class="nav-text">2.2.4.1.2.4 DataX调度决策思路</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-1-2-5-DataX与Sqoop对比"><span class="nav-number">2.2.4.1.2.5.</span> <span class="nav-text">2.2.4.1.2.5 DataX与Sqoop对比</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-4-1-3-DataX部署"><span class="nav-number">2.2.4.1.3.</span> <span class="nav-text">2.2.4.1.3 DataX部署</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-4-1-4-DataX使用"><span class="nav-number">2.2.4.1.4.</span> <span class="nav-text">2.2.4.1.4 DataX使用</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-1-4-1-DataX使用概述"><span class="nav-number">2.2.4.1.4.1.</span> <span class="nav-text">2.2.4.1.4.1 DataX使用概述</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-1-4-2-同步MySQL数据到HDFS案例"><span class="nav-number">2.2.4.1.4.2.</span> <span class="nav-text">2.2.4.1.4.2 同步MySQL数据到HDFS案例</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-1-4-3-同步HDFS数据到MySQL案例"><span class="nav-number">2.2.4.1.4.3.</span> <span class="nav-text">2.2.4.1.4.3 同步HDFS数据到MySQL案例</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-4-1-5-DataX优化"><span class="nav-number">2.2.4.1.5.</span> <span class="nav-text">2.2.4.1.5 DataX优化</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-1-5-1-速度控制"><span class="nav-number">2.2.4.1.5.1.</span> <span class="nav-text">2.2.4.1.5.1 速度控制</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-1-5-2-内存调整"><span class="nav-number">2.2.4.1.5.2.</span> <span class="nav-text">2.2.4.1.5.2 内存调整</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-4-2-数据通道"><span class="nav-number">2.2.4.2.</span> <span class="nav-text">2.2.4.2 数据通道</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-4-3-DataX配置文件"><span class="nav-number">2.2.4.3.</span> <span class="nav-text">2.2.4.3 DataX配置文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-4-4-DataX配置文件生成"><span class="nav-number">2.2.4.4.</span> <span class="nav-text">2.2.4.4 DataX配置文件生成</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-4-4-1-代码"><span class="nav-number">2.2.4.4.1.</span> <span class="nav-text">2.2.4.4.1 代码</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-4-1-1-新建Maven项目"><span class="nav-number">2.2.4.4.1.1.</span> <span class="nav-text">2.2.4.4.1.1 新建Maven项目</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-4-1-2-添加如下依赖"><span class="nav-number">2.2.4.4.1.2.</span> <span class="nav-text">2.2.4.4.1.2 添加如下依赖</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-4-1-3-新建com-atguigu-datax-beans-Column类"><span class="nav-number">2.2.4.4.1.3.</span> <span class="nav-text">2.2.4.4.1.3 新建com.atguigu.datax.beans.Column类</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-4-1-4-新建com-atguigu-datax-beans-Table类"><span class="nav-number">2.2.4.4.1.4.</span> <span class="nav-text">2.2.4.4.1.4 新建com.atguigu.datax.beans.Table类</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-4-1-5-新建com-atguigu-datax-configuration-Configuration类"><span class="nav-number">2.2.4.4.1.5.</span> <span class="nav-text">2.2.4.4.1.5 新建com.atguigu.datax.configuration.Configuration类</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-4-1-6-新建com-atguigu-datax-helper-DataxJsonHelper类"><span class="nav-number">2.2.4.4.1.6.</span> <span class="nav-text">2.2.4.4.1.6 新建com.atguigu.datax.helper.DataxJsonHelper类</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-4-1-7-新建com-atguigu-datax-helper-MysqlHelper类"><span class="nav-number">2.2.4.4.1.7.</span> <span class="nav-text">2.2.4.4.1.7 新建com.atguigu.datax.helper.MysqlHelper类</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-4-1-8-新建com-atguigu-datax-Main类"><span class="nav-number">2.2.4.4.1.8.</span> <span class="nav-text">2.2.4.4.1.8 新建com.atguigu.datax.Main类</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-4-4-2-如何使用"><span class="nav-number">2.2.4.4.2.</span> <span class="nav-text">2.2.4.4.2 如何使用</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-4-2-1-修改配置文件"><span class="nav-number">2.2.4.4.2.1.</span> <span class="nav-text">2.2.4.4.2.1 修改配置文件</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-4-2-2-运行"><span class="nav-number">2.2.4.4.2.2.</span> <span class="nav-text">2.2.4.4.2.2 运行</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-4-5-测试生成的DataX配置文件"><span class="nav-number">2.2.4.5.</span> <span class="nav-text">2.2.4.5 测试生成的DataX配置文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-4-6-全量表数据同步脚本"><span class="nav-number">2.2.4.6.</span> <span class="nav-text">2.2.4.6 全量表数据同步脚本</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-5-增量表数据同步"><span class="nav-number">2.2.5.</span> <span class="nav-text">2.2.5 增量表数据同步</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-采集通道启动-停止脚本"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 采集通道启动&#x2F;停止脚本</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第3章-数仓环境准备"><span class="nav-number">3.</span> <span class="nav-text">第3章 数仓环境准备</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-Hive安装部署"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 Hive安装部署</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-Hive元数据配置到MySQL"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 Hive元数据配置到MySQL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-拷贝驱动"><span class="nav-number">3.2.1.</span> <span class="nav-text">3.2.1 拷贝驱动</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-配置Metastore到MySQL"><span class="nav-number">3.2.2.</span> <span class="nav-text">3.2.2 配置Metastore到MySQL</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-启动Hive"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 启动Hive</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-1-初始化元数据库"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.3.1 初始化元数据库</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-2-启动Hive客户端"><span class="nav-number">3.3.2.</span> <span class="nav-text">3.3.2 启动Hive客户端</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Rui Zhang"
      src="/images/head.jpg">
  <p class="site-author-name" itemprop="name">Rui Zhang</p>
  <div class="site-description" itemprop="description">不在沉默中爆发，就在沉默中灭亡</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">52</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rui Zhang</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">1.7m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">25:10</span>
</div>




        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":120,"height":230},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>
